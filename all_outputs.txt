Streaming output truncated to the last 5000 lines.
step 500: train loss 2.4262, val loss 2.4279
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size8_max_iters2000_dropout0.2
iter 500: loss 2.4267, time 414.19ms, mfu 0.14%
iter 505: loss 2.4298, time 20.92ms, mfu 0.14%
iter 510: loss 2.4774, time 20.16ms, mfu 0.14%
iter 515: loss 2.4197, time 20.73ms, mfu 0.14%
iter 520: loss 2.5470, time 22.67ms, mfu 0.14%
iter 525: loss 2.4739, time 20.07ms, mfu 0.14%
iter 530: loss 2.3392, time 19.74ms, mfu 0.14%
iter 535: loss 2.3769, time 28.03ms, mfu 0.13%
iter 540: loss 2.3591, time 28.39ms, mfu 0.13%
iter 545: loss 2.4279, time 25.56ms, mfu 0.13%
iter 550: loss 2.4089, time 19.59ms, mfu 0.13%
iter 555: loss 2.4555, time 20.54ms, mfu 0.13%
iter 560: loss 2.4588, time 21.91ms, mfu 0.13%
iter 565: loss 2.4811, time 26.59ms, mfu 0.13%
iter 570: loss 2.3892, time 31.41ms, mfu 0.12%
iter 575: loss 2.4887, time 20.21ms, mfu 0.12%
iter 580: loss 2.4137, time 26.68ms, mfu 0.12%
iter 585: loss 2.4127, time 25.89ms, mfu 0.12%
iter 590: loss 2.4295, time 27.19ms, mfu 0.12%
iter 595: loss 2.4823, time 24.61ms, mfu 0.12%
iter 600: loss 2.3661, time 28.79ms, mfu 0.11%
iter 605: loss 2.4833, time 15.68ms, mfu 0.12%
iter 610: loss 2.4092, time 16.20ms, mfu 0.13%
iter 615: loss 2.3771, time 16.08ms, mfu 0.13%
iter 620: loss 2.4514, time 16.16ms, mfu 0.13%
iter 625: loss 2.3950, time 16.47ms, mfu 0.14%
iter 630: loss 2.4802, time 16.03ms, mfu 0.14%
iter 635: loss 2.3720, time 17.96ms, mfu 0.14%
iter 640: loss 2.3403, time 27.52ms, mfu 0.14%
iter 645: loss 2.4421, time 19.81ms, mfu 0.14%
iter 650: loss 2.4690, time 15.96ms, mfu 0.14%
iter 655: loss 2.3816, time 15.91ms, mfu 0.14%
iter 660: loss 2.4534, time 16.78ms, mfu 0.15%
iter 665: loss 2.3639, time 15.42ms, mfu 0.15%
iter 670: loss 2.4145, time 17.17ms, mfu 0.15%
iter 675: loss 2.3648, time 16.42ms, mfu 0.15%
iter 680: loss 2.3708, time 15.99ms, mfu 0.15%
iter 685: loss 2.3465, time 17.71ms, mfu 0.15%
iter 690: loss 2.5124, time 15.66ms, mfu 0.16%
iter 695: loss 2.3394, time 15.77ms, mfu 0.16%
iter 700: loss 2.4357, time 26.07ms, mfu 0.15%
iter 705: loss 2.4610, time 16.06ms, mfu 0.15%
iter 710: loss 2.3467, time 17.20ms, mfu 0.15%
iter 715: loss 2.3492, time 16.39ms, mfu 0.16%
iter 720: loss 2.4030, time 23.19ms, mfu 0.15%
iter 725: loss 2.4174, time 16.44ms, mfu 0.15%
iter 730: loss 2.4114, time 15.69ms, mfu 0.16%
iter 735: loss 2.3828, time 15.87ms, mfu 0.16%
iter 740: loss 2.4229, time 17.75ms, mfu 0.16%
iter 745: loss 2.4161, time 16.75ms, mfu 0.16%
step 750: train loss 2.3421, val loss 2.3349
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size8_max_iters2000_dropout0.2
iter 750: loss 2.4180, time 315.44ms, mfu 0.14%
iter 755: loss 2.3752, time 16.29ms, mfu 0.14%
iter 760: loss 2.4170, time 18.04ms, mfu 0.15%
iter 765: loss 2.3522, time 16.03ms, mfu 0.15%
iter 770: loss 2.4462, time 16.49ms, mfu 0.15%
iter 775: loss 2.3461, time 15.85ms, mfu 0.15%
iter 780: loss 2.3312, time 19.28ms, mfu 0.15%
iter 785: loss 2.3734, time 15.99ms, mfu 0.15%
iter 790: loss 2.4009, time 15.68ms, mfu 0.16%
iter 795: loss 2.3818, time 16.05ms, mfu 0.16%
iter 800: loss 2.3642, time 15.28ms, mfu 0.16%
iter 805: loss 2.4024, time 16.96ms, mfu 0.16%
iter 810: loss 2.3817, time 16.26ms, mfu 0.16%
iter 815: loss 2.3267, time 21.67ms, mfu 0.16%
iter 820: loss 2.4288, time 16.06ms, mfu 0.16%
iter 825: loss 2.4159, time 16.02ms, mfu 0.16%
iter 830: loss 2.4025, time 15.47ms, mfu 0.16%
iter 835: loss 2.3213, time 15.97ms, mfu 0.16%
iter 840: loss 2.3666, time 16.08ms, mfu 0.16%
iter 845: loss 2.4496, time 20.82ms, mfu 0.16%
iter 850: loss 2.3308, time 16.75ms, mfu 0.16%
iter 855: loss 2.3982, time 16.21ms, mfu 0.16%
iter 860: loss 2.3522, time 16.11ms, mfu 0.16%
iter 865: loss 2.3502, time 16.53ms, mfu 0.16%
iter 870: loss 2.3830, time 15.64ms, mfu 0.16%
iter 875: loss 2.4150, time 17.04ms, mfu 0.16%
iter 880: loss 2.3671, time 19.30ms, mfu 0.16%
iter 885: loss 2.3690, time 18.27ms, mfu 0.16%
iter 890: loss 2.3394, time 15.78ms, mfu 0.16%
iter 895: loss 2.4078, time 15.67ms, mfu 0.16%
iter 900: loss 2.3615, time 17.43ms, mfu 0.16%
iter 905: loss 2.3558, time 16.44ms, mfu 0.16%
iter 910: loss 2.3150, time 15.36ms, mfu 0.16%
iter 915: loss 2.3920, time 16.92ms, mfu 0.16%
iter 920: loss 2.3732, time 27.34ms, mfu 0.16%
iter 925: loss 2.2988, time 23.80ms, mfu 0.15%
iter 930: loss 2.4029, time 18.30ms, mfu 0.15%
iter 935: loss 2.2801, time 16.17ms, mfu 0.15%
iter 940: loss 2.4287, time 16.36ms, mfu 0.16%
iter 945: loss 2.2941, time 15.93ms, mfu 0.16%
iter 950: loss 2.3448, time 16.00ms, mfu 0.16%
iter 955: loss 2.3958, time 15.52ms, mfu 0.16%
iter 960: loss 2.3278, time 17.70ms, mfu 0.16%
iter 965: loss 2.3511, time 16.56ms, mfu 0.16%
iter 970: loss 2.3774, time 16.12ms, mfu 0.16%
iter 975: loss 2.3830, time 18.24ms, mfu 0.16%
iter 980: loss 2.3447, time 19.88ms, mfu 0.16%
iter 985: loss 2.2963, time 17.10ms, mfu 0.16%
iter 990: loss 2.2900, time 17.95ms, mfu 0.16%
iter 995: loss 2.3607, time 16.46ms, mfu 0.16%
step 1000: train loss 2.2817, val loss 2.2799
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size8_max_iters2000_dropout0.2
iter 1000: loss 2.2479, time 299.41ms, mfu 0.14%
iter 1005: loss 2.2843, time 15.73ms, mfu 0.15%
iter 1010: loss 2.3483, time 15.90ms, mfu 0.15%
iter 1015: loss 2.3544, time 16.25ms, mfu 0.15%
iter 1020: loss 2.3317, time 15.90ms, mfu 0.15%
iter 1025: loss 2.3619, time 15.94ms, mfu 0.15%
iter 1030: loss 2.3658, time 19.49ms, mfu 0.15%
iter 1035: loss 2.3206, time 15.66ms, mfu 0.16%
iter 1040: loss 2.3345, time 15.60ms, mfu 0.16%
iter 1045: loss 2.4274, time 15.72ms, mfu 0.16%
iter 1050: loss 2.3429, time 15.63ms, mfu 0.16%
iter 1055: loss 2.3234, time 15.84ms, mfu 0.16%
iter 1060: loss 2.3076, time 19.07ms, mfu 0.16%
iter 1065: loss 2.3189, time 16.26ms, mfu 0.16%
iter 1070: loss 2.3626, time 16.30ms, mfu 0.16%
iter 1075: loss 2.3336, time 15.81ms, mfu 0.16%
iter 1080: loss 2.3629, time 15.67ms, mfu 0.16%
iter 1085: loss 2.3746, time 22.59ms, mfu 0.16%
iter 1090: loss 2.3143, time 16.42ms, mfu 0.16%
iter 1095: loss 2.4200, time 17.07ms, mfu 0.16%
iter 1100: loss 2.2953, time 16.13ms, mfu 0.16%
iter 1105: loss 2.2858, time 16.81ms, mfu 0.16%
iter 1110: loss 2.3304, time 15.50ms, mfu 0.16%
iter 1115: loss 2.3188, time 15.72ms, mfu 0.16%
iter 1120: loss 2.3381, time 15.83ms, mfu 0.16%
iter 1125: loss 2.3666, time 16.32ms, mfu 0.16%
iter 1130: loss 2.4054, time 16.07ms, mfu 0.17%
iter 1135: loss 2.2991, time 18.78ms, mfu 0.16%
iter 1140: loss 2.3161, time 15.15ms, mfu 0.17%
iter 1145: loss 2.4057, time 23.40ms, mfu 0.16%
iter 1150: loss 2.3177, time 28.07ms, mfu 0.15%
iter 1155: loss 2.3648, time 24.27ms, mfu 0.15%
iter 1160: loss 2.3077, time 29.24ms, mfu 0.14%
iter 1165: loss 2.4103, time 21.39ms, mfu 0.14%
iter 1170: loss 2.3873, time 23.15ms, mfu 0.14%
iter 1175: loss 2.3152, time 22.26ms, mfu 0.14%
iter 1180: loss 2.3218, time 21.58ms, mfu 0.14%
iter 1185: loss 2.3115, time 30.89ms, mfu 0.13%
iter 1190: loss 2.3123, time 24.51ms, mfu 0.13%
iter 1195: loss 2.3383, time 21.40ms, mfu 0.13%
iter 1200: loss 2.2980, time 20.81ms, mfu 0.13%
iter 1205: loss 2.2862, time 23.87ms, mfu 0.13%
iter 1210: loss 2.3378, time 19.82ms, mfu 0.13%
iter 1215: loss 2.3020, time 20.19ms, mfu 0.13%
iter 1220: loss 2.3053, time 23.54ms, mfu 0.13%
iter 1225: loss 2.3026, time 24.44ms, mfu 0.13%
iter 1230: loss 2.2749, time 32.66ms, mfu 0.12%
iter 1235: loss 2.3441, time 20.01ms, mfu 0.12%
iter 1240: loss 2.3295, time 24.56ms, mfu 0.12%
iter 1245: loss 2.3479, time 20.56ms, mfu 0.12%
step 1250: train loss 2.2578, val loss 2.2913
iter 1250: loss 2.2721, time 391.56ms, mfu 0.11%
iter 1255: loss 2.3754, time 26.37ms, mfu 0.11%
iter 1260: loss 2.3425, time 34.62ms, mfu 0.11%
iter 1265: loss 2.3167, time 26.23ms, mfu 0.11%
iter 1270: loss 2.3630, time 18.66ms, mfu 0.11%
iter 1275: loss 2.2783, time 17.31ms, mfu 0.12%
iter 1280: loss 2.2699, time 19.94ms, mfu 0.12%
iter 1285: loss 2.2844, time 16.29ms, mfu 0.12%
iter 1290: loss 2.2786, time 16.33ms, mfu 0.13%
iter 1295: loss 2.4094, time 15.97ms, mfu 0.13%
iter 1300: loss 2.3286, time 16.34ms, mfu 0.14%
iter 1305: loss 2.3564, time 15.66ms, mfu 0.14%
iter 1310: loss 2.3435, time 16.15ms, mfu 0.14%
iter 1315: loss 2.3312, time 24.16ms, mfu 0.14%
iter 1320: loss 2.2942, time 19.16ms, mfu 0.14%
iter 1325: loss 2.3501, time 31.32ms, mfu 0.13%
iter 1330: loss 2.3856, time 20.94ms, mfu 0.13%
iter 1335: loss 2.3411, time 43.81ms, mfu 0.13%
iter 1340: loss 2.3993, time 16.16ms, mfu 0.13%
iter 1345: loss 2.3226, time 16.56ms, mfu 0.13%
iter 1350: loss 2.3117, time 16.82ms, mfu 0.14%
iter 1355: loss 2.3345, time 17.45ms, mfu 0.14%
iter 1360: loss 2.3505, time 37.82ms, mfu 0.13%
iter 1365: loss 2.3368, time 16.74ms, mfu 0.14%
iter 1370: loss 2.3155, time 17.42ms, mfu 0.14%
iter 1375: loss 2.3193, time 16.37ms, mfu 0.14%
iter 1380: loss 2.2559, time 19.42ms, mfu 0.14%
iter 1385: loss 2.4039, time 15.99ms, mfu 0.14%
iter 1390: loss 2.2756, time 16.20ms, mfu 0.15%
iter 1395: loss 2.2537, time 19.57ms, mfu 0.15%
iter 1400: loss 2.3367, time 16.32ms, mfu 0.15%
iter 1405: loss 2.3754, time 16.18ms, mfu 0.15%
iter 1410: loss 2.3285, time 16.29ms, mfu 0.15%
iter 1415: loss 2.3374, time 16.06ms, mfu 0.15%
iter 1420: loss 2.2293, time 32.38ms, mfu 0.15%
iter 1425: loss 2.3281, time 15.96ms, mfu 0.15%
iter 1430: loss 2.2912, time 15.76ms, mfu 0.15%
iter 1435: loss 2.3027, time 21.38ms, mfu 0.15%
iter 1440: loss 2.3430, time 16.75ms, mfu 0.15%
iter 1445: loss 2.2939, time 17.87ms, mfu 0.15%
iter 1450: loss 2.3176, time 16.52ms, mfu 0.15%
iter 1455: loss 2.2888, time 16.24ms, mfu 0.15%
iter 1460: loss 2.2757, time 16.19ms, mfu 0.16%
iter 1465: loss 2.3622, time 16.33ms, mfu 0.16%
iter 1470: loss 2.4453, time 17.12ms, mfu 0.16%
iter 1475: loss 2.3411, time 19.15ms, mfu 0.16%
iter 1480: loss 2.3311, time 16.54ms, mfu 0.16%
iter 1485: loss 2.3783, time 16.68ms, mfu 0.16%
iter 1490: loss 2.3234, time 17.32ms, mfu 0.16%
iter 1495: loss 2.2848, time 16.19ms, mfu 0.16%
step 1500: train loss 2.2436, val loss 2.2563
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size8_max_iters2000_dropout0.2
iter 1500: loss 2.3158, time 299.57ms, mfu 0.14%
iter 1505: loss 2.2941, time 16.29ms, mfu 0.15%
iter 1510: loss 2.2896, time 20.78ms, mfu 0.14%
iter 1515: loss 2.3023, time 16.26ms, mfu 0.15%
iter 1520: loss 2.3607, time 20.87ms, mfu 0.15%
iter 1525: loss 2.3122, time 18.09ms, mfu 0.15%
iter 1530: loss 2.3335, time 16.21ms, mfu 0.15%
iter 1535: loss 2.2562, time 15.75ms, mfu 0.15%
iter 1540: loss 2.2332, time 16.28ms, mfu 0.15%
iter 1545: loss 2.2547, time 16.77ms, mfu 0.15%
iter 1550: loss 2.3217, time 16.37ms, mfu 0.15%
iter 1555: loss 2.3011, time 17.36ms, mfu 0.15%
iter 1560: loss 2.2671, time 16.08ms, mfu 0.16%
iter 1565: loss 2.2825, time 16.14ms, mfu 0.16%
iter 1570: loss 2.3606, time 15.76ms, mfu 0.16%
iter 1575: loss 2.3164, time 16.47ms, mfu 0.16%
iter 1580: loss 2.2440, time 33.37ms, mfu 0.15%
iter 1585: loss 2.2924, time 16.43ms, mfu 0.15%
iter 1590: loss 2.2649, time 16.01ms, mfu 0.16%
iter 1595: loss 2.3450, time 16.07ms, mfu 0.16%
iter 1600: loss 2.3726, time 16.61ms, mfu 0.16%
iter 1605: loss 2.3066, time 16.19ms, mfu 0.16%
iter 1610: loss 2.3156, time 16.16ms, mfu 0.16%
iter 1615: loss 2.3065, time 16.98ms, mfu 0.16%
iter 1620: loss 2.3045, time 17.67ms, mfu 0.16%
iter 1625: loss 2.3178, time 21.80ms, mfu 0.16%
iter 1630: loss 2.2786, time 15.85ms, mfu 0.16%
iter 1635: loss 2.2739, time 15.67ms, mfu 0.16%
iter 1640: loss 2.3926, time 16.31ms, mfu 0.16%
iter 1645: loss 2.3343, time 16.83ms, mfu 0.16%
iter 1650: loss 2.2930, time 15.75ms, mfu 0.16%
iter 1655: loss 2.2522, time 16.88ms, mfu 0.16%
iter 1660: loss 2.1711, time 16.40ms, mfu 0.16%
iter 1665: loss 2.4072, time 16.29ms, mfu 0.16%
iter 1670: loss 2.2786, time 16.84ms, mfu 0.16%
iter 1675: loss 2.3770, time 19.39ms, mfu 0.16%
iter 1680: loss 2.2527, time 20.76ms, mfu 0.16%
iter 1685: loss 2.3383, time 15.91ms, mfu 0.16%
iter 1690: loss 2.2769, time 17.39ms, mfu 0.16%
iter 1695: loss 2.3284, time 16.43ms, mfu 0.16%
iter 1700: loss 2.2856, time 15.95ms, mfu 0.16%
iter 1705: loss 2.2413, time 21.83ms, mfu 0.16%
iter 1710: loss 2.2573, time 16.29ms, mfu 0.16%
iter 1715: loss 2.3530, time 17.02ms, mfu 0.16%
iter 1720: loss 2.2132, time 16.90ms, mfu 0.16%
iter 1725: loss 2.2798, time 15.78ms, mfu 0.16%
iter 1730: loss 2.2082, time 16.64ms, mfu 0.16%
iter 1735: loss 2.2507, time 15.48ms, mfu 0.16%
iter 1740: loss 2.3174, time 23.48ms, mfu 0.16%
iter 1745: loss 2.3160, time 16.32ms, mfu 0.16%
step 1750: train loss 2.2457, val loss 2.2548
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size8_max_iters2000_dropout0.2
iter 1750: loss 2.2595, time 330.47ms, mfu 0.14%
iter 1755: loss 2.2213, time 15.75ms, mfu 0.15%
iter 1760: loss 2.3084, time 16.81ms, mfu 0.15%
iter 1765: loss 2.3314, time 15.69ms, mfu 0.15%
iter 1770: loss 2.2695, time 16.08ms, mfu 0.15%
iter 1775: loss 2.3488, time 17.13ms, mfu 0.15%
iter 1780: loss 2.2885, time 17.16ms, mfu 0.15%
iter 1785: loss 2.2284, time 19.51ms, mfu 0.15%
iter 1790: loss 2.2770, time 15.79ms, mfu 0.15%
iter 1795: loss 2.2669, time 28.48ms, mfu 0.15%
iter 1800: loss 2.2946, time 27.69ms, mfu 0.14%
iter 1805: loss 2.3147, time 20.90ms, mfu 0.14%
iter 1810: loss 2.3900, time 23.03ms, mfu 0.14%
iter 1815: loss 2.2590, time 21.77ms, mfu 0.14%
iter 1820: loss 2.2507, time 27.53ms, mfu 0.13%
iter 1825: loss 2.2811, time 20.81ms, mfu 0.13%
iter 1830: loss 2.2186, time 24.32ms, mfu 0.13%
iter 1835: loss 2.3007, time 21.87ms, mfu 0.13%
iter 1840: loss 2.2480, time 19.92ms, mfu 0.13%
iter 1845: loss 2.3796, time 25.32ms, mfu 0.13%
iter 1850: loss 2.3302, time 20.24ms, mfu 0.13%
iter 1855: loss 2.2789, time 22.85ms, mfu 0.13%
iter 1860: loss 2.2797, time 22.62ms, mfu 0.13%
iter 1865: loss 2.2881, time 29.95ms, mfu 0.12%
iter 1870: loss 2.3030, time 20.51ms, mfu 0.13%
iter 1875: loss 2.3770, time 21.65ms, mfu 0.13%
iter 1880: loss 2.2525, time 23.56ms, mfu 0.12%
iter 1885: loss 2.3443, time 23.31ms, mfu 0.12%
iter 1890: loss 2.2799, time 20.02ms, mfu 0.12%
iter 1895: loss 2.3201, time 30.39ms, mfu 0.12%
iter 1900: loss 2.2664, time 32.72ms, mfu 0.12%
iter 1905: loss 2.3324, time 25.45ms, mfu 0.12%
iter 1910: loss 2.2802, time 28.30ms, mfu 0.11%
iter 1915: loss 2.2228, time 24.80ms, mfu 0.11%
iter 1920: loss 2.2971, time 16.19ms, mfu 0.12%
iter 1925: loss 2.2647, time 15.98ms, mfu 0.12%
iter 1930: loss 2.2564, time 16.78ms, mfu 0.13%
iter 1935: loss 2.2237, time 18.52ms, mfu 0.13%
iter 1940: loss 2.3365, time 15.74ms, mfu 0.13%
iter 1945: loss 2.2436, time 15.98ms, mfu 0.14%
iter 1950: loss 2.2651, time 18.63ms, mfu 0.14%
iter 1955: loss 2.2740, time 16.54ms, mfu 0.14%
iter 1960: loss 2.2325, time 16.85ms, mfu 0.14%
iter 1965: loss 2.1784, time 15.75ms, mfu 0.15%
iter 1970: loss 2.3456, time 17.54ms, mfu 0.15%
iter 1975: loss 2.3375, time 15.73ms, mfu 0.15%
iter 1980: loss 2.2367, time 16.06ms, mfu 0.15%
iter 1985: loss 2.2061, time 17.85ms, mfu 0.15%
iter 1990: loss 2.3154, time 19.65ms, mfu 0.15%
iter 1995: loss 2.3534, time 16.30ms, mfu 0.15%
step 2000: train loss 2.2117, val loss 2.2255
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size8_max_iters2000_dropout0.2
iter 2000: loss 2.1835, time 304.78ms, mfu 0.14%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd128_batch_size8_max_iters2000_dropout0.2 at: 
wandb: Find logs at: wandb/run-20251024_005251-ii2rl3da/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 128
Overriding: batch_size = 16
Overriding: max_iters = 1000
Overriding: dropout = 0.1
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.1
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.1
tokens per iteration will be: 2,048
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 1.19M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 1,204,352 parameters
num non-decayed parameter tensors: 13, with 1,664 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_005339-8kxjewdd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.1
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/8kxjewdd
step 0: train loss 4.1954, val loss 4.1906
iter 0: loss 4.1939, time 946.68ms, mfu -100.00%
iter 5: loss 3.9568, time 24.45ms, mfu 0.22%
iter 10: loss 3.7577, time 24.18ms, mfu 0.22%
iter 15: loss 3.6678, time 22.06ms, mfu 0.23%
iter 20: loss 3.5227, time 21.91ms, mfu 0.23%
iter 25: loss 3.3652, time 22.70ms, mfu 0.23%
iter 30: loss 3.2194, time 22.05ms, mfu 0.23%
iter 35: loss 3.1667, time 21.92ms, mfu 0.23%
iter 40: loss 3.0345, time 20.59ms, mfu 0.24%
iter 45: loss 2.9339, time 20.24ms, mfu 0.24%
iter 50: loss 2.8562, time 20.73ms, mfu 0.24%
iter 55: loss 2.7828, time 24.28ms, mfu 0.24%
iter 60: loss 2.7530, time 26.20ms, mfu 0.24%
iter 65: loss 2.7335, time 21.39ms, mfu 0.24%
iter 70: loss 2.6578, time 21.75ms, mfu 0.24%
iter 75: loss 2.6557, time 24.21ms, mfu 0.24%
iter 80: loss 2.6354, time 21.95ms, mfu 0.24%
iter 85: loss 2.6099, time 28.86ms, mfu 0.23%
iter 90: loss 2.6743, time 23.54ms, mfu 0.23%
iter 95: loss 2.6275, time 28.16ms, mfu 0.23%
iter 100: loss 2.6354, time 29.22ms, mfu 0.23%
iter 105: loss 2.6227, time 26.50ms, mfu 0.22%
iter 110: loss 2.5427, time 25.62ms, mfu 0.22%
iter 115: loss 2.6228, time 27.26ms, mfu 0.22%
iter 120: loss 2.4929, time 16.61ms, mfu 0.23%
iter 125: loss 2.5320, time 15.71ms, mfu 0.24%
iter 130: loss 2.5608, time 18.05ms, mfu 0.25%
iter 135: loss 2.5542, time 17.46ms, mfu 0.26%
iter 140: loss 2.5375, time 16.82ms, mfu 0.26%
iter 145: loss 2.5549, time 24.50ms, mfu 0.26%
iter 150: loss 2.5004, time 16.72ms, mfu 0.27%
iter 155: loss 2.5255, time 17.05ms, mfu 0.27%
iter 160: loss 2.5140, time 20.69ms, mfu 0.27%
iter 165: loss 2.5504, time 17.12ms, mfu 0.27%
iter 170: loss 2.5064, time 18.83ms, mfu 0.28%
iter 175: loss 2.5600, time 16.31ms, mfu 0.28%
iter 180: loss 2.4966, time 23.62ms, mfu 0.28%
iter 185: loss 2.4572, time 16.80ms, mfu 0.28%
iter 190: loss 2.4329, time 19.87ms, mfu 0.28%
iter 195: loss 2.4778, time 16.26ms, mfu 0.29%
iter 200: loss 2.4913, time 20.79ms, mfu 0.28%
iter 205: loss 2.5070, time 16.82ms, mfu 0.29%
iter 210: loss 2.4619, time 16.37ms, mfu 0.29%
iter 215: loss 2.4344, time 28.31ms, mfu 0.28%
iter 220: loss 2.4868, time 23.73ms, mfu 0.28%
iter 225: loss 2.4609, time 23.22ms, mfu 0.27%
iter 230: loss 2.4277, time 16.70ms, mfu 0.28%
iter 235: loss 2.4554, time 16.44ms, mfu 0.28%
iter 240: loss 2.4277, time 17.10ms, mfu 0.29%
iter 245: loss 2.4849, time 16.58ms, mfu 0.29%
step 250: train loss 2.4342, val loss 2.4260
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.1
iter 250: loss 2.4230, time 368.95ms, mfu 0.26%
iter 255: loss 2.4614, time 16.79ms, mfu 0.27%
iter 260: loss 2.4616, time 18.51ms, mfu 0.27%
iter 265: loss 2.4005, time 16.40ms, mfu 0.28%
iter 270: loss 2.4603, time 17.59ms, mfu 0.28%
iter 275: loss 2.4346, time 17.03ms, mfu 0.29%
iter 280: loss 2.4116, time 16.54ms, mfu 0.29%
iter 285: loss 2.4182, time 19.95ms, mfu 0.29%
iter 290: loss 2.4965, time 16.40ms, mfu 0.29%
iter 295: loss 2.4580, time 20.05ms, mfu 0.29%
iter 300: loss 2.4126, time 19.03ms, mfu 0.29%
iter 305: loss 2.4700, time 22.84ms, mfu 0.29%
iter 310: loss 2.4160, time 16.55ms, mfu 0.29%
iter 315: loss 2.4333, time 17.62ms, mfu 0.29%
iter 320: loss 2.4513, time 23.51ms, mfu 0.29%
iter 325: loss 2.4968, time 19.17ms, mfu 0.29%
iter 330: loss 2.4587, time 18.46ms, mfu 0.29%
iter 335: loss 2.3688, time 18.64ms, mfu 0.29%
iter 340: loss 2.4229, time 17.80ms, mfu 0.29%
iter 345: loss 2.3881, time 18.08ms, mfu 0.29%
iter 350: loss 2.4364, time 29.83ms, mfu 0.28%
iter 355: loss 2.4140, time 17.17ms, mfu 0.28%
iter 360: loss 2.3051, time 18.55ms, mfu 0.28%
iter 365: loss 2.4422, time 17.14ms, mfu 0.29%
iter 370: loss 2.3754, time 17.01ms, mfu 0.29%
iter 375: loss 2.3775, time 22.70ms, mfu 0.29%
iter 380: loss 2.3459, time 16.78ms, mfu 0.29%
iter 385: loss 2.3695, time 19.62ms, mfu 0.29%
iter 390: loss 2.3146, time 17.01ms, mfu 0.29%
iter 395: loss 2.3837, time 19.68ms, mfu 0.29%
iter 400: loss 2.3785, time 22.16ms, mfu 0.29%
iter 405: loss 2.3718, time 17.06ms, mfu 0.29%
iter 410: loss 2.3969, time 21.28ms, mfu 0.29%
iter 415: loss 2.3560, time 18.82ms, mfu 0.29%
iter 420: loss 2.3567, time 18.37ms, mfu 0.29%
iter 425: loss 2.3379, time 18.18ms, mfu 0.29%
iter 430: loss 2.3366, time 16.72ms, mfu 0.29%
iter 435: loss 2.3525, time 24.54ms, mfu 0.29%
iter 440: loss 2.3198, time 16.69ms, mfu 0.29%
iter 445: loss 2.3148, time 21.59ms, mfu 0.29%
iter 450: loss 2.3368, time 17.75ms, mfu 0.29%
iter 455: loss 2.3029, time 18.40ms, mfu 0.29%
iter 460: loss 2.3267, time 20.48ms, mfu 0.29%
iter 465: loss 2.3447, time 17.78ms, mfu 0.29%
iter 470: loss 2.3810, time 23.96ms, mfu 0.28%
iter 475: loss 2.2987, time 17.72ms, mfu 0.29%
iter 480: loss 2.2667, time 23.19ms, mfu 0.28%
iter 485: loss 2.3104, time 17.83ms, mfu 0.28%
iter 490: loss 2.2715, time 17.98ms, mfu 0.29%
iter 495: loss 2.3007, time 22.40ms, mfu 0.28%
step 500: train loss 2.2480, val loss 2.2631
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.1
iter 500: loss 2.2778, time 374.56ms, mfu 0.25%
iter 505: loss 2.2859, time 16.30ms, mfu 0.26%
iter 510: loss 2.2772, time 16.29ms, mfu 0.27%
iter 515: loss 2.3000, time 15.92ms, mfu 0.28%
iter 520: loss 2.2928, time 16.90ms, mfu 0.28%
iter 525: loss 2.3183, time 16.38ms, mfu 0.29%
iter 530: loss 2.2752, time 16.26ms, mfu 0.29%
iter 535: loss 2.2706, time 16.11ms, mfu 0.30%
iter 540: loss 2.2530, time 20.30ms, mfu 0.29%
iter 545: loss 2.2697, time 17.02ms, mfu 0.30%
iter 550: loss 2.2192, time 16.71ms, mfu 0.30%
iter 555: loss 2.2513, time 16.40ms, mfu 0.30%
iter 560: loss 2.2356, time 16.58ms, mfu 0.31%
iter 565: loss 2.2643, time 16.35ms, mfu 0.31%
iter 570: loss 2.2182, time 17.74ms, mfu 0.31%
iter 575: loss 2.2357, time 17.05ms, mfu 0.31%
iter 580: loss 2.2251, time 26.56ms, mfu 0.30%
iter 585: loss 2.2294, time 25.16ms, mfu 0.29%
iter 590: loss 2.2198, time 23.19ms, mfu 0.29%
iter 595: loss 2.2263, time 20.52ms, mfu 0.28%
iter 600: loss 2.2702, time 21.98ms, mfu 0.28%
iter 605: loss 2.2357, time 21.64ms, mfu 0.28%
iter 610: loss 2.2518, time 21.79ms, mfu 0.27%
iter 615: loss 2.2347, time 21.34ms, mfu 0.27%
iter 620: loss 2.2632, time 26.54ms, mfu 0.27%
iter 625: loss 2.2240, time 20.71ms, mfu 0.27%
iter 630: loss 2.2473, time 21.24ms, mfu 0.26%
iter 635: loss 2.2643, time 20.95ms, mfu 0.26%
iter 640: loss 2.1985, time 25.94ms, mfu 0.26%
iter 645: loss 2.2154, time 22.06ms, mfu 0.26%
iter 650: loss 2.2361, time 21.24ms, mfu 0.26%
iter 655: loss 2.2095, time 21.13ms, mfu 0.26%
iter 660: loss 2.1941, time 25.13ms, mfu 0.25%
iter 665: loss 2.2356, time 27.25ms, mfu 0.25%
iter 670: loss 2.2634, time 22.84ms, mfu 0.25%
iter 675: loss 2.2543, time 32.79ms, mfu 0.24%
iter 680: loss 2.2114, time 28.15ms, mfu 0.23%
iter 685: loss 2.2029, time 27.56ms, mfu 0.23%
iter 690: loss 2.1963, time 25.83ms, mfu 0.23%
iter 695: loss 2.1823, time 28.23ms, mfu 0.23%
iter 700: loss 2.1999, time 19.82ms, mfu 0.23%
iter 705: loss 2.2470, time 16.60ms, mfu 0.24%
iter 710: loss 2.1145, time 16.66ms, mfu 0.25%
iter 715: loss 2.1572, time 16.42ms, mfu 0.26%
iter 720: loss 2.2035, time 16.90ms, mfu 0.26%
iter 725: loss 2.2181, time 16.54ms, mfu 0.27%
iter 730: loss 2.1462, time 16.97ms, mfu 0.28%
iter 735: loss 2.2266, time 16.40ms, mfu 0.28%
iter 740: loss 2.1736, time 15.83ms, mfu 0.29%
iter 745: loss 2.2386, time 18.52ms, mfu 0.29%
step 750: train loss 2.1004, val loss 2.1347
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.1
iter 750: loss 2.2473, time 363.73ms, mfu 0.26%
iter 755: loss 2.1526, time 16.20ms, mfu 0.27%
iter 760: loss 2.0870, time 16.44ms, mfu 0.28%
iter 765: loss 2.1997, time 16.39ms, mfu 0.28%
iter 770: loss 2.1072, time 16.13ms, mfu 0.29%
iter 775: loss 2.1590, time 18.15ms, mfu 0.29%
iter 780: loss 2.1769, time 16.54ms, mfu 0.29%
iter 785: loss 2.1445, time 16.50ms, mfu 0.30%
iter 790: loss 2.1694, time 16.37ms, mfu 0.30%
iter 795: loss 2.1089, time 18.02ms, mfu 0.30%
iter 800: loss 2.1899, time 16.25ms, mfu 0.30%
iter 805: loss 2.1171, time 17.37ms, mfu 0.30%
iter 810: loss 2.1007, time 17.18ms, mfu 0.31%
iter 815: loss 2.1760, time 17.34ms, mfu 0.31%
iter 820: loss 2.0815, time 16.76ms, mfu 0.31%
iter 825: loss 2.1402, time 17.04ms, mfu 0.31%
iter 830: loss 2.2091, time 18.20ms, mfu 0.31%
iter 835: loss 2.1041, time 16.56ms, mfu 0.31%
iter 840: loss 2.1332, time 17.69ms, mfu 0.31%
iter 845: loss 2.0899, time 16.07ms, mfu 0.31%
iter 850: loss 2.1565, time 16.70ms, mfu 0.32%
iter 855: loss 2.1104, time 24.70ms, mfu 0.31%
iter 860: loss 2.1698, time 17.08ms, mfu 0.31%
iter 865: loss 2.1556, time 16.90ms, mfu 0.31%
iter 870: loss 2.1367, time 22.27ms, mfu 0.30%
iter 875: loss 2.1585, time 17.28ms, mfu 0.30%
iter 880: loss 2.1471, time 16.47ms, mfu 0.31%
iter 885: loss 2.1950, time 16.79ms, mfu 0.31%
iter 890: loss 2.1058, time 17.95ms, mfu 0.31%
iter 895: loss 2.0938, time 23.43ms, mfu 0.30%
iter 900: loss 2.1555, time 16.97ms, mfu 0.30%
iter 905: loss 2.1832, time 22.00ms, mfu 0.30%
iter 910: loss 2.1803, time 17.44ms, mfu 0.30%
iter 915: loss 2.0621, time 19.47ms, mfu 0.30%
iter 920: loss 2.0434, time 16.46ms, mfu 0.30%
iter 925: loss 2.1344, time 17.00ms, mfu 0.30%
iter 930: loss 2.1670, time 20.85ms, mfu 0.30%
iter 935: loss 2.1671, time 16.50ms, mfu 0.30%
iter 940: loss 2.0887, time 16.32ms, mfu 0.30%
iter 945: loss 2.0937, time 15.95ms, mfu 0.31%
iter 950: loss 2.1230, time 16.60ms, mfu 0.31%
iter 955: loss 2.1128, time 16.54ms, mfu 0.31%
iter 960: loss 2.1212, time 17.08ms, mfu 0.31%
iter 965: loss 2.0607, time 17.29ms, mfu 0.31%
iter 970: loss 2.0682, time 16.16ms, mfu 0.32%
iter 975: loss 2.1255, time 16.15ms, mfu 0.32%
iter 980: loss 2.0747, time 16.59ms, mfu 0.32%
iter 985: loss 2.1670, time 16.70ms, mfu 0.32%
iter 990: loss 2.0736, time 16.45ms, mfu 0.32%
iter 995: loss 2.0986, time 16.52ms, mfu 0.32%
step 1000: train loss 2.0390, val loss 2.0851
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.1
iter 1000: loss 2.0776, time 373.73ms, mfu 0.29%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.1 at: 
wandb: Find logs at: wandb/run-20251024_005339-8kxjewdd/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 128
Overriding: batch_size = 16
Overriding: max_iters = 1000
Overriding: dropout = 0.2
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.2
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.2
tokens per iteration will be: 2,048
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 1.19M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 1,204,352 parameters
num non-decayed parameter tensors: 13, with 1,664 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_005411-onvta4qv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.2
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/onvta4qv
step 0: train loss 4.1954, val loss 4.1906
iter 0: loss 4.1957, time 892.01ms, mfu -100.00%
iter 5: loss 3.9899, time 23.92ms, mfu 0.23%
iter 10: loss 3.7746, time 17.12ms, mfu 0.24%
iter 15: loss 3.6939, time 17.64ms, mfu 0.24%
iter 20: loss 3.5557, time 16.41ms, mfu 0.25%
iter 25: loss 3.3984, time 17.13ms, mfu 0.26%
iter 30: loss 3.2589, time 17.27ms, mfu 0.27%
iter 35: loss 3.2017, time 18.23ms, mfu 0.27%
iter 40: loss 3.0624, time 16.22ms, mfu 0.28%
iter 45: loss 2.9666, time 16.89ms, mfu 0.28%
iter 50: loss 2.8813, time 16.45ms, mfu 0.29%
iter 55: loss 2.8100, time 17.48ms, mfu 0.29%
iter 60: loss 2.7808, time 16.43ms, mfu 0.29%
iter 65: loss 2.7650, time 16.31ms, mfu 0.30%
iter 70: loss 2.6759, time 16.60ms, mfu 0.30%
iter 75: loss 2.6793, time 16.71ms, mfu 0.30%
iter 80: loss 2.6569, time 16.31ms, mfu 0.31%
iter 85: loss 2.6330, time 16.12ms, mfu 0.31%
iter 90: loss 2.6998, time 16.25ms, mfu 0.31%
iter 95: loss 2.6538, time 16.47ms, mfu 0.31%
iter 100: loss 2.6526, time 16.97ms, mfu 0.31%
iter 105: loss 2.6214, time 16.78ms, mfu 0.32%
iter 110: loss 2.5723, time 22.99ms, mfu 0.31%
iter 115: loss 2.6407, time 16.21ms, mfu 0.31%
iter 120: loss 2.5180, time 16.68ms, mfu 0.31%
iter 125: loss 2.5521, time 17.12ms, mfu 0.31%
iter 130: loss 2.5774, time 17.22ms, mfu 0.31%
iter 135: loss 2.5836, time 17.75ms, mfu 0.31%
iter 140: loss 2.5543, time 17.73ms, mfu 0.31%
iter 145: loss 2.5737, time 16.95ms, mfu 0.31%
iter 150: loss 2.5297, time 16.46ms, mfu 0.32%
iter 155: loss 2.5419, time 27.02ms, mfu 0.30%
iter 160: loss 2.5220, time 24.72ms, mfu 0.30%
iter 165: loss 2.5632, time 25.95ms, mfu 0.29%
iter 170: loss 2.5178, time 22.00ms, mfu 0.28%
iter 175: loss 2.5769, time 21.64ms, mfu 0.28%
iter 180: loss 2.5041, time 22.65ms, mfu 0.28%
iter 185: loss 2.4812, time 23.02ms, mfu 0.27%
iter 190: loss 2.4552, time 22.34ms, mfu 0.27%
iter 195: loss 2.5032, time 24.65ms, mfu 0.26%
iter 200: loss 2.5151, time 21.85ms, mfu 0.26%
iter 205: loss 2.5257, time 27.03ms, mfu 0.26%
iter 210: loss 2.4652, time 23.24ms, mfu 0.25%
iter 215: loss 2.4535, time 24.70ms, mfu 0.25%
iter 220: loss 2.5153, time 20.51ms, mfu 0.25%
iter 225: loss 2.4888, time 21.77ms, mfu 0.25%
iter 230: loss 2.4517, time 25.57ms, mfu 0.25%
iter 235: loss 2.4672, time 23.17ms, mfu 0.25%
iter 240: loss 2.4578, time 21.15ms, mfu 0.25%
iter 245: loss 2.5068, time 20.14ms, mfu 0.25%
step 250: train loss 2.4607, val loss 2.4492
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.2
iter 250: loss 2.4610, time 457.28ms, mfu 0.23%
iter 255: loss 2.5027, time 22.72ms, mfu 0.23%
iter 260: loss 2.4991, time 28.73ms, mfu 0.22%
iter 265: loss 2.4312, time 25.57ms, mfu 0.22%
iter 270: loss 2.4967, time 34.39ms, mfu 0.22%
iter 275: loss 2.4738, time 30.86ms, mfu 0.21%
iter 280: loss 2.4461, time 18.41ms, mfu 0.22%
iter 285: loss 2.4520, time 29.60ms, mfu 0.22%
iter 290: loss 2.5187, time 24.30ms, mfu 0.22%
iter 295: loss 2.4751, time 21.68ms, mfu 0.22%
iter 300: loss 2.4460, time 21.37ms, mfu 0.23%
iter 305: loss 2.5044, time 22.35ms, mfu 0.23%
iter 310: loss 2.4446, time 22.52ms, mfu 0.23%
iter 315: loss 2.4667, time 21.23ms, mfu 0.23%
iter 320: loss 2.4933, time 24.52ms, mfu 0.23%
iter 325: loss 2.5208, time 21.30ms, mfu 0.23%
iter 330: loss 2.4976, time 20.43ms, mfu 0.24%
iter 335: loss 2.4276, time 21.67ms, mfu 0.24%
iter 340: loss 2.4576, time 20.60ms, mfu 0.24%
iter 345: loss 2.4231, time 22.19ms, mfu 0.24%
iter 350: loss 2.4767, time 21.74ms, mfu 0.24%
iter 355: loss 2.4698, time 29.17ms, mfu 0.24%
iter 360: loss 2.3658, time 21.59ms, mfu 0.24%
iter 365: loss 2.4796, time 25.48ms, mfu 0.24%
iter 370: loss 2.4216, time 27.30ms, mfu 0.23%
iter 375: loss 2.4159, time 21.71ms, mfu 0.23%
iter 380: loss 2.3997, time 24.12ms, mfu 0.23%
iter 385: loss 2.4257, time 24.09ms, mfu 0.23%
iter 390: loss 2.3812, time 29.63ms, mfu 0.23%
iter 395: loss 2.4429, time 27.09ms, mfu 0.23%
iter 400: loss 2.4174, time 16.11ms, mfu 0.24%
iter 405: loss 2.4114, time 16.65ms, mfu 0.25%
iter 410: loss 2.4450, time 16.00ms, mfu 0.26%
iter 415: loss 2.4033, time 16.14ms, mfu 0.26%
iter 420: loss 2.3950, time 17.19ms, mfu 0.27%
iter 425: loss 2.3933, time 15.58ms, mfu 0.28%
iter 430: loss 2.4026, time 15.84ms, mfu 0.28%
iter 435: loss 2.3959, time 15.91ms, mfu 0.29%
iter 440: loss 2.3865, time 16.38ms, mfu 0.29%
iter 445: loss 2.3870, time 16.33ms, mfu 0.30%
iter 450: loss 2.4088, time 16.75ms, mfu 0.30%
iter 455: loss 2.3799, time 18.17ms, mfu 0.30%
iter 460: loss 2.3980, time 23.82ms, mfu 0.29%
iter 465: loss 2.4049, time 17.04ms, mfu 0.30%
iter 470: loss 2.4408, time 15.82ms, mfu 0.30%
iter 475: loss 2.3605, time 16.80ms, mfu 0.30%
iter 480: loss 2.3405, time 16.31ms, mfu 0.31%
iter 485: loss 2.3902, time 15.93ms, mfu 0.31%
iter 490: loss 2.3393, time 18.37ms, mfu 0.31%
iter 495: loss 2.3534, time 16.67ms, mfu 0.31%
step 500: train loss 2.3114, val loss 2.3171
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.2
iter 500: loss 2.3487, time 387.47ms, mfu 0.28%
iter 505: loss 2.3714, time 16.84ms, mfu 0.29%
iter 510: loss 2.3492, time 16.56ms, mfu 0.29%
iter 515: loss 2.3792, time 16.66ms, mfu 0.29%
iter 520: loss 2.3559, time 16.20ms, mfu 0.30%
iter 525: loss 2.3736, time 16.79ms, mfu 0.30%
iter 530: loss 2.3448, time 21.40ms, mfu 0.30%
iter 535: loss 2.3429, time 16.20ms, mfu 0.30%
iter 540: loss 2.3301, time 17.38ms, mfu 0.30%
iter 545: loss 2.3476, time 18.45ms, mfu 0.30%
iter 550: loss 2.3102, time 17.44ms, mfu 0.30%
iter 555: loss 2.3470, time 17.31ms, mfu 0.30%
iter 560: loss 2.3326, time 18.64ms, mfu 0.30%
iter 565: loss 2.3383, time 16.42ms, mfu 0.31%
iter 570: loss 2.3008, time 31.06ms, mfu 0.29%
iter 575: loss 2.3221, time 34.81ms, mfu 0.28%
iter 580: loss 2.3084, time 31.11ms, mfu 0.27%
iter 585: loss 2.2997, time 16.36ms, mfu 0.28%
iter 590: loss 2.3038, time 16.89ms, mfu 0.28%
iter 595: loss 2.3146, time 16.31ms, mfu 0.29%
iter 600: loss 2.3258, time 16.72ms, mfu 0.29%
iter 605: loss 2.3361, time 16.04ms, mfu 0.29%
iter 610: loss 2.3221, time 16.18ms, mfu 0.30%
iter 615: loss 2.3000, time 17.03ms, mfu 0.30%
iter 620: loss 2.3509, time 17.16ms, mfu 0.30%
iter 625: loss 2.3009, time 17.18ms, mfu 0.30%
iter 630: loss 2.3207, time 18.36ms, mfu 0.30%
iter 635: loss 2.3408, time 16.61ms, mfu 0.31%
iter 640: loss 2.2764, time 19.14ms, mfu 0.30%
iter 645: loss 2.3145, time 16.22ms, mfu 0.31%
iter 650: loss 2.3104, time 15.92ms, mfu 0.31%
iter 655: loss 2.2819, time 16.02ms, mfu 0.31%
iter 660: loss 2.2715, time 16.05ms, mfu 0.32%
iter 665: loss 2.3170, time 16.80ms, mfu 0.32%
iter 670: loss 2.3525, time 16.11ms, mfu 0.32%
iter 675: loss 2.3154, time 16.10ms, mfu 0.32%
iter 680: loss 2.2918, time 18.14ms, mfu 0.32%
iter 685: loss 2.2840, time 16.60ms, mfu 0.32%
iter 690: loss 2.2775, time 16.07ms, mfu 0.32%
iter 695: loss 2.2624, time 16.51ms, mfu 0.32%
iter 700: loss 2.2955, time 16.70ms, mfu 0.32%
iter 705: loss 2.3297, time 16.54ms, mfu 0.32%
iter 710: loss 2.2149, time 16.10ms, mfu 0.33%
iter 715: loss 2.2456, time 16.02ms, mfu 0.33%
iter 720: loss 2.2982, time 16.77ms, mfu 0.33%
iter 725: loss 2.3049, time 16.54ms, mfu 0.33%
iter 730: loss 2.2398, time 18.48ms, mfu 0.32%
iter 735: loss 2.3300, time 17.24ms, mfu 0.32%
iter 740: loss 2.2724, time 16.91ms, mfu 0.32%
iter 745: loss 2.3407, time 16.78ms, mfu 0.32%
step 750: train loss 2.1834, val loss 2.2025
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.2
iter 750: loss 2.3322, time 356.63ms, mfu 0.29%
iter 755: loss 2.2547, time 18.67ms, mfu 0.29%
iter 760: loss 2.1876, time 18.16ms, mfu 0.29%
iter 765: loss 2.2997, time 16.43ms, mfu 0.30%
iter 770: loss 2.2157, time 16.52ms, mfu 0.30%
iter 775: loss 2.2471, time 16.42ms, mfu 0.30%
iter 780: loss 2.2676, time 15.89ms, mfu 0.31%
iter 785: loss 2.2359, time 15.67ms, mfu 0.31%
iter 790: loss 2.2601, time 16.51ms, mfu 0.31%
iter 795: loss 2.2196, time 17.03ms, mfu 0.31%
iter 800: loss 2.2881, time 19.96ms, mfu 0.31%
iter 805: loss 2.2095, time 16.77ms, mfu 0.31%
iter 810: loss 2.1996, time 16.39ms, mfu 0.31%
iter 815: loss 2.2704, time 17.03ms, mfu 0.31%
iter 820: loss 2.1898, time 22.19ms, mfu 0.31%
iter 825: loss 2.2459, time 16.30ms, mfu 0.31%
iter 830: loss 2.2923, time 16.66ms, mfu 0.31%
iter 835: loss 2.2042, time 16.00ms, mfu 0.32%
iter 840: loss 2.2232, time 16.21ms, mfu 0.32%
iter 845: loss 2.1900, time 16.58ms, mfu 0.32%
iter 850: loss 2.2553, time 16.67ms, mfu 0.32%
iter 855: loss 2.2136, time 29.49ms, mfu 0.31%
iter 860: loss 2.2618, time 21.65ms, mfu 0.30%
iter 865: loss 2.2627, time 23.73ms, mfu 0.29%
iter 870: loss 2.2412, time 21.44ms, mfu 0.29%
iter 875: loss 2.2594, time 24.50ms, mfu 0.28%
iter 880: loss 2.2495, time 21.37ms, mfu 0.28%
iter 885: loss 2.2886, time 25.73ms, mfu 0.27%
iter 890: loss 2.2144, time 21.52ms, mfu 0.27%
iter 895: loss 2.2229, time 20.63ms, mfu 0.27%
iter 900: loss 2.2555, time 20.81ms, mfu 0.27%
iter 905: loss 2.2892, time 20.61ms, mfu 0.27%
iter 910: loss 2.2717, time 23.93ms, mfu 0.27%
iter 915: loss 2.1714, time 22.50ms, mfu 0.26%
iter 920: loss 2.1614, time 29.21ms, mfu 0.26%
iter 925: loss 2.2291, time 20.61ms, mfu 0.26%
iter 930: loss 2.2728, time 23.41ms, mfu 0.25%
iter 935: loss 2.2724, time 21.46ms, mfu 0.25%
iter 940: loss 2.2005, time 25.18ms, mfu 0.25%
iter 945: loss 2.2038, time 25.73ms, mfu 0.25%
iter 950: loss 2.2175, time 28.61ms, mfu 0.24%
iter 955: loss 2.2243, time 33.19ms, mfu 0.23%
iter 960: loss 2.2134, time 25.73ms, mfu 0.23%
iter 965: loss 2.1574, time 24.82ms, mfu 0.23%
iter 970: loss 2.1613, time 27.18ms, mfu 0.23%
iter 975: loss 2.2461, time 16.67ms, mfu 0.24%
iter 980: loss 2.1728, time 16.56ms, mfu 0.25%
iter 985: loss 2.2747, time 16.49ms, mfu 0.25%
iter 990: loss 2.1887, time 17.42ms, mfu 0.26%
iter 995: loss 2.2059, time 16.02ms, mfu 0.27%
step 1000: train loss 2.1357, val loss 2.1670
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.2
iter 1000: loss 2.1845, time 412.54ms, mfu 0.24%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters1000_dropout0.2 at: 
wandb: Find logs at: wandb/run-20251024_005411-onvta4qv/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 128
Overriding: batch_size = 16
Overriding: max_iters = 2000
Overriding: dropout = 0.1
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
tokens per iteration will be: 2,048
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 1.19M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 1,204,352 parameters
num non-decayed parameter tensors: 13, with 1,664 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: ⣽ setting up run 3te41fbn (0.3s)
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_005443-3te41fbn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/3te41fbn
step 0: train loss 4.1954, val loss 4.1906
iter 0: loss 4.1939, time 896.00ms, mfu -100.00%
iter 5: loss 3.9568, time 17.11ms, mfu 0.32%
iter 10: loss 3.7577, time 19.89ms, mfu 0.31%
iter 15: loss 3.6678, time 18.84ms, mfu 0.31%
iter 20: loss 3.5227, time 16.86ms, mfu 0.31%
iter 25: loss 3.3652, time 21.42ms, mfu 0.31%
iter 30: loss 3.2194, time 17.00ms, mfu 0.31%
iter 35: loss 3.1667, time 16.95ms, mfu 0.31%
iter 40: loss 3.0345, time 33.11ms, mfu 0.30%
iter 45: loss 2.9339, time 26.28ms, mfu 0.29%
iter 50: loss 2.8562, time 26.17ms, mfu 0.28%
iter 55: loss 2.7828, time 24.36ms, mfu 0.27%
iter 60: loss 2.7530, time 21.29ms, mfu 0.27%
iter 65: loss 2.7335, time 22.08ms, mfu 0.27%
iter 70: loss 2.6578, time 22.46ms, mfu 0.27%
iter 75: loss 2.6557, time 24.51ms, mfu 0.26%
iter 80: loss 2.6354, time 19.74ms, mfu 0.26%
iter 85: loss 2.6099, time 21.80ms, mfu 0.26%
iter 90: loss 2.6743, time 26.46ms, mfu 0.26%
iter 95: loss 2.6275, time 23.41ms, mfu 0.25%
iter 100: loss 2.6354, time 25.26ms, mfu 0.25%
iter 105: loss 2.6227, time 25.52ms, mfu 0.25%
iter 110: loss 2.5427, time 28.06ms, mfu 0.24%
iter 115: loss 2.6228, time 22.19ms, mfu 0.24%
iter 120: loss 2.4929, time 21.29ms, mfu 0.24%
iter 125: loss 2.5320, time 30.43ms, mfu 0.24%
iter 130: loss 2.5608, time 23.83ms, mfu 0.24%
iter 135: loss 2.5542, time 21.70ms, mfu 0.24%
iter 140: loss 2.5375, time 21.54ms, mfu 0.24%
iter 145: loss 2.5549, time 43.05ms, mfu 0.23%
iter 150: loss 2.5004, time 23.01ms, mfu 0.23%
iter 155: loss 2.5255, time 24.59ms, mfu 0.23%
iter 160: loss 2.5140, time 21.63ms, mfu 0.23%
iter 165: loss 2.5504, time 37.68ms, mfu 0.22%
iter 170: loss 2.5064, time 28.82ms, mfu 0.22%
iter 175: loss 2.5600, time 26.33ms, mfu 0.22%
iter 180: loss 2.4966, time 25.23ms, mfu 0.22%
iter 185: loss 2.4572, time 25.91ms, mfu 0.22%
iter 190: loss 2.4329, time 25.11ms, mfu 0.22%
iter 195: loss 2.4778, time 26.84ms, mfu 0.22%
iter 200: loss 2.4913, time 31.72ms, mfu 0.21%
iter 205: loss 2.5070, time 25.27ms, mfu 0.21%
iter 210: loss 2.4619, time 16.20ms, mfu 0.22%
iter 215: loss 2.4344, time 15.98ms, mfu 0.24%
iter 220: loss 2.4868, time 16.58ms, mfu 0.25%
iter 225: loss 2.4609, time 16.78ms, mfu 0.25%
iter 230: loss 2.4277, time 17.84ms, mfu 0.26%
iter 235: loss 2.4554, time 18.00ms, mfu 0.26%
iter 240: loss 2.4277, time 16.65ms, mfu 0.27%
iter 245: loss 2.4849, time 17.35ms, mfu 0.27%
step 250: train loss 2.4342, val loss 2.4260
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
iter 250: loss 2.4230, time 352.55ms, mfu 0.25%
iter 255: loss 2.4614, time 16.57ms, mfu 0.26%
iter 260: loss 2.4616, time 16.08ms, mfu 0.26%
iter 265: loss 2.4005, time 17.24ms, mfu 0.27%
iter 270: loss 2.4603, time 15.70ms, mfu 0.28%
iter 275: loss 2.4346, time 16.38ms, mfu 0.28%
iter 280: loss 2.4116, time 16.17ms, mfu 0.29%
iter 285: loss 2.4182, time 17.18ms, mfu 0.29%
iter 290: loss 2.4965, time 16.95ms, mfu 0.29%
iter 295: loss 2.4580, time 16.71ms, mfu 0.30%
iter 300: loss 2.4126, time 16.41ms, mfu 0.30%
iter 305: loss 2.4700, time 16.42ms, mfu 0.30%
iter 310: loss 2.4160, time 16.84ms, mfu 0.31%
iter 315: loss 2.4333, time 16.32ms, mfu 0.31%
iter 320: loss 2.4513, time 15.97ms, mfu 0.31%
iter 325: loss 2.4968, time 16.52ms, mfu 0.31%
iter 330: loss 2.4587, time 16.12ms, mfu 0.32%
iter 335: loss 2.3688, time 17.19ms, mfu 0.32%
iter 340: loss 2.4229, time 17.91ms, mfu 0.32%
iter 345: loss 2.3881, time 16.63ms, mfu 0.32%
iter 350: loss 2.4364, time 15.79ms, mfu 0.32%
iter 355: loss 2.4140, time 16.24ms, mfu 0.32%
iter 360: loss 2.3051, time 16.25ms, mfu 0.32%
iter 365: loss 2.4422, time 17.00ms, mfu 0.32%
iter 370: loss 2.3754, time 16.09ms, mfu 0.32%
iter 375: loss 2.3775, time 16.40ms, mfu 0.33%
iter 380: loss 2.3459, time 16.61ms, mfu 0.33%
iter 385: loss 2.3695, time 17.32ms, mfu 0.32%
iter 390: loss 2.3146, time 18.95ms, mfu 0.32%
iter 395: loss 2.3837, time 16.47ms, mfu 0.32%
iter 400: loss 2.3785, time 15.89ms, mfu 0.32%
iter 405: loss 2.3718, time 16.88ms, mfu 0.32%
iter 410: loss 2.3969, time 16.23ms, mfu 0.33%
iter 415: loss 2.3560, time 16.64ms, mfu 0.33%
iter 420: loss 2.3567, time 16.35ms, mfu 0.33%
iter 425: loss 2.3379, time 16.33ms, mfu 0.33%
iter 430: loss 2.3366, time 16.85ms, mfu 0.33%
iter 435: loss 2.3525, time 16.27ms, mfu 0.33%
iter 440: loss 2.3198, time 16.12ms, mfu 0.33%
iter 445: loss 2.3148, time 16.45ms, mfu 0.33%
iter 450: loss 2.3368, time 16.98ms, mfu 0.33%
iter 455: loss 2.3029, time 16.32ms, mfu 0.33%
iter 460: loss 2.3267, time 16.10ms, mfu 0.33%
iter 465: loss 2.3447, time 16.26ms, mfu 0.33%
iter 470: loss 2.3810, time 16.26ms, mfu 0.33%
iter 475: loss 2.2987, time 18.40ms, mfu 0.33%
iter 480: loss 2.2667, time 16.55ms, mfu 0.33%
iter 485: loss 2.3104, time 19.17ms, mfu 0.32%
iter 490: loss 2.2715, time 16.71ms, mfu 0.32%
iter 495: loss 2.3007, time 22.34ms, mfu 0.32%
step 500: train loss 2.2480, val loss 2.2631
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
iter 500: loss 2.2778, time 353.88ms, mfu 0.29%
iter 505: loss 2.2859, time 17.10ms, mfu 0.29%
iter 510: loss 2.2772, time 17.55ms, mfu 0.29%
iter 515: loss 2.3000, time 16.34ms, mfu 0.30%
iter 520: loss 2.2928, time 15.94ms, mfu 0.30%
iter 525: loss 2.3183, time 15.85ms, mfu 0.30%
iter 530: loss 2.2752, time 17.21ms, mfu 0.31%
iter 535: loss 2.2706, time 16.17ms, mfu 0.31%
iter 540: loss 2.2530, time 15.98ms, mfu 0.31%
iter 545: loss 2.2697, time 16.98ms, mfu 0.31%
iter 550: loss 2.2192, time 16.72ms, mfu 0.31%
iter 555: loss 2.2513, time 16.77ms, mfu 0.32%
iter 560: loss 2.2356, time 15.96ms, mfu 0.32%
iter 565: loss 2.2643, time 16.27ms, mfu 0.32%
iter 570: loss 2.2182, time 16.16ms, mfu 0.32%
iter 575: loss 2.2357, time 15.19ms, mfu 0.33%
iter 580: loss 2.2251, time 15.79ms, mfu 0.33%
iter 585: loss 2.2294, time 26.20ms, mfu 0.32%
iter 590: loss 2.2198, time 15.73ms, mfu 0.32%
iter 595: loss 2.2263, time 16.14ms, mfu 0.32%
iter 600: loss 2.2702, time 16.06ms, mfu 0.32%
iter 605: loss 2.2357, time 16.03ms, mfu 0.32%
iter 610: loss 2.2518, time 16.20ms, mfu 0.33%
iter 615: loss 2.2347, time 18.42ms, mfu 0.32%
iter 620: loss 2.2632, time 17.90ms, mfu 0.32%
iter 625: loss 2.2240, time 16.51ms, mfu 0.32%
iter 630: loss 2.2473, time 15.80ms, mfu 0.32%
iter 635: loss 2.2643, time 15.98ms, mfu 0.33%
iter 640: loss 2.1985, time 16.14ms, mfu 0.33%
iter 645: loss 2.2154, time 16.14ms, mfu 0.33%
iter 650: loss 2.2361, time 16.53ms, mfu 0.33%
iter 655: loss 2.2095, time 16.40ms, mfu 0.33%
iter 660: loss 2.1941, time 16.48ms, mfu 0.33%
iter 665: loss 2.2356, time 16.84ms, mfu 0.33%
iter 670: loss 2.2634, time 26.85ms, mfu 0.32%
iter 675: loss 2.2543, time 21.37ms, mfu 0.31%
iter 680: loss 2.2114, time 26.80ms, mfu 0.30%
iter 685: loss 2.2029, time 26.57ms, mfu 0.29%
iter 690: loss 2.1963, time 21.73ms, mfu 0.29%
iter 695: loss 2.1823, time 24.43ms, mfu 0.28%
iter 700: loss 2.1999, time 24.78ms, mfu 0.27%
iter 705: loss 2.2470, time 22.04ms, mfu 0.27%
iter 710: loss 2.1145, time 21.32ms, mfu 0.27%
iter 715: loss 2.1572, time 20.52ms, mfu 0.27%
iter 720: loss 2.2035, time 20.34ms, mfu 0.27%
iter 725: loss 2.2181, time 21.39ms, mfu 0.27%
iter 730: loss 2.1462, time 26.08ms, mfu 0.26%
iter 735: loss 2.2266, time 21.87ms, mfu 0.26%
iter 740: loss 2.1736, time 21.16ms, mfu 0.26%
iter 745: loss 2.2386, time 21.68ms, mfu 0.26%
step 750: train loss 2.1004, val loss 2.1347
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
iter 750: loss 2.2473, time 464.54ms, mfu 0.23%
iter 755: loss 2.1526, time 32.59ms, mfu 0.23%
iter 760: loss 2.0870, time 26.13ms, mfu 0.23%
iter 765: loss 2.1997, time 25.76ms, mfu 0.22%
iter 770: loss 2.1072, time 25.52ms, mfu 0.22%
iter 775: loss 2.1590, time 25.82ms, mfu 0.22%
iter 780: loss 2.1769, time 25.52ms, mfu 0.22%
iter 785: loss 2.1445, time 16.01ms, mfu 0.23%
iter 790: loss 2.1694, time 15.98ms, mfu 0.24%
iter 795: loss 2.1089, time 26.55ms, mfu 0.24%
iter 800: loss 2.1899, time 16.00ms, mfu 0.25%
iter 805: loss 2.1171, time 16.21ms, mfu 0.26%
iter 810: loss 2.1007, time 16.04ms, mfu 0.27%
iter 815: loss 2.1760, time 16.46ms, mfu 0.27%
iter 820: loss 2.0815, time 15.99ms, mfu 0.28%
iter 825: loss 2.1402, time 16.38ms, mfu 0.29%
iter 830: loss 2.2091, time 16.22ms, mfu 0.29%
iter 835: loss 2.1041, time 15.97ms, mfu 0.30%
iter 840: loss 2.1332, time 16.81ms, mfu 0.30%
iter 845: loss 2.0899, time 18.79ms, mfu 0.30%
iter 850: loss 2.1565, time 16.28ms, mfu 0.30%
iter 855: loss 2.1104, time 16.41ms, mfu 0.30%
iter 860: loss 2.1698, time 16.08ms, mfu 0.31%
iter 865: loss 2.1556, time 16.05ms, mfu 0.31%
iter 870: loss 2.1367, time 17.28ms, mfu 0.31%
iter 875: loss 2.1585, time 16.37ms, mfu 0.31%
iter 880: loss 2.1471, time 16.60ms, mfu 0.32%
iter 885: loss 2.1950, time 16.74ms, mfu 0.32%
iter 890: loss 2.1058, time 16.07ms, mfu 0.32%
iter 895: loss 2.0938, time 16.16ms, mfu 0.32%
iter 900: loss 2.1555, time 16.47ms, mfu 0.32%
iter 905: loss 2.1832, time 17.13ms, mfu 0.32%
iter 910: loss 2.1803, time 16.20ms, mfu 0.32%
iter 915: loss 2.0621, time 17.13ms, mfu 0.32%
iter 920: loss 2.0434, time 21.00ms, mfu 0.32%
iter 925: loss 2.1344, time 16.43ms, mfu 0.32%
iter 930: loss 2.1670, time 15.88ms, mfu 0.32%
iter 935: loss 2.1671, time 16.33ms, mfu 0.32%
iter 940: loss 2.0887, time 16.24ms, mfu 0.32%
iter 945: loss 2.0937, time 16.17ms, mfu 0.32%
iter 950: loss 2.1230, time 16.54ms, mfu 0.33%
iter 955: loss 2.1128, time 17.38ms, mfu 0.32%
iter 960: loss 2.1212, time 16.46ms, mfu 0.32%
iter 965: loss 2.0607, time 16.96ms, mfu 0.32%
iter 970: loss 2.0682, time 20.36ms, mfu 0.32%
iter 975: loss 2.1255, time 18.65ms, mfu 0.32%
iter 980: loss 2.0747, time 16.50ms, mfu 0.32%
iter 985: loss 2.1670, time 15.84ms, mfu 0.32%
iter 990: loss 2.0736, time 16.20ms, mfu 0.32%
iter 995: loss 2.0986, time 15.70ms, mfu 0.32%
step 1000: train loss 2.0390, val loss 2.0851
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
iter 1000: loss 2.0776, time 361.31ms, mfu 0.29%
iter 1005: loss 2.0929, time 16.87ms, mfu 0.30%
iter 1010: loss 2.1062, time 16.93ms, mfu 0.30%
iter 1015: loss 2.2101, time 16.30ms, mfu 0.30%
iter 1020: loss 2.1504, time 16.78ms, mfu 0.31%
iter 1025: loss 2.0769, time 16.47ms, mfu 0.31%
iter 1030: loss 2.0971, time 16.22ms, mfu 0.31%
iter 1035: loss 2.1405, time 17.48ms, mfu 0.31%
iter 1040: loss 2.1107, time 15.91ms, mfu 0.31%
iter 1045: loss 2.1070, time 16.54ms, mfu 0.32%
iter 1050: loss 2.0363, time 17.57ms, mfu 0.32%
iter 1055: loss 2.0835, time 16.20ms, mfu 0.32%
iter 1060: loss 2.0710, time 19.77ms, mfu 0.31%
iter 1065: loss 2.1247, time 16.17ms, mfu 0.32%
iter 1070: loss 2.1355, time 16.45ms, mfu 0.32%
iter 1075: loss 2.0855, time 16.63ms, mfu 0.32%
iter 1080: loss 2.1030, time 16.13ms, mfu 0.32%
iter 1085: loss 2.0925, time 24.57ms, mfu 0.31%
iter 1090: loss 2.0830, time 16.16ms, mfu 0.31%
iter 1095: loss 2.1092, time 16.69ms, mfu 0.31%
iter 1100: loss 2.0982, time 18.13ms, mfu 0.31%
iter 1105: loss 2.0735, time 16.20ms, mfu 0.32%
iter 1110: loss 2.1104, time 17.51ms, mfu 0.32%
iter 1115: loss 2.1288, time 16.38ms, mfu 0.32%
iter 1120: loss 2.1268, time 21.33ms, mfu 0.31%
iter 1125: loss 2.1055, time 18.17ms, mfu 0.31%
iter 1130: loss 2.1533, time 17.48ms, mfu 0.31%
iter 1135: loss 2.0821, time 23.52ms, mfu 0.30%
iter 1140: loss 2.0487, time 16.57ms, mfu 0.31%
iter 1145: loss 2.1233, time 17.78ms, mfu 0.31%
iter 1150: loss 2.0615, time 19.45ms, mfu 0.30%
iter 1155: loss 2.0671, time 16.38ms, mfu 0.31%
iter 1160: loss 2.1432, time 17.82ms, mfu 0.31%
iter 1165: loss 2.0834, time 16.28ms, mfu 0.31%
iter 1170: loss 2.1218, time 20.57ms, mfu 0.30%
iter 1175: loss 2.1213, time 19.24ms, mfu 0.30%
iter 1180: loss 2.1729, time 16.38ms, mfu 0.31%
iter 1185: loss 2.0868, time 20.75ms, mfu 0.30%
iter 1190: loss 2.0844, time 16.54ms, mfu 0.30%
iter 1195: loss 2.0805, time 19.95ms, mfu 0.30%
iter 1200: loss 2.1029, time 19.44ms, mfu 0.30%
iter 1205: loss 2.0680, time 16.21ms, mfu 0.30%
iter 1210: loss 2.0863, time 18.40ms, mfu 0.30%
iter 1215: loss 2.0899, time 16.73ms, mfu 0.30%
iter 1220: loss 2.0690, time 21.04ms, mfu 0.30%
iter 1225: loss 2.0479, time 17.48ms, mfu 0.30%
iter 1230: loss 2.1227, time 15.88ms, mfu 0.31%
iter 1235: loss 2.1080, time 19.94ms, mfu 0.30%
iter 1240: loss 2.0938, time 22.67ms, mfu 0.30%
iter 1245: loss 2.0515, time 19.31ms, mfu 0.29%
step 1250: train loss 2.0197, val loss 2.0617
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
iter 1250: loss 2.0245, time 402.96ms, mfu 0.27%
iter 1255: loss 2.0710, time 24.81ms, mfu 0.26%
iter 1260: loss 2.0988, time 21.53ms, mfu 0.26%
iter 1265: loss 2.0596, time 24.97ms, mfu 0.26%
iter 1270: loss 2.1278, time 28.66ms, mfu 0.25%
iter 1275: loss 2.1458, time 21.67ms, mfu 0.25%
iter 1280: loss 2.0106, time 26.40ms, mfu 0.25%
iter 1285: loss 2.0302, time 23.10ms, mfu 0.25%
iter 1290: loss 2.1207, time 20.31ms, mfu 0.25%
iter 1295: loss 2.0836, time 20.09ms, mfu 0.25%
iter 1300: loss 2.0856, time 20.49ms, mfu 0.25%
iter 1305: loss 2.0391, time 20.63ms, mfu 0.25%
iter 1310: loss 2.0315, time 23.50ms, mfu 0.25%
iter 1315: loss 2.0587, time 29.90ms, mfu 0.24%
iter 1320: loss 2.0549, time 22.49ms, mfu 0.24%
iter 1325: loss 2.0825, time 21.16ms, mfu 0.25%
iter 1330: loss 2.1312, time 24.79ms, mfu 0.24%
iter 1335: loss 2.0479, time 24.65ms, mfu 0.24%
iter 1340: loss 2.0670, time 24.02ms, mfu 0.24%
iter 1345: loss 2.0228, time 22.08ms, mfu 0.24%
iter 1350: loss 2.0406, time 24.76ms, mfu 0.24%
iter 1355: loss 2.0772, time 29.48ms, mfu 0.23%
iter 1360: loss 2.1138, time 31.04ms, mfu 0.23%
iter 1365: loss 2.1203, time 26.93ms, mfu 0.22%
iter 1370: loss 2.1396, time 26.55ms, mfu 0.22%
iter 1375: loss 2.0261, time 28.79ms, mfu 0.22%
iter 1380: loss 1.9625, time 28.91ms, mfu 0.22%
iter 1385: loss 2.0894, time 19.56ms, mfu 0.22%
iter 1390: loss 2.0050, time 16.19ms, mfu 0.23%
iter 1395: loss 2.0233, time 24.10ms, mfu 0.23%
iter 1400: loss 2.0579, time 15.95ms, mfu 0.24%
iter 1405: loss 2.0765, time 17.55ms, mfu 0.25%
iter 1410: loss 2.0737, time 16.41ms, mfu 0.26%
iter 1415: loss 2.0400, time 17.16ms, mfu 0.27%
iter 1420: loss 2.0720, time 16.22ms, mfu 0.27%
iter 1425: loss 2.0169, time 19.85ms, mfu 0.27%
iter 1430: loss 2.0847, time 16.29ms, mfu 0.28%
iter 1435: loss 2.0445, time 16.83ms, mfu 0.28%
iter 1440: loss 2.0479, time 19.50ms, mfu 0.28%
iter 1445: loss 2.0523, time 16.13ms, mfu 0.29%
iter 1450: loss 2.0226, time 20.82ms, mfu 0.29%
iter 1455: loss 2.0138, time 17.58ms, mfu 0.29%
iter 1460: loss 2.0906, time 21.92ms, mfu 0.28%
iter 1465: loss 2.0366, time 16.34ms, mfu 0.29%
iter 1470: loss 2.1105, time 16.51ms, mfu 0.29%
iter 1475: loss 2.0968, time 20.71ms, mfu 0.29%
iter 1480: loss 2.0853, time 23.64ms, mfu 0.28%
iter 1485: loss 2.0346, time 17.75ms, mfu 0.29%
iter 1490: loss 2.0599, time 16.61ms, mfu 0.29%
iter 1495: loss 2.1072, time 16.78ms, mfu 0.29%
step 1500: train loss 1.9761, val loss 2.0410
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
iter 1500: loss 2.0221, time 368.08ms, mfu 0.27%
iter 1505: loss 2.0696, time 16.08ms, mfu 0.27%
iter 1510: loss 2.0617, time 15.88ms, mfu 0.28%
iter 1515: loss 2.0800, time 16.63ms, mfu 0.29%
iter 1520: loss 2.0370, time 15.95ms, mfu 0.29%
iter 1525: loss 2.0446, time 16.92ms, mfu 0.29%
iter 1530: loss 2.0308, time 16.25ms, mfu 0.30%
iter 1535: loss 2.0380, time 16.12ms, mfu 0.30%
iter 1540: loss 2.0017, time 18.48ms, mfu 0.30%
iter 1545: loss 2.0053, time 16.66ms, mfu 0.30%
iter 1550: loss 2.0290, time 16.54ms, mfu 0.31%
iter 1555: loss 2.0521, time 16.57ms, mfu 0.31%
iter 1560: loss 2.0191, time 15.95ms, mfu 0.31%
iter 1565: loss 2.0328, time 17.48ms, mfu 0.31%
iter 1570: loss 2.0026, time 16.68ms, mfu 0.31%
iter 1575: loss 2.0574, time 16.18ms, mfu 0.32%
iter 1580: loss 1.9456, time 16.10ms, mfu 0.32%
iter 1585: loss 2.0457, time 17.20ms, mfu 0.32%
iter 1590: loss 2.0338, time 16.13ms, mfu 0.32%
iter 1595: loss 2.0482, time 16.09ms, mfu 0.32%
iter 1600: loss 2.0396, time 16.00ms, mfu 0.32%
iter 1605: loss 2.0576, time 18.61ms, mfu 0.32%
iter 1610: loss 2.0775, time 16.06ms, mfu 0.32%
iter 1615: loss 2.0131, time 16.31ms, mfu 0.32%
iter 1620: loss 2.0479, time 17.00ms, mfu 0.32%
iter 1625: loss 2.1197, time 16.47ms, mfu 0.32%
iter 1630: loss 2.0779, time 16.56ms, mfu 0.33%
iter 1635: loss 2.1128, time 18.15ms, mfu 0.32%
iter 1640: loss 2.0111, time 17.66ms, mfu 0.32%
iter 1645: loss 2.0400, time 16.68ms, mfu 0.32%
iter 1650: loss 1.9797, time 16.82ms, mfu 0.32%
iter 1655: loss 1.9997, time 16.42ms, mfu 0.32%
iter 1660: loss 2.0392, time 16.84ms, mfu 0.32%
iter 1665: loss 1.9893, time 16.11ms, mfu 0.32%
iter 1670: loss 1.9944, time 18.56ms, mfu 0.32%
iter 1675: loss 2.0569, time 16.43ms, mfu 0.32%
iter 1680: loss 2.0071, time 20.46ms, mfu 0.32%
iter 1685: loss 2.0597, time 18.44ms, mfu 0.32%
iter 1690: loss 2.0146, time 16.68ms, mfu 0.32%
iter 1695: loss 2.0186, time 30.04ms, mfu 0.30%
iter 1700: loss 2.0952, time 17.04ms, mfu 0.30%
iter 1705: loss 2.0623, time 18.76ms, mfu 0.30%
iter 1710: loss 2.0331, time 16.62ms, mfu 0.31%
iter 1715: loss 2.0361, time 16.48ms, mfu 0.31%
iter 1720: loss 2.0877, time 22.81ms, mfu 0.30%
iter 1725: loss 2.0037, time 17.17ms, mfu 0.30%
iter 1730: loss 2.0555, time 19.50ms, mfu 0.30%
iter 1735: loss 2.0911, time 18.13ms, mfu 0.30%
iter 1740: loss 1.9248, time 17.24ms, mfu 0.30%
iter 1745: loss 1.9451, time 20.53ms, mfu 0.30%
step 1750: train loss 1.9604, val loss 2.0022
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
iter 1750: loss 1.9601, time 359.21ms, mfu 0.27%
iter 1755: loss 2.0708, time 16.78ms, mfu 0.28%
iter 1760: loss 2.0372, time 16.72ms, mfu 0.28%
iter 1765: loss 2.0033, time 15.90ms, mfu 0.29%
iter 1770: loss 2.0026, time 17.67ms, mfu 0.29%
iter 1775: loss 2.0913, time 15.83ms, mfu 0.29%
iter 1780: loss 2.0138, time 15.64ms, mfu 0.30%
iter 1785: loss 1.9873, time 17.06ms, mfu 0.30%
iter 1790: loss 2.0882, time 16.66ms, mfu 0.30%
iter 1795: loss 2.0671, time 16.38ms, mfu 0.31%
iter 1800: loss 2.0721, time 15.90ms, mfu 0.31%
iter 1805: loss 2.0450, time 15.79ms, mfu 0.31%
iter 1810: loss 2.0641, time 17.80ms, mfu 0.31%
iter 1815: loss 2.0402, time 16.34ms, mfu 0.32%
iter 1820: loss 2.0068, time 17.25ms, mfu 0.32%
iter 1825: loss 2.0116, time 15.72ms, mfu 0.32%
iter 1830: loss 2.0403, time 15.60ms, mfu 0.32%
iter 1835: loss 2.0383, time 17.20ms, mfu 0.32%
iter 1840: loss 1.9716, time 19.32ms, mfu 0.32%
iter 1845: loss 1.9863, time 16.63ms, mfu 0.32%
iter 1850: loss 2.0273, time 24.21ms, mfu 0.31%
iter 1855: loss 2.0364, time 22.84ms, mfu 0.30%
iter 1860: loss 2.0468, time 23.57ms, mfu 0.30%
iter 1865: loss 1.9845, time 23.42ms, mfu 0.29%
iter 1870: loss 1.9338, time 21.60ms, mfu 0.29%
iter 1875: loss 1.9755, time 26.00ms, mfu 0.28%
iter 1880: loss 2.0299, time 25.58ms, mfu 0.27%
iter 1885: loss 2.0455, time 20.84ms, mfu 0.27%
iter 1890: loss 2.0175, time 20.65ms, mfu 0.27%
iter 1895: loss 1.9858, time 20.32ms, mfu 0.27%
iter 1900: loss 1.9764, time 20.33ms, mfu 0.27%
iter 1905: loss 1.9753, time 22.33ms, mfu 0.27%
iter 1910: loss 2.0376, time 24.22ms, mfu 0.26%
iter 1915: loss 1.9540, time 21.44ms, mfu 0.26%
iter 1920: loss 1.9887, time 21.66ms, mfu 0.26%
iter 1925: loss 2.0072, time 24.38ms, mfu 0.26%
iter 1930: loss 2.0166, time 21.59ms, mfu 0.26%
iter 1935: loss 1.9746, time 20.96ms, mfu 0.26%
iter 1940: loss 2.0262, time 27.58ms, mfu 0.25%
iter 1945: loss 1.9448, time 24.15ms, mfu 0.25%
iter 1950: loss 1.9731, time 22.93ms, mfu 0.25%
iter 1955: loss 1.9385, time 21.37ms, mfu 0.25%
iter 1960: loss 1.9843, time 26.94ms, mfu 0.24%
iter 1965: loss 1.9866, time 26.08ms, mfu 0.24%
iter 1970: loss 2.0094, time 24.64ms, mfu 0.24%
iter 1975: loss 2.0026, time 25.34ms, mfu 0.24%
iter 1980: loss 1.9841, time 28.39ms, mfu 0.23%
iter 1985: loss 1.9620, time 16.37ms, mfu 0.24%
iter 1990: loss 1.9832, time 16.75ms, mfu 0.25%
iter 1995: loss 1.9758, time 19.09ms, mfu 0.25%
step 2000: train loss 1.9257, val loss 1.9938
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1
iter 2000: loss 1.9893, time 378.46ms, mfu 0.23%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.1 at: 
wandb: Find logs at: wandb/run-20251024_005443-3te41fbn/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 128
Overriding: batch_size = 16
Overriding: max_iters = 2000
Overriding: dropout = 0.2
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
tokens per iteration will be: 2,048
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 1.19M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 1,204,352 parameters
num non-decayed parameter tensors: 13, with 1,664 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_005536-28wzbxkh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/28wzbxkh
step 0: train loss 4.1954, val loss 4.1906
iter 0: loss 4.1957, time 882.07ms, mfu -100.00%
iter 5: loss 3.9899, time 16.34ms, mfu 0.33%
iter 10: loss 3.7746, time 18.15ms, mfu 0.33%
iter 15: loss 3.6939, time 17.56ms, mfu 0.33%
iter 20: loss 3.5557, time 16.44ms, mfu 0.33%
iter 25: loss 3.3984, time 16.09ms, mfu 0.33%
iter 30: loss 3.2589, time 17.21ms, mfu 0.33%
iter 35: loss 3.2017, time 16.17ms, mfu 0.33%
iter 40: loss 3.0624, time 16.33ms, mfu 0.33%
iter 45: loss 2.9666, time 16.36ms, mfu 0.33%
iter 50: loss 2.8813, time 16.08ms, mfu 0.33%
iter 55: loss 2.8100, time 19.24ms, mfu 0.33%
iter 60: loss 2.7808, time 16.50ms, mfu 0.33%
iter 65: loss 2.7650, time 16.51ms, mfu 0.33%
iter 70: loss 2.6759, time 28.18ms, mfu 0.31%
iter 75: loss 2.6793, time 23.02ms, mfu 0.31%
iter 80: loss 2.6569, time 23.53ms, mfu 0.30%
iter 85: loss 2.6330, time 20.84ms, mfu 0.30%
iter 90: loss 2.6998, time 21.47ms, mfu 0.29%
iter 95: loss 2.6538, time 21.56ms, mfu 0.29%
iter 100: loss 2.6526, time 21.85ms, mfu 0.28%
iter 105: loss 2.6214, time 22.87ms, mfu 0.28%
iter 110: loss 2.5723, time 22.18ms, mfu 0.28%
iter 115: loss 2.6407, time 21.88ms, mfu 0.27%
iter 120: loss 2.5180, time 22.70ms, mfu 0.27%
iter 125: loss 2.5521, time 20.34ms, mfu 0.27%
iter 130: loss 2.5774, time 21.07ms, mfu 0.27%
iter 135: loss 2.5836, time 21.36ms, mfu 0.27%
iter 140: loss 2.5543, time 22.69ms, mfu 0.26%
iter 145: loss 2.5737, time 20.28ms, mfu 0.27%
iter 150: loss 2.5297, time 21.26ms, mfu 0.26%
iter 155: loss 2.5419, time 20.55ms, mfu 0.26%
iter 160: loss 2.5220, time 34.94ms, mfu 0.25%
iter 165: loss 2.5632, time 21.76ms, mfu 0.25%
iter 170: loss 2.5178, time 24.58ms, mfu 0.25%
iter 175: loss 2.5769, time 22.72ms, mfu 0.25%
iter 180: loss 2.5041, time 30.35ms, mfu 0.24%
iter 185: loss 2.4812, time 24.76ms, mfu 0.24%
iter 190: loss 2.4552, time 50.69ms, mfu 0.23%
iter 195: loss 2.5032, time 37.94ms, mfu 0.22%
iter 200: loss 2.5151, time 32.66ms, mfu 0.21%
iter 205: loss 2.5257, time 26.61ms, mfu 0.21%
iter 210: loss 2.4652, time 16.77ms, mfu 0.22%
iter 215: loss 2.4535, time 16.19ms, mfu 0.24%
iter 220: loss 2.5153, time 16.16ms, mfu 0.25%
iter 225: loss 2.4888, time 15.94ms, mfu 0.26%
iter 230: loss 2.4517, time 15.83ms, mfu 0.26%
iter 235: loss 2.4672, time 15.89ms, mfu 0.27%
iter 240: loss 2.4578, time 16.49ms, mfu 0.28%
iter 245: loss 2.5068, time 16.35ms, mfu 0.28%
step 250: train loss 2.4607, val loss 2.4492
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
iter 250: loss 2.4610, time 358.77ms, mfu 0.26%
iter 255: loss 2.5027, time 17.49ms, mfu 0.26%
iter 260: loss 2.4991, time 16.89ms, mfu 0.27%
iter 265: loss 2.4312, time 16.38ms, mfu 0.27%
iter 270: loss 2.4967, time 17.32ms, mfu 0.28%
iter 275: loss 2.4738, time 16.69ms, mfu 0.28%
iter 280: loss 2.4461, time 18.03ms, mfu 0.29%
iter 285: loss 2.4520, time 16.61ms, mfu 0.29%
iter 290: loss 2.5187, time 16.09ms, mfu 0.29%
iter 295: loss 2.4751, time 16.00ms, mfu 0.30%
iter 300: loss 2.4460, time 16.15ms, mfu 0.30%
iter 305: loss 2.5044, time 16.37ms, mfu 0.31%
iter 310: loss 2.4446, time 16.78ms, mfu 0.31%
iter 315: loss 2.4667, time 16.48ms, mfu 0.31%
iter 320: loss 2.4933, time 17.86ms, mfu 0.31%
iter 325: loss 2.5208, time 17.53ms, mfu 0.31%
iter 330: loss 2.4976, time 28.11ms, mfu 0.30%
iter 335: loss 2.4276, time 18.38ms, mfu 0.30%
iter 340: loss 2.4576, time 16.38ms, mfu 0.30%
iter 345: loss 2.4231, time 16.19ms, mfu 0.31%
iter 350: loss 2.4767, time 19.34ms, mfu 0.30%
iter 355: loss 2.4698, time 15.85ms, mfu 0.31%
iter 360: loss 2.3658, time 15.73ms, mfu 0.31%
iter 365: loss 2.4796, time 15.90ms, mfu 0.31%
iter 370: loss 2.4216, time 17.06ms, mfu 0.32%
iter 375: loss 2.4159, time 15.92ms, mfu 0.32%
iter 380: loss 2.3997, time 15.83ms, mfu 0.32%
iter 385: loss 2.4257, time 16.22ms, mfu 0.32%
iter 390: loss 2.3812, time 15.94ms, mfu 0.32%
iter 395: loss 2.4429, time 16.74ms, mfu 0.32%
iter 400: loss 2.4174, time 17.99ms, mfu 0.32%
iter 405: loss 2.4114, time 16.43ms, mfu 0.32%
iter 410: loss 2.4450, time 15.88ms, mfu 0.33%
iter 415: loss 2.4033, time 16.36ms, mfu 0.33%
iter 420: loss 2.3950, time 16.82ms, mfu 0.33%
iter 425: loss 2.3933, time 16.29ms, mfu 0.33%
iter 430: loss 2.4026, time 16.23ms, mfu 0.33%
iter 435: loss 2.3959, time 17.21ms, mfu 0.33%
iter 440: loss 2.3865, time 16.12ms, mfu 0.33%
iter 445: loss 2.3870, time 16.47ms, mfu 0.33%
iter 450: loss 2.4088, time 17.65ms, mfu 0.33%
iter 455: loss 2.3799, time 15.95ms, mfu 0.33%
iter 460: loss 2.3980, time 15.80ms, mfu 0.33%
iter 465: loss 2.4049, time 15.79ms, mfu 0.33%
iter 470: loss 2.4408, time 16.09ms, mfu 0.33%
iter 475: loss 2.3605, time 16.13ms, mfu 0.33%
iter 480: loss 2.3405, time 34.89ms, mfu 0.32%
iter 485: loss 2.3902, time 22.67ms, mfu 0.31%
iter 490: loss 2.3393, time 23.37ms, mfu 0.30%
iter 495: loss 2.3534, time 16.98ms, mfu 0.30%
step 500: train loss 2.3114, val loss 2.3171
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
iter 500: loss 2.3487, time 364.96ms, mfu 0.27%
iter 505: loss 2.3714, time 16.36ms, mfu 0.28%
iter 510: loss 2.3492, time 18.50ms, mfu 0.28%
iter 515: loss 2.3792, time 20.52ms, mfu 0.28%
iter 520: loss 2.3559, time 17.53ms, mfu 0.28%
iter 525: loss 2.3736, time 18.74ms, mfu 0.28%
iter 530: loss 2.3448, time 16.45ms, mfu 0.29%
iter 535: loss 2.3429, time 18.10ms, mfu 0.29%
iter 540: loss 2.3301, time 16.12ms, mfu 0.29%
iter 545: loss 2.3476, time 20.37ms, mfu 0.29%
iter 550: loss 2.3102, time 16.23ms, mfu 0.30%
iter 555: loss 2.3470, time 20.77ms, mfu 0.29%
iter 560: loss 2.3326, time 16.73ms, mfu 0.30%
iter 565: loss 2.3383, time 26.81ms, mfu 0.29%
iter 570: loss 2.3008, time 16.83ms, mfu 0.29%
iter 575: loss 2.3221, time 16.22ms, mfu 0.30%
iter 580: loss 2.3084, time 19.71ms, mfu 0.29%
iter 585: loss 2.2997, time 17.01ms, mfu 0.30%
iter 590: loss 2.3038, time 21.37ms, mfu 0.29%
iter 595: loss 2.3146, time 16.71ms, mfu 0.30%
iter 600: loss 2.3258, time 16.53ms, mfu 0.30%
iter 605: loss 2.3361, time 19.66ms, mfu 0.30%
iter 610: loss 2.3221, time 16.60ms, mfu 0.30%
iter 615: loss 2.3000, time 19.54ms, mfu 0.30%
iter 620: loss 2.3509, time 16.37ms, mfu 0.30%
iter 625: loss 2.3009, time 17.13ms, mfu 0.30%
iter 630: loss 2.3207, time 21.90ms, mfu 0.30%
iter 635: loss 2.3408, time 18.44ms, mfu 0.30%
iter 640: loss 2.2764, time 18.55ms, mfu 0.30%
iter 645: loss 2.3145, time 16.61ms, mfu 0.30%
iter 650: loss 2.3104, time 16.71ms, mfu 0.30%
iter 655: loss 2.2819, time 16.42ms, mfu 0.31%
iter 660: loss 2.2715, time 17.23ms, mfu 0.31%
iter 665: loss 2.3170, time 17.26ms, mfu 0.31%
iter 670: loss 2.3525, time 30.31ms, mfu 0.30%
iter 675: loss 2.3154, time 24.38ms, mfu 0.29%
iter 680: loss 2.2918, time 22.55ms, mfu 0.28%
iter 685: loss 2.2840, time 25.97ms, mfu 0.28%
iter 690: loss 2.2775, time 22.12ms, mfu 0.27%
iter 695: loss 2.2624, time 22.50ms, mfu 0.27%
iter 700: loss 2.2955, time 22.47ms, mfu 0.27%
iter 705: loss 2.3297, time 21.97ms, mfu 0.27%
iter 710: loss 2.2149, time 20.40ms, mfu 0.27%
iter 715: loss 2.2456, time 21.97ms, mfu 0.26%
iter 720: loss 2.2982, time 20.26ms, mfu 0.26%
iter 725: loss 2.3049, time 20.18ms, mfu 0.27%
iter 730: loss 2.2398, time 32.75ms, mfu 0.26%
iter 735: loss 2.3300, time 25.30ms, mfu 0.25%
iter 740: loss 2.2724, time 22.06ms, mfu 0.25%
iter 745: loss 2.3407, time 21.53ms, mfu 0.25%
step 750: train loss 2.1834, val loss 2.2025
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
iter 750: loss 2.3322, time 460.30ms, mfu 0.23%
iter 755: loss 2.2547, time 24.52ms, mfu 0.23%
iter 760: loss 2.1876, time 26.75ms, mfu 0.22%
iter 765: loss 2.2997, time 28.35ms, mfu 0.22%
iter 770: loss 2.2157, time 26.69ms, mfu 0.22%
iter 775: loss 2.2471, time 16.36ms, mfu 0.23%
iter 780: loss 2.2676, time 17.37ms, mfu 0.24%
iter 785: loss 2.2359, time 16.09ms, mfu 0.25%
iter 790: loss 2.2601, time 16.32ms, mfu 0.26%
iter 795: loss 2.2196, time 16.63ms, mfu 0.26%
iter 800: loss 2.2881, time 17.34ms, mfu 0.27%
iter 805: loss 2.2095, time 16.28ms, mfu 0.28%
iter 810: loss 2.1996, time 16.74ms, mfu 0.28%
iter 815: loss 2.2704, time 16.76ms, mfu 0.29%
iter 820: loss 2.1898, time 15.97ms, mfu 0.29%
iter 825: loss 2.2459, time 17.71ms, mfu 0.29%
iter 830: loss 2.2923, time 16.84ms, mfu 0.30%
iter 835: loss 2.2042, time 16.39ms, mfu 0.30%
iter 840: loss 2.2232, time 16.48ms, mfu 0.30%
iter 845: loss 2.1900, time 16.45ms, mfu 0.31%
iter 850: loss 2.2553, time 17.52ms, mfu 0.31%
iter 855: loss 2.2136, time 16.65ms, mfu 0.31%
iter 860: loss 2.2618, time 16.73ms, mfu 0.31%
iter 865: loss 2.2627, time 16.92ms, mfu 0.31%
iter 870: loss 2.2412, time 16.72ms, mfu 0.31%
iter 875: loss 2.2594, time 16.78ms, mfu 0.31%
iter 880: loss 2.2495, time 17.17ms, mfu 0.31%
iter 885: loss 2.2886, time 26.84ms, mfu 0.30%
iter 890: loss 2.2144, time 16.37ms, mfu 0.31%
iter 895: loss 2.2229, time 16.92ms, mfu 0.31%
iter 900: loss 2.2555, time 18.18ms, mfu 0.31%
iter 905: loss 2.2892, time 16.84ms, mfu 0.31%
iter 910: loss 2.2717, time 16.99ms, mfu 0.31%
iter 915: loss 2.1714, time 16.50ms, mfu 0.31%
iter 920: loss 2.1614, time 16.64ms, mfu 0.31%
iter 925: loss 2.2291, time 17.11ms, mfu 0.31%
iter 930: loss 2.2728, time 17.11ms, mfu 0.31%
iter 935: loss 2.2724, time 16.49ms, mfu 0.32%
iter 940: loss 2.2005, time 16.92ms, mfu 0.32%
iter 945: loss 2.2038, time 16.88ms, mfu 0.32%
iter 950: loss 2.2175, time 19.04ms, mfu 0.31%
iter 955: loss 2.2243, time 17.05ms, mfu 0.32%
iter 960: loss 2.2134, time 16.71ms, mfu 0.32%
iter 965: loss 2.1574, time 16.41ms, mfu 0.32%
iter 970: loss 2.1613, time 16.69ms, mfu 0.32%
iter 975: loss 2.2461, time 17.37ms, mfu 0.32%
iter 980: loss 2.1728, time 20.46ms, mfu 0.31%
iter 985: loss 2.2747, time 16.76ms, mfu 0.31%
iter 990: loss 2.1887, time 18.59ms, mfu 0.31%
iter 995: loss 2.2059, time 16.22ms, mfu 0.31%
step 1000: train loss 2.1357, val loss 2.1670
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
iter 1000: loss 2.1845, time 364.75ms, mfu 0.28%
iter 1005: loss 2.1834, time 17.03ms, mfu 0.29%
iter 1010: loss 2.2141, time 18.92ms, mfu 0.29%
iter 1015: loss 2.3060, time 16.50ms, mfu 0.29%
iter 1020: loss 2.2369, time 16.75ms, mfu 0.30%
iter 1025: loss 2.1885, time 16.34ms, mfu 0.30%
iter 1030: loss 2.1933, time 16.64ms, mfu 0.30%
iter 1035: loss 2.2476, time 17.04ms, mfu 0.30%
iter 1040: loss 2.2175, time 18.13ms, mfu 0.30%
iter 1045: loss 2.2214, time 16.70ms, mfu 0.31%
iter 1050: loss 2.1391, time 15.88ms, mfu 0.31%
iter 1055: loss 2.1897, time 16.14ms, mfu 0.31%
iter 1060: loss 2.1873, time 17.25ms, mfu 0.31%
iter 1065: loss 2.2334, time 16.09ms, mfu 0.32%
iter 1070: loss 2.2223, time 15.99ms, mfu 0.32%
iter 1075: loss 2.1870, time 16.31ms, mfu 0.32%
iter 1080: loss 2.2256, time 15.86ms, mfu 0.32%
iter 1085: loss 2.1926, time 16.25ms, mfu 0.32%
iter 1090: loss 2.2063, time 17.56ms, mfu 0.32%
iter 1095: loss 2.1885, time 16.42ms, mfu 0.32%
iter 1100: loss 2.1962, time 16.42ms, mfu 0.32%
iter 1105: loss 2.1863, time 17.02ms, mfu 0.32%
iter 1110: loss 2.2319, time 16.23ms, mfu 0.33%
iter 1115: loss 2.2261, time 16.45ms, mfu 0.33%
iter 1120: loss 2.2257, time 17.24ms, mfu 0.33%
iter 1125: loss 2.2134, time 22.10ms, mfu 0.32%
iter 1130: loss 2.2482, time 16.53ms, mfu 0.32%
iter 1135: loss 2.1826, time 16.41ms, mfu 0.32%
iter 1140: loss 2.1754, time 17.96ms, mfu 0.32%
iter 1145: loss 2.2234, time 16.78ms, mfu 0.32%
iter 1150: loss 2.1768, time 16.31ms, mfu 0.32%
iter 1155: loss 2.1658, time 15.92ms, mfu 0.32%
iter 1160: loss 2.2510, time 16.80ms, mfu 0.32%
iter 1165: loss 2.2009, time 16.17ms, mfu 0.32%
iter 1170: loss 2.2347, time 16.33ms, mfu 0.33%
iter 1175: loss 2.2016, time 18.79ms, mfu 0.32%
iter 1180: loss 2.2719, time 16.34ms, mfu 0.32%
iter 1185: loss 2.1894, time 16.76ms, mfu 0.32%
iter 1190: loss 2.1892, time 18.87ms, mfu 0.32%
iter 1195: loss 2.1828, time 15.85ms, mfu 0.32%
iter 1200: loss 2.2081, time 17.62ms, mfu 0.32%
iter 1205: loss 2.1746, time 16.36ms, mfu 0.32%
iter 1210: loss 2.1819, time 16.68ms, mfu 0.32%
iter 1215: loss 2.2087, time 15.92ms, mfu 0.32%
iter 1220: loss 2.1680, time 16.88ms, mfu 0.32%
iter 1225: loss 2.1680, time 19.71ms, mfu 0.32%
iter 1230: loss 2.2136, time 16.47ms, mfu 0.32%
iter 1235: loss 2.2394, time 18.33ms, mfu 0.32%
iter 1240: loss 2.2024, time 17.73ms, mfu 0.32%
iter 1245: loss 2.1754, time 15.86ms, mfu 0.32%
step 1250: train loss 2.1163, val loss 2.1427
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
iter 1250: loss 2.1525, time 456.30ms, mfu 0.29%
iter 1255: loss 2.1710, time 21.65ms, mfu 0.29%
iter 1260: loss 2.2257, time 28.42ms, mfu 0.28%
iter 1265: loss 2.1979, time 23.55ms, mfu 0.27%
iter 1270: loss 2.2383, time 21.43ms, mfu 0.27%
iter 1275: loss 2.2436, time 21.26ms, mfu 0.27%
iter 1280: loss 2.1051, time 22.47ms, mfu 0.27%
iter 1285: loss 2.1633, time 20.33ms, mfu 0.27%
iter 1290: loss 2.2156, time 20.65ms, mfu 0.27%
iter 1295: loss 2.1834, time 34.03ms, mfu 0.26%
iter 1300: loss 2.1796, time 22.49ms, mfu 0.25%
iter 1305: loss 2.1576, time 25.38ms, mfu 0.25%
iter 1310: loss 2.1355, time 20.63ms, mfu 0.25%
iter 1315: loss 2.1459, time 23.70ms, mfu 0.25%
iter 1320: loss 2.1587, time 20.45ms, mfu 0.25%
iter 1325: loss 2.2007, time 22.20ms, mfu 0.25%
iter 1330: loss 2.2429, time 23.29ms, mfu 0.25%
iter 1335: loss 2.1619, time 27.51ms, mfu 0.24%
iter 1340: loss 2.1716, time 29.00ms, mfu 0.24%
iter 1345: loss 2.1483, time 30.27ms, mfu 0.23%
iter 1350: loss 2.1497, time 27.95ms, mfu 0.23%
iter 1355: loss 2.1803, time 26.74ms, mfu 0.23%
iter 1360: loss 2.2093, time 16.12ms, mfu 0.24%
iter 1365: loss 2.2228, time 18.86ms, mfu 0.24%
iter 1370: loss 2.2436, time 16.72ms, mfu 0.25%
iter 1375: loss 2.1422, time 16.43ms, mfu 0.26%
iter 1380: loss 2.1026, time 16.06ms, mfu 0.27%
iter 1385: loss 2.1928, time 17.13ms, mfu 0.27%
iter 1390: loss 2.1281, time 17.26ms, mfu 0.28%
iter 1395: loss 2.1362, time 17.08ms, mfu 0.28%
iter 1400: loss 2.1719, time 16.01ms, mfu 0.29%
iter 1405: loss 2.1820, time 17.89ms, mfu 0.29%
iter 1410: loss 2.1894, time 16.49ms, mfu 0.29%
iter 1415: loss 2.1670, time 17.09ms, mfu 0.30%
iter 1420: loss 2.1747, time 17.10ms, mfu 0.30%
iter 1425: loss 2.1278, time 18.31ms, mfu 0.30%
iter 1430: loss 2.1960, time 15.68ms, mfu 0.30%
iter 1435: loss 2.1667, time 16.42ms, mfu 0.31%
iter 1440: loss 2.1676, time 17.22ms, mfu 0.31%
iter 1445: loss 2.1619, time 26.09ms, mfu 0.30%
iter 1450: loss 2.1292, time 18.56ms, mfu 0.30%
iter 1455: loss 2.1217, time 16.52ms, mfu 0.30%
iter 1460: loss 2.1810, time 16.18ms, mfu 0.30%
iter 1465: loss 2.1345, time 19.57ms, mfu 0.30%
iter 1470: loss 2.2200, time 15.82ms, mfu 0.31%
iter 1475: loss 2.2039, time 15.52ms, mfu 0.31%
iter 1480: loss 2.2245, time 17.47ms, mfu 0.31%
iter 1485: loss 2.1823, time 18.00ms, mfu 0.31%
iter 1490: loss 2.1760, time 20.39ms, mfu 0.31%
iter 1495: loss 2.2045, time 16.41ms, mfu 0.31%
step 1500: train loss 2.0843, val loss 2.1257
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
iter 1500: loss 2.1562, time 357.43ms, mfu 0.28%
iter 1505: loss 2.1811, time 15.75ms, mfu 0.29%
iter 1510: loss 2.1759, time 17.10ms, mfu 0.29%
iter 1515: loss 2.1782, time 16.22ms, mfu 0.29%
iter 1520: loss 2.1510, time 16.65ms, mfu 0.30%
iter 1525: loss 2.1661, time 16.44ms, mfu 0.30%
iter 1530: loss 2.1549, time 16.69ms, mfu 0.30%
iter 1535: loss 2.1560, time 17.87ms, mfu 0.30%
iter 1540: loss 2.1321, time 16.28ms, mfu 0.31%
iter 1545: loss 2.1305, time 16.54ms, mfu 0.31%
iter 1550: loss 2.1542, time 15.82ms, mfu 0.31%
iter 1555: loss 2.1601, time 16.53ms, mfu 0.31%
iter 1560: loss 2.1199, time 16.25ms, mfu 0.32%
iter 1565: loss 2.1429, time 15.86ms, mfu 0.32%
iter 1570: loss 2.1419, time 16.05ms, mfu 0.32%
iter 1575: loss 2.1649, time 15.84ms, mfu 0.32%
iter 1580: loss 2.0838, time 17.92ms, mfu 0.32%
iter 1585: loss 2.1742, time 16.85ms, mfu 0.32%
iter 1590: loss 2.1629, time 15.85ms, mfu 0.32%
iter 1595: loss 2.1633, time 16.41ms, mfu 0.33%
iter 1600: loss 2.1576, time 16.57ms, mfu 0.33%
iter 1605: loss 2.1580, time 18.65ms, mfu 0.32%
iter 1610: loss 2.1852, time 16.07ms, mfu 0.32%
iter 1615: loss 2.1325, time 15.85ms, mfu 0.33%
iter 1620: loss 2.1624, time 16.13ms, mfu 0.33%
iter 1625: loss 2.2178, time 16.58ms, mfu 0.33%
iter 1630: loss 2.1940, time 16.70ms, mfu 0.33%
iter 1635: loss 2.2263, time 17.01ms, mfu 0.33%
iter 1640: loss 2.1284, time 16.07ms, mfu 0.33%
iter 1645: loss 2.1610, time 16.90ms, mfu 0.33%
iter 1650: loss 2.1029, time 16.47ms, mfu 0.33%
iter 1655: loss 2.1047, time 17.49ms, mfu 0.33%
iter 1660: loss 2.1548, time 16.17ms, mfu 0.33%
iter 1665: loss 2.1248, time 16.00ms, mfu 0.33%
iter 1670: loss 2.1261, time 16.09ms, mfu 0.33%
iter 1675: loss 2.1683, time 16.03ms, mfu 0.33%
iter 1680: loss 2.1130, time 16.67ms, mfu 0.33%
iter 1685: loss 2.1509, time 26.70ms, mfu 0.32%
iter 1690: loss 2.1304, time 16.09ms, mfu 0.32%
iter 1695: loss 2.1261, time 15.89ms, mfu 0.32%
iter 1700: loss 2.2246, time 15.95ms, mfu 0.32%
iter 1705: loss 2.1657, time 15.93ms, mfu 0.33%
iter 1710: loss 2.1450, time 18.91ms, mfu 0.32%
iter 1715: loss 2.1416, time 15.95ms, mfu 0.32%
iter 1720: loss 2.1927, time 16.01ms, mfu 0.33%
iter 1725: loss 2.1432, time 15.83ms, mfu 0.33%
iter 1730: loss 2.1734, time 17.34ms, mfu 0.33%
iter 1735: loss 2.1791, time 17.58ms, mfu 0.33%
iter 1740: loss 2.0690, time 16.49ms, mfu 0.33%
iter 1745: loss 2.0647, time 17.06ms, mfu 0.33%
step 1750: train loss 2.0694, val loss 2.0875
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
iter 1750: loss 2.0954, time 359.02ms, mfu 0.29%
iter 1755: loss 2.1825, time 16.73ms, mfu 0.30%
iter 1760: loss 2.1507, time 16.48ms, mfu 0.30%
iter 1765: loss 2.1219, time 16.14ms, mfu 0.30%
iter 1770: loss 2.1225, time 16.16ms, mfu 0.31%
iter 1775: loss 2.2037, time 16.78ms, mfu 0.31%
iter 1780: loss 2.1370, time 20.93ms, mfu 0.30%
iter 1785: loss 2.1058, time 17.12ms, mfu 0.31%
iter 1790: loss 2.2118, time 16.09ms, mfu 0.31%
iter 1795: loss 2.1915, time 16.73ms, mfu 0.31%
iter 1800: loss 2.1840, time 16.42ms, mfu 0.31%
iter 1805: loss 2.1791, time 16.20ms, mfu 0.32%
iter 1810: loss 2.1705, time 17.17ms, mfu 0.32%
iter 1815: loss 2.1584, time 16.08ms, mfu 0.32%
iter 1820: loss 2.1244, time 16.21ms, mfu 0.32%
iter 1825: loss 2.1442, time 30.76ms, mfu 0.31%
iter 1830: loss 2.1600, time 23.08ms, mfu 0.30%
iter 1835: loss 2.1604, time 24.57ms, mfu 0.29%
iter 1840: loss 2.1058, time 21.32ms, mfu 0.29%
iter 1845: loss 2.1047, time 21.16ms, mfu 0.28%
iter 1850: loss 2.1525, time 21.05ms, mfu 0.28%
iter 1855: loss 2.1551, time 22.15ms, mfu 0.28%
iter 1860: loss 2.1578, time 22.67ms, mfu 0.27%
iter 1865: loss 2.1082, time 30.15ms, mfu 0.27%
iter 1870: loss 2.0782, time 22.62ms, mfu 0.26%
iter 1875: loss 2.0936, time 19.64ms, mfu 0.26%
iter 1880: loss 2.1490, time 21.23ms, mfu 0.26%
iter 1885: loss 2.1627, time 21.94ms, mfu 0.26%
iter 1890: loss 2.1533, time 21.44ms, mfu 0.26%
iter 1895: loss 2.1180, time 19.81ms, mfu 0.26%
iter 1900: loss 2.0953, time 22.94ms, mfu 0.26%
iter 1905: loss 2.0935, time 22.89ms, mfu 0.26%
iter 1910: loss 2.1387, time 25.85ms, mfu 0.25%
iter 1915: loss 2.1023, time 25.24ms, mfu 0.25%
iter 1920: loss 2.1228, time 21.13ms, mfu 0.25%
iter 1925: loss 2.1275, time 27.07ms, mfu 0.25%
iter 1930: loss 2.1558, time 26.73ms, mfu 0.24%
iter 1935: loss 2.1129, time 29.62ms, mfu 0.24%
iter 1940: loss 2.1466, time 26.01ms, mfu 0.23%
iter 1945: loss 2.0904, time 26.24ms, mfu 0.23%
iter 1950: loss 2.1078, time 16.32ms, mfu 0.24%
iter 1955: loss 2.0690, time 16.18ms, mfu 0.25%
iter 1960: loss 2.1185, time 16.33ms, mfu 0.26%
iter 1965: loss 2.1433, time 16.13ms, mfu 0.27%
iter 1970: loss 2.1345, time 16.14ms, mfu 0.27%
iter 1975: loss 2.1224, time 15.78ms, mfu 0.28%
iter 1980: loss 2.1009, time 16.05ms, mfu 0.29%
iter 1985: loss 2.1024, time 20.85ms, mfu 0.28%
iter 1990: loss 2.1043, time 16.60ms, mfu 0.29%
iter 1995: loss 2.0966, time 15.80ms, mfu 0.29%
step 2000: train loss 2.0409, val loss 2.0788
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2
iter 2000: loss 2.1024, time 357.09ms, mfu 0.27%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd128_batch_size16_max_iters2000_dropout0.2 at: 
wandb: Find logs at: wandb/run-20251024_005536-28wzbxkh/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 256
Overriding: batch_size = 8
Overriding: max_iters = 1000
Overriding: dropout = 0.1
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.1
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.1
tokens per iteration will be: 1,024
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 4.74M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 4,768,000 parameters
num non-decayed parameter tensors: 13, with 3,328 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_005630-tg8g63x7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.1
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/tg8g63x7
step 0: train loss 4.2260, val loss 4.2239
iter 0: loss 4.2279, time 906.32ms, mfu -100.00%
iter 5: loss 3.5800, time 24.20ms, mfu 0.42%
iter 10: loss 3.3645, time 31.28ms, mfu 0.41%
iter 15: loss 3.1840, time 23.52ms, mfu 0.41%
iter 20: loss 3.0183, time 21.81ms, mfu 0.42%
iter 25: loss 2.9606, time 22.11ms, mfu 0.42%
iter 30: loss 2.7907, time 20.83ms, mfu 0.43%
iter 35: loss 2.7732, time 24.86ms, mfu 0.42%
iter 40: loss 2.6993, time 20.10ms, mfu 0.43%
iter 45: loss 2.6920, time 24.01ms, mfu 0.43%
iter 50: loss 2.6316, time 20.79ms, mfu 0.44%
iter 55: loss 2.7356, time 20.18ms, mfu 0.44%
iter 60: loss 2.6296, time 21.91ms, mfu 0.44%
iter 65: loss 2.6698, time 25.45ms, mfu 0.44%
iter 70: loss 2.6215, time 23.65ms, mfu 0.44%
iter 75: loss 2.6281, time 20.08ms, mfu 0.45%
iter 80: loss 2.6106, time 20.63ms, mfu 0.45%
iter 85: loss 2.6225, time 24.18ms, mfu 0.45%
iter 90: loss 2.6673, time 26.12ms, mfu 0.44%
iter 95: loss 2.5896, time 20.34ms, mfu 0.45%
iter 100: loss 2.5966, time 27.79ms, mfu 0.44%
iter 105: loss 2.5300, time 26.55ms, mfu 0.43%
iter 110: loss 2.5936, time 31.49ms, mfu 0.42%
iter 115: loss 2.5482, time 25.87ms, mfu 0.42%
iter 120: loss 2.4806, time 25.63ms, mfu 0.42%
iter 125: loss 2.5872, time 26.35ms, mfu 0.41%
iter 130: loss 2.5892, time 16.35ms, mfu 0.43%
iter 135: loss 2.6047, time 15.92ms, mfu 0.45%
iter 140: loss 2.5555, time 15.77ms, mfu 0.47%
iter 145: loss 2.5126, time 17.58ms, mfu 0.48%
iter 150: loss 2.5185, time 16.88ms, mfu 0.49%
iter 155: loss 2.5344, time 25.49ms, mfu 0.48%
iter 160: loss 2.4721, time 16.12ms, mfu 0.50%
iter 165: loss 2.5211, time 15.41ms, mfu 0.51%
iter 170: loss 2.5205, time 16.23ms, mfu 0.52%
iter 175: loss 2.4874, time 15.32ms, mfu 0.54%
iter 180: loss 2.5244, time 15.74ms, mfu 0.55%
iter 185: loss 2.4726, time 15.69ms, mfu 0.56%
iter 190: loss 2.5036, time 16.54ms, mfu 0.56%
iter 195: loss 2.4710, time 15.74ms, mfu 0.57%
iter 200: loss 2.4675, time 15.55ms, mfu 0.58%
iter 205: loss 2.5166, time 15.15ms, mfu 0.59%
iter 210: loss 2.5064, time 18.42ms, mfu 0.58%
iter 215: loss 2.5091, time 17.20ms, mfu 0.58%
iter 220: loss 2.5020, time 15.47ms, mfu 0.59%
iter 225: loss 2.4260, time 16.77ms, mfu 0.59%
iter 230: loss 2.4956, time 15.57ms, mfu 0.60%
iter 235: loss 2.4928, time 15.50ms, mfu 0.60%
iter 240: loss 2.4292, time 15.97ms, mfu 0.61%
iter 245: loss 2.6015, time 16.23ms, mfu 0.61%
step 250: train loss 2.4765, val loss 2.4588
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.1
iter 250: loss 2.4977, time 404.43ms, mfu 0.55%
iter 255: loss 2.4583, time 15.56ms, mfu 0.56%
iter 260: loss 2.5000, time 15.74ms, mfu 0.57%
iter 265: loss 2.4792, time 15.41ms, mfu 0.58%
iter 270: loss 2.4446, time 15.45ms, mfu 0.58%
iter 275: loss 2.4655, time 15.81ms, mfu 0.59%
iter 280: loss 2.4763, time 17.17ms, mfu 0.59%
iter 285: loss 2.4918, time 15.93ms, mfu 0.59%
iter 290: loss 2.4978, time 15.73ms, mfu 0.60%
iter 295: loss 2.4769, time 25.35ms, mfu 0.58%
iter 300: loss 2.4007, time 15.93ms, mfu 0.58%
iter 305: loss 2.4298, time 16.11ms, mfu 0.59%
iter 310: loss 2.5649, time 16.78ms, mfu 0.59%
iter 315: loss 2.3930, time 16.34ms, mfu 0.59%
iter 320: loss 2.4808, time 16.26ms, mfu 0.60%
iter 325: loss 2.4110, time 16.14ms, mfu 0.60%
iter 330: loss 2.3825, time 17.73ms, mfu 0.60%
iter 335: loss 2.4289, time 16.53ms, mfu 0.60%
iter 340: loss 2.4743, time 17.69ms, mfu 0.59%
iter 345: loss 2.3153, time 15.41ms, mfu 0.60%
iter 350: loss 2.4018, time 18.09ms, mfu 0.60%
iter 355: loss 2.3815, time 16.04ms, mfu 0.60%
iter 360: loss 2.4290, time 15.94ms, mfu 0.60%
iter 365: loss 2.4344, time 16.36ms, mfu 0.60%
iter 370: loss 2.4461, time 16.36ms, mfu 0.61%
iter 375: loss 2.4363, time 15.56ms, mfu 0.61%
iter 380: loss 2.4306, time 15.67ms, mfu 0.61%
iter 385: loss 2.4212, time 16.27ms, mfu 0.61%
iter 390: loss 2.4277, time 17.52ms, mfu 0.61%
iter 395: loss 2.4261, time 17.36ms, mfu 0.61%
iter 400: loss 2.4440, time 28.24ms, mfu 0.58%
iter 405: loss 2.4351, time 15.39ms, mfu 0.59%
iter 410: loss 2.3917, time 16.16ms, mfu 0.59%
iter 415: loss 2.3269, time 21.18ms, mfu 0.58%
iter 420: loss 2.3223, time 15.87ms, mfu 0.59%
iter 425: loss 2.4383, time 20.86ms, mfu 0.58%
iter 430: loss 2.4490, time 16.12ms, mfu 0.58%
iter 435: loss 2.3416, time 16.82ms, mfu 0.58%
iter 440: loss 2.3515, time 15.96ms, mfu 0.59%
iter 445: loss 2.4029, time 16.26ms, mfu 0.59%
iter 450: loss 2.3869, time 16.58ms, mfu 0.59%
iter 455: loss 2.3504, time 16.17ms, mfu 0.60%
iter 460: loss 2.3577, time 15.89ms, mfu 0.60%
iter 465: loss 2.4108, time 15.59ms, mfu 0.61%
iter 470: loss 2.3692, time 16.32ms, mfu 0.61%
iter 475: loss 2.3317, time 18.60ms, mfu 0.60%
iter 480: loss 2.3345, time 15.70ms, mfu 0.60%
iter 485: loss 2.3919, time 15.50ms, mfu 0.61%
iter 490: loss 2.3568, time 15.83ms, mfu 0.61%
iter 495: loss 2.3012, time 15.88ms, mfu 0.61%
step 500: train loss 2.2975, val loss 2.3043
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.1
iter 500: loss 2.3354, time 435.14ms, mfu 0.56%
iter 505: loss 2.3193, time 15.86ms, mfu 0.56%
iter 510: loss 2.3045, time 16.10ms, mfu 0.57%
iter 515: loss 2.3016, time 15.85ms, mfu 0.58%
iter 520: loss 2.3284, time 15.47ms, mfu 0.58%
iter 525: loss 2.3067, time 16.06ms, mfu 0.59%
iter 530: loss 2.2657, time 16.10ms, mfu 0.59%
iter 535: loss 2.2852, time 15.59ms, mfu 0.60%
iter 540: loss 2.2906, time 16.17ms, mfu 0.60%
iter 545: loss 2.2800, time 15.88ms, mfu 0.60%
iter 550: loss 2.2052, time 16.19ms, mfu 0.61%
iter 555: loss 2.2715, time 15.68ms, mfu 0.61%
iter 560: loss 2.2959, time 15.97ms, mfu 0.61%
iter 565: loss 2.2647, time 15.74ms, mfu 0.62%
iter 570: loss 2.2151, time 15.61ms, mfu 0.62%
iter 575: loss 2.2857, time 16.46ms, mfu 0.62%
iter 580: loss 2.2316, time 16.69ms, mfu 0.62%
iter 585: loss 2.1927, time 16.57ms, mfu 0.62%
iter 590: loss 2.2399, time 22.25ms, mfu 0.60%
iter 595: loss 2.2816, time 16.70ms, mfu 0.60%
iter 600: loss 2.1502, time 29.64ms, mfu 0.57%
iter 605: loss 2.2146, time 28.96ms, mfu 0.55%
iter 610: loss 2.1987, time 21.39ms, mfu 0.54%
iter 615: loss 2.1503, time 30.26ms, mfu 0.52%
iter 620: loss 2.2039, time 22.19ms, mfu 0.52%
iter 625: loss 2.0996, time 20.80ms, mfu 0.51%
iter 630: loss 2.2550, time 20.56ms, mfu 0.51%
iter 635: loss 2.1872, time 35.35ms, mfu 0.49%
iter 640: loss 2.1604, time 19.95ms, mfu 0.49%
iter 645: loss 2.0841, time 20.70ms, mfu 0.49%
iter 650: loss 2.1276, time 20.18ms, mfu 0.49%
iter 655: loss 2.1212, time 20.25ms, mfu 0.49%
iter 660: loss 2.2307, time 21.15ms, mfu 0.49%
iter 665: loss 2.1276, time 21.51ms, mfu 0.49%
iter 670: loss 2.1999, time 19.89ms, mfu 0.49%
iter 675: loss 2.3051, time 19.95ms, mfu 0.49%
iter 680: loss 2.1256, time 32.94ms, mfu 0.47%
iter 685: loss 2.1616, time 21.73ms, mfu 0.47%
iter 690: loss 2.1450, time 21.60ms, mfu 0.47%
iter 695: loss 2.1708, time 22.04ms, mfu 0.47%
iter 700: loss 2.1246, time 23.12ms, mfu 0.47%
iter 705: loss 2.1991, time 25.71ms, mfu 0.46%
iter 710: loss 2.1703, time 26.36ms, mfu 0.45%
iter 715: loss 2.1460, time 25.32ms, mfu 0.45%
iter 720: loss 2.1213, time 21.32ms, mfu 0.45%
iter 725: loss 2.0921, time 16.15ms, mfu 0.47%
iter 730: loss 2.0951, time 16.14ms, mfu 0.48%
iter 735: loss 2.1410, time 16.01ms, mfu 0.50%
iter 740: loss 2.1678, time 16.18ms, mfu 0.51%
iter 745: loss 2.1215, time 15.80ms, mfu 0.52%
step 750: train loss 2.0389, val loss 2.0919
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.1
iter 750: loss 2.0968, time 459.00ms, mfu 0.47%
iter 755: loss 2.1371, time 19.45ms, mfu 0.48%
iter 760: loss 2.1552, time 15.82ms, mfu 0.49%
iter 765: loss 2.1127, time 25.16ms, mfu 0.48%
iter 770: loss 2.1567, time 26.01ms, mfu 0.48%
iter 775: loss 2.0799, time 21.41ms, mfu 0.47%
iter 780: loss 2.0995, time 23.65ms, mfu 0.47%
iter 785: loss 2.1176, time 17.02ms, mfu 0.48%
iter 790: loss 2.0727, time 18.57ms, mfu 0.49%
iter 795: loss 2.0928, time 17.45ms, mfu 0.50%
iter 800: loss 2.0943, time 29.21ms, mfu 0.48%
iter 805: loss 2.1239, time 16.14ms, mfu 0.50%
iter 810: loss 2.0330, time 18.03ms, mfu 0.50%
iter 815: loss 2.0737, time 17.27ms, mfu 0.51%
iter 820: loss 2.0629, time 16.33ms, mfu 0.52%
iter 825: loss 2.1146, time 17.92ms, mfu 0.53%
iter 830: loss 2.0345, time 16.24ms, mfu 0.54%
iter 835: loss 2.1032, time 16.18ms, mfu 0.54%
iter 840: loss 2.1237, time 15.90ms, mfu 0.55%
iter 845: loss 2.1025, time 16.24ms, mfu 0.56%
iter 850: loss 2.0042, time 16.28ms, mfu 0.57%
iter 855: loss 2.1041, time 16.66ms, mfu 0.57%
iter 860: loss 2.1393, time 16.73ms, mfu 0.57%
iter 865: loss 2.0699, time 16.12ms, mfu 0.58%
iter 870: loss 2.0441, time 15.67ms, mfu 0.59%
iter 875: loss 2.0537, time 15.90ms, mfu 0.59%
iter 880: loss 2.1572, time 15.98ms, mfu 0.60%
iter 885: loss 2.1386, time 16.09ms, mfu 0.60%
iter 890: loss 2.0267, time 16.47ms, mfu 0.60%
iter 895: loss 2.1782, time 17.22ms, mfu 0.60%
iter 900: loss 2.0927, time 15.94ms, mfu 0.60%
iter 905: loss 2.0714, time 18.34ms, mfu 0.60%
iter 910: loss 1.9197, time 16.26ms, mfu 0.60%
iter 915: loss 2.0472, time 15.75ms, mfu 0.60%
iter 920: loss 2.0704, time 16.88ms, mfu 0.60%
iter 925: loss 2.0844, time 15.97ms, mfu 0.61%
iter 930: loss 1.9769, time 17.42ms, mfu 0.60%
iter 935: loss 1.9440, time 16.42ms, mfu 0.60%
iter 940: loss 1.9470, time 20.82ms, mfu 0.59%
iter 945: loss 1.9956, time 16.90ms, mfu 0.59%
iter 950: loss 2.0187, time 20.43ms, mfu 0.58%
iter 955: loss 2.0673, time 18.06ms, mfu 0.58%
iter 960: loss 2.1540, time 18.27ms, mfu 0.58%
iter 965: loss 2.0290, time 16.32ms, mfu 0.58%
iter 970: loss 1.9549, time 19.74ms, mfu 0.58%
iter 975: loss 1.8721, time 17.61ms, mfu 0.58%
iter 980: loss 2.0495, time 20.29ms, mfu 0.57%
iter 985: loss 1.9692, time 16.28ms, mfu 0.57%
iter 990: loss 2.1194, time 16.36ms, mfu 0.58%
iter 995: loss 2.0496, time 22.80ms, mfu 0.56%
step 1000: train loss 1.9791, val loss 2.0299
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.1
iter 1000: loss 2.0350, time 453.73ms, mfu 0.51%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.1 at: 
wandb: Find logs at: wandb/run-20251024_005630-tg8g63x7/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 256
Overriding: batch_size = 8
Overriding: max_iters = 1000
Overriding: dropout = 0.2
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.2
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.2
tokens per iteration will be: 1,024
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 4.74M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 4,768,000 parameters
num non-decayed parameter tensors: 13, with 3,328 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_005702-yq5pagkb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.2
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/yq5pagkb
step 0: train loss 4.2260, val loss 4.2239
iter 0: loss 4.2206, time 899.43ms, mfu -100.00%
iter 5: loss 3.6248, time 16.01ms, mfu 0.63%
iter 10: loss 3.3980, time 18.66ms, mfu 0.62%
iter 15: loss 3.2740, time 16.73ms, mfu 0.62%
iter 20: loss 3.0814, time 16.27ms, mfu 0.62%
iter 25: loss 3.0199, time 16.17ms, mfu 0.62%
iter 30: loss 2.8295, time 16.25ms, mfu 0.62%
iter 35: loss 2.8136, time 15.75ms, mfu 0.62%
iter 40: loss 2.7280, time 17.71ms, mfu 0.62%
iter 45: loss 2.7386, time 16.31ms, mfu 0.62%
iter 50: loss 2.6574, time 16.49ms, mfu 0.62%
iter 55: loss 2.7598, time 16.24ms, mfu 0.62%
iter 60: loss 2.6444, time 18.01ms, mfu 0.61%
iter 65: loss 2.6710, time 16.75ms, mfu 0.61%
iter 70: loss 2.6557, time 16.22ms, mfu 0.61%
iter 75: loss 2.6423, time 16.44ms, mfu 0.61%
iter 80: loss 2.6194, time 16.29ms, mfu 0.61%
iter 85: loss 2.6271, time 17.70ms, mfu 0.61%
iter 90: loss 2.6853, time 16.10ms, mfu 0.61%
iter 95: loss 2.6171, time 16.34ms, mfu 0.61%
iter 100: loss 2.6225, time 15.84ms, mfu 0.61%
iter 105: loss 2.5646, time 16.13ms, mfu 0.62%
iter 110: loss 2.6051, time 16.19ms, mfu 0.62%
iter 115: loss 2.5892, time 17.18ms, mfu 0.61%
iter 120: loss 2.5140, time 16.07ms, mfu 0.62%
iter 125: loss 2.6108, time 16.53ms, mfu 0.61%
iter 130: loss 2.6069, time 16.16ms, mfu 0.62%
iter 135: loss 2.5838, time 17.56ms, mfu 0.61%
iter 140: loss 2.5775, time 16.69ms, mfu 0.61%
iter 145: loss 2.5184, time 19.79ms, mfu 0.60%
iter 150: loss 2.5365, time 16.89ms, mfu 0.60%
iter 155: loss 2.5612, time 15.91ms, mfu 0.60%
iter 160: loss 2.5057, time 21.14ms, mfu 0.59%
iter 165: loss 2.5480, time 16.76ms, mfu 0.59%
iter 170: loss 2.5239, time 20.39ms, mfu 0.58%
iter 175: loss 2.5223, time 16.71ms, mfu 0.59%
iter 180: loss 2.5381, time 16.03ms, mfu 0.59%
iter 185: loss 2.4912, time 15.50ms, mfu 0.60%
iter 190: loss 2.5049, time 15.69ms, mfu 0.60%
iter 195: loss 2.4993, time 15.89ms, mfu 0.60%
iter 200: loss 2.4878, time 16.39ms, mfu 0.61%
iter 205: loss 2.5322, time 15.48ms, mfu 0.61%
iter 210: loss 2.5194, time 15.99ms, mfu 0.61%
iter 215: loss 2.5430, time 25.95ms, mfu 0.59%
iter 220: loss 2.5338, time 20.30ms, mfu 0.58%
iter 225: loss 2.4204, time 20.46ms, mfu 0.57%
iter 230: loss 2.5074, time 15.89ms, mfu 0.58%
iter 235: loss 2.5023, time 15.66ms, mfu 0.59%
iter 240: loss 2.4472, time 15.66ms, mfu 0.59%
iter 245: loss 2.6178, time 15.81ms, mfu 0.60%
step 250: train loss 2.4884, val loss 2.4699
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.2
iter 250: loss 2.5091, time 408.03ms, mfu 0.54%
iter 255: loss 2.4733, time 21.29ms, mfu 0.53%
iter 260: loss 2.5337, time 16.06ms, mfu 0.54%
iter 265: loss 2.5094, time 31.82ms, mfu 0.52%
iter 270: loss 2.4713, time 29.88ms, mfu 0.50%
iter 275: loss 2.4967, time 38.60ms, mfu 0.48%
iter 280: loss 2.4915, time 16.49ms, mfu 0.49%
iter 285: loss 2.5052, time 15.57ms, mfu 0.51%
iter 290: loss 2.5242, time 16.64ms, mfu 0.52%
iter 295: loss 2.5110, time 16.80ms, mfu 0.53%
iter 300: loss 2.4357, time 16.98ms, mfu 0.53%
iter 305: loss 2.4457, time 16.66ms, mfu 0.54%
iter 310: loss 2.5802, time 16.25ms, mfu 0.55%
iter 315: loss 2.4267, time 16.27ms, mfu 0.56%
iter 320: loss 2.4735, time 15.92ms, mfu 0.56%
iter 325: loss 2.4501, time 16.77ms, mfu 0.57%
iter 330: loss 2.4131, time 30.62ms, mfu 0.54%
iter 335: loss 2.4550, time 29.64ms, mfu 0.52%
iter 340: loss 2.5034, time 21.78ms, mfu 0.52%
iter 345: loss 2.3367, time 24.23ms, mfu 0.51%
iter 350: loss 2.4348, time 21.17ms, mfu 0.50%
iter 355: loss 2.4164, time 21.16ms, mfu 0.50%
iter 360: loss 2.4414, time 20.94ms, mfu 0.50%
iter 365: loss 2.4602, time 21.08ms, mfu 0.50%
iter 370: loss 2.4854, time 22.58ms, mfu 0.49%
iter 375: loss 2.4619, time 20.61ms, mfu 0.49%
iter 380: loss 2.4754, time 21.12ms, mfu 0.49%
iter 385: loss 2.4456, time 20.80ms, mfu 0.49%
iter 390: loss 2.4539, time 28.11ms, mfu 0.48%
iter 395: loss 2.4847, time 21.64ms, mfu 0.48%
iter 400: loss 2.4794, time 34.45ms, mfu 0.46%
iter 405: loss 2.4546, time 21.99ms, mfu 0.46%
iter 410: loss 2.4253, time 21.15ms, mfu 0.46%
iter 415: loss 2.3866, time 25.79ms, mfu 0.45%
iter 420: loss 2.3742, time 23.41ms, mfu 0.45%
iter 425: loss 2.4988, time 21.22ms, mfu 0.45%
iter 430: loss 2.4935, time 27.06ms, mfu 0.45%
iter 435: loss 2.4116, time 32.37ms, mfu 0.43%
iter 440: loss 2.4130, time 26.45ms, mfu 0.43%
iter 445: loss 2.4622, time 27.54ms, mfu 0.42%
iter 450: loss 2.4464, time 24.00ms, mfu 0.42%
iter 455: loss 2.4234, time 25.76ms, mfu 0.42%
iter 460: loss 2.4316, time 16.13ms, mfu 0.44%
iter 465: loss 2.4844, time 15.95ms, mfu 0.46%
iter 470: loss 2.4187, time 16.13ms, mfu 0.48%
iter 475: loss 2.4224, time 16.08ms, mfu 0.49%
iter 480: loss 2.4173, time 19.36ms, mfu 0.49%
iter 485: loss 2.4561, time 16.23ms, mfu 0.51%
iter 490: loss 2.4406, time 17.77ms, mfu 0.51%
iter 495: loss 2.4018, time 16.51ms, mfu 0.52%
step 500: train loss 2.3910, val loss 2.3945
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.2
iter 500: loss 2.4135, time 439.05ms, mfu 0.47%
iter 505: loss 2.4221, time 21.56ms, mfu 0.47%
iter 510: loss 2.3911, time 16.51ms, mfu 0.49%
iter 515: loss 2.4374, time 17.20ms, mfu 0.50%
iter 520: loss 2.4152, time 17.93ms, mfu 0.50%
iter 525: loss 2.4043, time 21.48ms, mfu 0.50%
iter 530: loss 2.3800, time 16.69ms, mfu 0.51%
iter 535: loss 2.3958, time 18.50ms, mfu 0.51%
iter 540: loss 2.4270, time 20.34ms, mfu 0.51%
iter 545: loss 2.3842, time 15.45ms, mfu 0.53%
iter 550: loss 2.3170, time 18.08ms, mfu 0.53%
iter 555: loss 2.3953, time 15.75ms, mfu 0.54%
iter 560: loss 2.3917, time 15.32ms, mfu 0.55%
iter 565: loss 2.3890, time 16.83ms, mfu 0.56%
iter 570: loss 2.3424, time 16.76ms, mfu 0.56%
iter 575: loss 2.4128, time 15.91ms, mfu 0.57%
iter 580: loss 2.3581, time 15.50ms, mfu 0.58%
iter 585: loss 2.3466, time 16.25ms, mfu 0.58%
iter 590: loss 2.3747, time 16.03ms, mfu 0.59%
iter 595: loss 2.4102, time 15.69ms, mfu 0.59%
iter 600: loss 2.3029, time 18.45ms, mfu 0.59%
iter 605: loss 2.3565, time 16.34ms, mfu 0.59%
iter 610: loss 2.3200, time 15.84ms, mfu 0.60%
iter 615: loss 2.2901, time 25.23ms, mfu 0.58%
iter 620: loss 2.3814, time 16.04ms, mfu 0.58%
iter 625: loss 2.2483, time 15.96ms, mfu 0.59%
iter 630: loss 2.3907, time 16.28ms, mfu 0.59%
iter 635: loss 2.3363, time 15.64ms, mfu 0.60%
iter 640: loss 2.2981, time 17.38ms, mfu 0.59%
iter 645: loss 2.2614, time 15.61ms, mfu 0.60%
iter 650: loss 2.2951, time 17.53ms, mfu 0.60%
iter 655: loss 2.2763, time 16.19ms, mfu 0.60%
iter 660: loss 2.3547, time 16.63ms, mfu 0.60%
iter 665: loss 2.2934, time 16.46ms, mfu 0.60%
iter 670: loss 2.3754, time 18.40ms, mfu 0.60%
iter 675: loss 2.4254, time 16.87ms, mfu 0.60%
iter 680: loss 2.3046, time 16.23ms, mfu 0.60%
iter 685: loss 2.3033, time 16.53ms, mfu 0.60%
iter 690: loss 2.2803, time 20.93ms, mfu 0.59%
iter 695: loss 2.3055, time 16.48ms, mfu 0.59%
iter 700: loss 2.2732, time 19.58ms, mfu 0.58%
iter 705: loss 2.3496, time 16.22ms, mfu 0.59%
iter 710: loss 2.2933, time 16.30ms, mfu 0.59%
iter 715: loss 2.3160, time 15.58ms, mfu 0.60%
iter 720: loss 2.2833, time 18.04ms, mfu 0.59%
iter 725: loss 2.2586, time 15.86ms, mfu 0.60%
iter 730: loss 2.2721, time 16.03ms, mfu 0.60%
iter 735: loss 2.3083, time 16.13ms, mfu 0.60%
iter 740: loss 2.3031, time 15.61ms, mfu 0.61%
iter 745: loss 2.2577, time 16.04ms, mfu 0.61%
step 750: train loss 2.1857, val loss 2.2258
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.2
iter 750: loss 2.2794, time 510.03ms, mfu 0.55%
iter 755: loss 2.3004, time 16.15ms, mfu 0.56%
iter 760: loss 2.3099, time 17.87ms, mfu 0.56%
iter 765: loss 2.2601, time 16.44ms, mfu 0.56%
iter 770: loss 2.3041, time 18.35ms, mfu 0.56%
iter 775: loss 2.2388, time 18.95ms, mfu 0.56%
iter 780: loss 2.2852, time 15.77ms, mfu 0.57%
iter 785: loss 2.2854, time 15.71ms, mfu 0.58%
iter 790: loss 2.2511, time 16.52ms, mfu 0.58%
iter 795: loss 2.2416, time 17.03ms, mfu 0.58%
iter 800: loss 2.2714, time 16.12ms, mfu 0.59%
iter 805: loss 2.2799, time 16.28ms, mfu 0.59%
iter 810: loss 2.2103, time 16.99ms, mfu 0.59%
iter 815: loss 2.2428, time 16.36ms, mfu 0.59%
iter 820: loss 2.2598, time 16.06ms, mfu 0.60%
iter 825: loss 2.2810, time 15.61ms, mfu 0.60%
iter 830: loss 2.2051, time 16.01ms, mfu 0.60%
iter 835: loss 2.2476, time 15.61ms, mfu 0.61%
iter 840: loss 2.2687, time 16.36ms, mfu 0.61%
iter 845: loss 2.2465, time 15.84ms, mfu 0.61%
iter 850: loss 2.1966, time 15.88ms, mfu 0.61%
iter 855: loss 2.2804, time 16.02ms, mfu 0.62%
iter 860: loss 2.3359, time 16.52ms, mfu 0.62%
iter 865: loss 2.2066, time 16.00ms, mfu 0.62%
iter 870: loss 2.2157, time 16.18ms, mfu 0.62%
iter 875: loss 2.2207, time 17.70ms, mfu 0.61%
iter 880: loss 2.3268, time 16.61ms, mfu 0.61%
iter 885: loss 2.3053, time 15.98ms, mfu 0.61%
iter 890: loss 2.2130, time 16.01ms, mfu 0.62%
iter 895: loss 2.3222, time 16.07ms, mfu 0.62%
iter 900: loss 2.2547, time 16.67ms, mfu 0.62%
iter 905: loss 2.2064, time 16.44ms, mfu 0.62%
iter 910: loss 2.1071, time 17.45ms, mfu 0.61%
iter 915: loss 2.2478, time 26.54ms, mfu 0.59%
iter 920: loss 2.2317, time 21.69ms, mfu 0.58%
iter 925: loss 2.2431, time 21.43ms, mfu 0.57%
iter 930: loss 2.1480, time 19.94ms, mfu 0.56%
iter 935: loss 2.1539, time 20.64ms, mfu 0.55%
iter 940: loss 2.1474, time 20.98ms, mfu 0.55%
iter 945: loss 2.1593, time 24.39ms, mfu 0.53%
iter 950: loss 2.1754, time 22.12ms, mfu 0.53%
iter 955: loss 2.2360, time 20.58ms, mfu 0.52%
iter 960: loss 2.2858, time 26.00ms, mfu 0.51%
iter 965: loss 2.2056, time 20.04ms, mfu 0.51%
iter 970: loss 2.1392, time 20.70ms, mfu 0.51%
iter 975: loss 2.0624, time 20.39ms, mfu 0.51%
iter 980: loss 2.2080, time 21.11ms, mfu 0.50%
iter 985: loss 2.1457, time 21.04ms, mfu 0.50%
iter 990: loss 2.3189, time 25.63ms, mfu 0.49%
iter 995: loss 2.2440, time 21.01ms, mfu 0.49%
step 1000: train loss 2.1441, val loss 2.1557
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.2
iter 1000: loss 2.2231, time 539.03ms, mfu 0.44%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters1000_dropout0.2 at: 
wandb: Find logs at: wandb/run-20251024_005702-yq5pagkb/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 256
Overriding: batch_size = 8
Overriding: max_iters = 2000
Overriding: dropout = 0.1
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
tokens per iteration will be: 1,024
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 4.74M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 4,768,000 parameters
num non-decayed parameter tensors: 13, with 3,328 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_005733-ifuarvsk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/ifuarvsk
step 0: train loss 4.2260, val loss 4.2239
iter 0: loss 4.2279, time 984.54ms, mfu -100.00%
iter 5: loss 3.5800, time 15.85ms, mfu 0.64%
iter 10: loss 3.3645, time 16.17ms, mfu 0.64%
iter 15: loss 3.1840, time 17.41ms, mfu 0.63%
iter 20: loss 3.0183, time 17.01ms, mfu 0.63%
iter 25: loss 2.9606, time 19.07ms, mfu 0.62%
iter 30: loss 2.7907, time 15.81ms, mfu 0.62%
iter 35: loss 2.7732, time 15.90ms, mfu 0.62%
iter 40: loss 2.6993, time 19.46ms, mfu 0.61%
iter 45: loss 2.6920, time 16.51ms, mfu 0.61%
iter 50: loss 2.6316, time 18.22ms, mfu 0.61%
iter 55: loss 2.7356, time 16.10ms, mfu 0.61%
iter 60: loss 2.6296, time 29.41ms, mfu 0.58%
iter 65: loss 2.6698, time 17.05ms, mfu 0.58%
iter 70: loss 2.6215, time 15.78ms, mfu 0.59%
iter 75: loss 2.6281, time 16.32ms, mfu 0.59%
iter 80: loss 2.6106, time 16.43ms, mfu 0.59%
iter 85: loss 2.6225, time 15.97ms, mfu 0.60%
iter 90: loss 2.6673, time 16.54ms, mfu 0.60%
iter 95: loss 2.5896, time 16.68ms, mfu 0.60%
iter 100: loss 2.5966, time 16.81ms, mfu 0.60%
iter 105: loss 2.5300, time 30.91ms, mfu 0.57%
iter 110: loss 2.5936, time 27.46ms, mfu 0.55%
iter 115: loss 2.5482, time 21.22ms, mfu 0.54%
iter 120: loss 2.4806, time 26.49ms, mfu 0.53%
iter 125: loss 2.5872, time 28.10ms, mfu 0.51%
iter 130: loss 2.5892, time 21.12ms, mfu 0.51%
iter 135: loss 2.6047, time 21.15ms, mfu 0.51%
iter 140: loss 2.5555, time 28.52ms, mfu 0.49%
iter 145: loss 2.5126, time 21.09ms, mfu 0.49%
iter 150: loss 2.5185, time 22.52ms, mfu 0.48%
iter 155: loss 2.5344, time 25.95ms, mfu 0.48%
iter 160: loss 2.4721, time 19.77ms, mfu 0.48%
iter 165: loss 2.5211, time 25.00ms, mfu 0.47%
iter 170: loss 2.5205, time 21.10ms, mfu 0.47%
iter 175: loss 2.4874, time 21.75ms, mfu 0.47%
iter 180: loss 2.5244, time 21.25ms, mfu 0.47%
iter 185: loss 2.4726, time 22.63ms, mfu 0.47%
iter 190: loss 2.5036, time 23.93ms, mfu 0.46%
iter 195: loss 2.4710, time 25.54ms, mfu 0.46%
iter 200: loss 2.4675, time 42.75ms, mfu 0.44%
iter 205: loss 2.5166, time 28.15ms, mfu 0.43%
iter 210: loss 2.5064, time 27.12ms, mfu 0.42%
iter 215: loss 2.5091, time 28.50ms, mfu 0.42%
iter 220: loss 2.5020, time 24.91ms, mfu 0.41%
iter 225: loss 2.4260, time 26.86ms, mfu 0.41%
iter 230: loss 2.4956, time 16.75ms, mfu 0.43%
iter 235: loss 2.4928, time 16.44ms, mfu 0.45%
iter 240: loss 2.4292, time 18.23ms, mfu 0.46%
iter 245: loss 2.6015, time 16.37ms, mfu 0.47%
step 250: train loss 2.4765, val loss 2.4588
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
iter 250: loss 2.4977, time 406.46ms, mfu 0.43%
iter 255: loss 2.4583, time 16.81ms, mfu 0.45%
iter 260: loss 2.5000, time 16.34ms, mfu 0.46%
iter 265: loss 2.4792, time 15.73ms, mfu 0.48%
iter 270: loss 2.4446, time 17.60ms, mfu 0.49%
iter 275: loss 2.4655, time 18.61ms, mfu 0.50%
iter 280: loss 2.4763, time 15.94ms, mfu 0.51%
iter 285: loss 2.4918, time 16.05ms, mfu 0.52%
iter 290: loss 2.4978, time 16.19ms, mfu 0.53%
iter 295: loss 2.4769, time 15.65ms, mfu 0.54%
iter 300: loss 2.4007, time 15.91ms, mfu 0.55%
iter 305: loss 2.4298, time 15.56ms, mfu 0.56%
iter 310: loss 2.5649, time 16.43ms, mfu 0.57%
iter 315: loss 2.3930, time 15.85ms, mfu 0.57%
iter 320: loss 2.4808, time 15.66ms, mfu 0.58%
iter 325: loss 2.4110, time 16.83ms, mfu 0.58%
iter 330: loss 2.3825, time 16.59ms, mfu 0.59%
iter 335: loss 2.4289, time 15.76ms, mfu 0.59%
iter 340: loss 2.4743, time 15.89ms, mfu 0.60%
iter 345: loss 2.3153, time 16.21ms, mfu 0.60%
iter 350: loss 2.4018, time 16.16ms, mfu 0.60%
iter 355: loss 2.3815, time 21.28ms, mfu 0.59%
iter 360: loss 2.4290, time 16.30ms, mfu 0.59%
iter 365: loss 2.4344, time 15.96ms, mfu 0.60%
iter 370: loss 2.4461, time 15.98ms, mfu 0.60%
iter 375: loss 2.4363, time 16.76ms, mfu 0.60%
iter 380: loss 2.4306, time 27.37ms, mfu 0.58%
iter 385: loss 2.4212, time 15.95ms, mfu 0.58%
iter 390: loss 2.4277, time 16.01ms, mfu 0.59%
iter 395: loss 2.4261, time 16.22ms, mfu 0.59%
iter 400: loss 2.4440, time 16.41ms, mfu 0.59%
iter 405: loss 2.4351, time 18.46ms, mfu 0.59%
iter 410: loss 2.3917, time 22.50ms, mfu 0.57%
iter 415: loss 2.3269, time 17.63ms, mfu 0.57%
iter 420: loss 2.3223, time 21.14ms, mfu 0.57%
iter 425: loss 2.4383, time 16.66ms, mfu 0.57%
iter 430: loss 2.4490, time 16.24ms, mfu 0.57%
iter 435: loss 2.3416, time 16.12ms, mfu 0.58%
iter 440: loss 2.3515, time 16.01ms, mfu 0.58%
iter 445: loss 2.4029, time 16.09ms, mfu 0.59%
iter 450: loss 2.3869, time 20.76ms, mfu 0.58%
iter 455: loss 2.3504, time 16.53ms, mfu 0.58%
iter 460: loss 2.3577, time 19.76ms, mfu 0.58%
iter 465: loss 2.4108, time 17.09ms, mfu 0.58%
iter 470: loss 2.3692, time 16.78ms, mfu 0.58%
iter 475: loss 2.3317, time 16.70ms, mfu 0.58%
iter 480: loss 2.3345, time 15.74ms, mfu 0.59%
iter 485: loss 2.3919, time 28.00ms, mfu 0.57%
iter 490: loss 2.3568, time 15.95ms, mfu 0.57%
iter 495: loss 2.3012, time 15.93ms, mfu 0.58%
step 500: train loss 2.2975, val loss 2.3043
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
iter 500: loss 2.3354, time 466.82ms, mfu 0.52%
iter 505: loss 2.3193, time 16.36ms, mfu 0.53%
iter 510: loss 2.3045, time 15.95ms, mfu 0.54%
iter 515: loss 2.3016, time 29.65ms, mfu 0.52%
iter 520: loss 2.3284, time 34.80ms, mfu 0.50%
iter 525: loss 2.3067, time 16.45ms, mfu 0.51%
iter 530: loss 2.2657, time 16.42ms, mfu 0.52%
iter 535: loss 2.2852, time 17.27ms, mfu 0.53%
iter 540: loss 2.2906, time 16.19ms, mfu 0.54%
iter 545: loss 2.2800, time 16.67ms, mfu 0.54%
iter 550: loss 2.2052, time 15.82ms, mfu 0.55%
iter 555: loss 2.2715, time 17.40ms, mfu 0.56%
iter 560: loss 2.2959, time 18.46ms, mfu 0.56%
iter 565: loss 2.2647, time 16.50ms, mfu 0.56%
iter 570: loss 2.2151, time 16.24ms, mfu 0.57%
iter 575: loss 2.2857, time 16.08ms, mfu 0.57%
iter 580: loss 2.2316, time 17.66ms, mfu 0.57%
iter 585: loss 2.1927, time 17.23ms, mfu 0.57%
iter 590: loss 2.2399, time 16.40ms, mfu 0.58%
iter 595: loss 2.2816, time 16.88ms, mfu 0.58%
iter 600: loss 2.1502, time 16.12ms, mfu 0.59%
iter 605: loss 2.2146, time 16.71ms, mfu 0.59%
iter 610: loss 2.1987, time 16.37ms, mfu 0.59%
iter 615: loss 2.1503, time 16.83ms, mfu 0.59%
iter 620: loss 2.2039, time 16.54ms, mfu 0.59%
iter 625: loss 2.0996, time 16.18ms, mfu 0.60%
iter 630: loss 2.2550, time 16.50ms, mfu 0.60%
iter 635: loss 2.1872, time 16.78ms, mfu 0.60%
iter 640: loss 2.1604, time 16.22ms, mfu 0.60%
iter 645: loss 2.0841, time 17.60ms, mfu 0.60%
iter 650: loss 2.1276, time 15.79ms, mfu 0.60%
iter 655: loss 2.1212, time 15.79ms, mfu 0.61%
iter 660: loss 2.2307, time 15.96ms, mfu 0.61%
iter 665: loss 2.1276, time 15.80ms, mfu 0.61%
iter 670: loss 2.1999, time 16.16ms, mfu 0.61%
iter 675: loss 2.3051, time 17.51ms, mfu 0.61%
iter 680: loss 2.1256, time 16.79ms, mfu 0.61%
iter 685: loss 2.1616, time 27.67ms, mfu 0.58%
iter 690: loss 2.1450, time 23.02ms, mfu 0.57%
iter 695: loss 2.1708, time 20.71ms, mfu 0.56%
iter 700: loss 2.1246, time 22.45ms, mfu 0.55%
iter 705: loss 2.1991, time 21.76ms, mfu 0.54%
iter 710: loss 2.1703, time 23.92ms, mfu 0.53%
iter 715: loss 2.1460, time 23.23ms, mfu 0.52%
iter 720: loss 2.1213, time 22.70ms, mfu 0.51%
iter 725: loss 2.0921, time 27.28ms, mfu 0.50%
iter 730: loss 2.0951, time 23.78ms, mfu 0.49%
iter 735: loss 2.1410, time 25.91ms, mfu 0.48%
iter 740: loss 2.1678, time 21.57ms, mfu 0.48%
iter 745: loss 2.1215, time 20.94ms, mfu 0.48%
step 750: train loss 2.0389, val loss 2.0919
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
iter 750: loss 2.0968, time 537.44ms, mfu 0.43%
iter 755: loss 2.1371, time 19.69ms, mfu 0.44%
iter 760: loss 2.1552, time 21.18ms, mfu 0.45%
iter 765: loss 2.1127, time 26.86ms, mfu 0.44%
iter 770: loss 2.1567, time 23.95ms, mfu 0.44%
iter 775: loss 2.0799, time 20.40ms, mfu 0.44%
iter 780: loss 2.0995, time 31.63ms, mfu 0.43%
iter 785: loss 2.1176, time 29.48ms, mfu 0.42%
iter 790: loss 2.0727, time 25.48ms, mfu 0.42%
iter 795: loss 2.0928, time 36.98ms, mfu 0.40%
iter 800: loss 2.0943, time 26.85ms, mfu 0.40%
iter 805: loss 2.1239, time 25.58ms, mfu 0.40%
iter 810: loss 2.0330, time 17.44ms, mfu 0.42%
iter 815: loss 2.0737, time 17.24ms, mfu 0.44%
iter 820: loss 2.0629, time 17.93ms, mfu 0.45%
iter 825: loss 2.1146, time 18.71ms, mfu 0.46%
iter 830: loss 2.0345, time 16.36ms, mfu 0.47%
iter 835: loss 2.1032, time 17.22ms, mfu 0.48%
iter 840: loss 2.1237, time 15.63ms, mfu 0.50%
iter 845: loss 2.1025, time 16.54ms, mfu 0.51%
iter 850: loss 2.0042, time 18.96ms, mfu 0.51%
iter 855: loss 2.1041, time 15.80ms, mfu 0.53%
iter 860: loss 2.1393, time 16.51ms, mfu 0.54%
iter 865: loss 2.0699, time 16.39ms, mfu 0.54%
iter 870: loss 2.0441, time 16.46ms, mfu 0.55%
iter 875: loss 2.0537, time 16.51ms, mfu 0.56%
iter 880: loss 2.1572, time 15.89ms, mfu 0.56%
iter 885: loss 2.1386, time 17.43ms, mfu 0.57%
iter 890: loss 2.0267, time 17.64ms, mfu 0.57%
iter 895: loss 2.1782, time 16.20ms, mfu 0.57%
iter 900: loss 2.0927, time 15.74ms, mfu 0.58%
iter 905: loss 2.0714, time 15.24ms, mfu 0.59%
iter 910: loss 1.9197, time 16.17ms, mfu 0.59%
iter 915: loss 2.0472, time 15.78ms, mfu 0.60%
iter 920: loss 2.0704, time 16.70ms, mfu 0.60%
iter 925: loss 2.0844, time 19.63ms, mfu 0.59%
iter 930: loss 1.9769, time 15.92ms, mfu 0.59%
iter 935: loss 1.9440, time 15.86ms, mfu 0.60%
iter 940: loss 1.9470, time 15.37ms, mfu 0.60%
iter 945: loss 1.9956, time 16.33ms, mfu 0.61%
iter 950: loss 2.0187, time 15.57ms, mfu 0.61%
iter 955: loss 2.0673, time 15.86ms, mfu 0.61%
iter 960: loss 2.1540, time 15.85ms, mfu 0.62%
iter 965: loss 2.0290, time 15.28ms, mfu 0.62%
iter 970: loss 1.9549, time 15.84ms, mfu 0.62%
iter 975: loss 1.8721, time 15.52ms, mfu 0.62%
iter 980: loss 2.0495, time 15.80ms, mfu 0.63%
iter 985: loss 1.9692, time 15.28ms, mfu 0.63%
iter 990: loss 2.1194, time 16.10ms, mfu 0.63%
iter 995: loss 2.0496, time 15.69ms, mfu 0.63%
step 1000: train loss 1.9791, val loss 2.0299
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
iter 1000: loss 2.0350, time 425.79ms, mfu 0.57%
iter 1005: loss 1.9341, time 16.31ms, mfu 0.58%
iter 1010: loss 2.0080, time 16.70ms, mfu 0.58%
iter 1015: loss 1.9336, time 16.24ms, mfu 0.58%
iter 1020: loss 2.0595, time 16.20ms, mfu 0.59%
iter 1025: loss 1.9796, time 17.99ms, mfu 0.58%
iter 1030: loss 2.0632, time 15.96ms, mfu 0.59%
iter 1035: loss 1.9555, time 15.92ms, mfu 0.59%
iter 1040: loss 2.0520, time 15.77ms, mfu 0.60%
iter 1045: loss 1.9867, time 15.71ms, mfu 0.60%
iter 1050: loss 2.0321, time 15.91ms, mfu 0.61%
iter 1055: loss 2.0071, time 16.28ms, mfu 0.61%
iter 1060: loss 2.0105, time 20.12ms, mfu 0.60%
iter 1065: loss 2.1163, time 16.33ms, mfu 0.60%
iter 1070: loss 2.0198, time 15.89ms, mfu 0.60%
iter 1075: loss 2.0588, time 15.76ms, mfu 0.61%
iter 1080: loss 2.0983, time 15.83ms, mfu 0.61%
iter 1085: loss 2.0073, time 15.70ms, mfu 0.61%
iter 1090: loss 2.0446, time 16.16ms, mfu 0.61%
iter 1095: loss 2.0392, time 16.66ms, mfu 0.61%
iter 1100: loss 2.0305, time 15.94ms, mfu 0.62%
iter 1105: loss 2.0237, time 19.20ms, mfu 0.61%
iter 1110: loss 2.0861, time 25.84ms, mfu 0.59%
iter 1115: loss 2.0127, time 18.11ms, mfu 0.58%
iter 1120: loss 1.9714, time 15.76ms, mfu 0.59%
iter 1125: loss 2.1316, time 16.93ms, mfu 0.59%
iter 1130: loss 2.0469, time 16.42ms, mfu 0.59%
iter 1135: loss 1.9583, time 16.24ms, mfu 0.59%
iter 1140: loss 2.0220, time 18.55ms, mfu 0.59%
iter 1145: loss 1.9850, time 16.28ms, mfu 0.59%
iter 1150: loss 2.0057, time 16.84ms, mfu 0.59%
iter 1155: loss 1.9587, time 16.20ms, mfu 0.60%
iter 1160: loss 1.9813, time 16.54ms, mfu 0.60%
iter 1165: loss 1.9781, time 16.79ms, mfu 0.60%
iter 1170: loss 1.9694, time 16.36ms, mfu 0.60%
iter 1175: loss 1.8901, time 18.21ms, mfu 0.60%
iter 1180: loss 1.8813, time 16.07ms, mfu 0.60%
iter 1185: loss 2.0290, time 15.78ms, mfu 0.60%
iter 1190: loss 1.9529, time 16.39ms, mfu 0.60%
iter 1195: loss 2.0135, time 15.68ms, mfu 0.61%
iter 1200: loss 1.9909, time 16.31ms, mfu 0.61%
iter 1205: loss 1.9538, time 17.22ms, mfu 0.61%
iter 1210: loss 1.9930, time 15.54ms, mfu 0.61%
iter 1215: loss 2.0236, time 17.23ms, mfu 0.61%
iter 1220: loss 1.9229, time 15.99ms, mfu 0.61%
iter 1225: loss 1.9533, time 18.22ms, mfu 0.61%
iter 1230: loss 1.9614, time 15.77ms, mfu 0.61%
iter 1235: loss 2.0121, time 16.04ms, mfu 0.61%
iter 1240: loss 1.9154, time 15.87ms, mfu 0.61%
iter 1245: loss 1.9799, time 16.48ms, mfu 0.61%
step 1250: train loss 1.9606, val loss 2.0071
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
iter 1250: loss 1.9384, time 451.64ms, mfu 0.55%
iter 1255: loss 2.0752, time 16.79ms, mfu 0.56%
iter 1260: loss 1.8871, time 16.17ms, mfu 0.57%
iter 1265: loss 2.0919, time 16.64ms, mfu 0.57%
iter 1270: loss 2.0864, time 35.45ms, mfu 0.54%
iter 1275: loss 1.8923, time 20.78ms, mfu 0.54%
iter 1280: loss 1.9441, time 22.73ms, mfu 0.53%
iter 1285: loss 2.0370, time 21.74ms, mfu 0.52%
iter 1290: loss 2.0244, time 32.35ms, mfu 0.50%
iter 1295: loss 1.9573, time 21.38ms, mfu 0.50%
iter 1300: loss 1.9261, time 22.39ms, mfu 0.49%
iter 1305: loss 2.0245, time 22.16ms, mfu 0.49%
iter 1310: loss 1.9366, time 19.98ms, mfu 0.49%
iter 1315: loss 2.0059, time 19.96ms, mfu 0.49%
iter 1320: loss 1.8893, time 22.73ms, mfu 0.49%
iter 1325: loss 1.9655, time 20.40ms, mfu 0.49%
iter 1330: loss 1.9961, time 23.77ms, mfu 0.48%
iter 1335: loss 2.0459, time 22.05ms, mfu 0.48%
iter 1340: loss 2.0512, time 20.09ms, mfu 0.48%
iter 1345: loss 2.0249, time 22.89ms, mfu 0.48%
iter 1350: loss 1.9997, time 20.30ms, mfu 0.48%
iter 1355: loss 2.0519, time 24.33ms, mfu 0.47%
iter 1360: loss 1.9734, time 22.92ms, mfu 0.47%
iter 1365: loss 1.9482, time 27.01ms, mfu 0.46%
iter 1370: loss 1.8866, time 24.99ms, mfu 0.45%
iter 1375: loss 2.0071, time 32.79ms, mfu 0.44%
iter 1380: loss 2.0847, time 26.59ms, mfu 0.43%
iter 1385: loss 1.9709, time 27.02ms, mfu 0.43%
iter 1390: loss 1.9120, time 24.47ms, mfu 0.43%
iter 1395: loss 1.9834, time 28.75ms, mfu 0.42%
iter 1400: loss 2.0529, time 16.52ms, mfu 0.44%
iter 1405: loss 1.9569, time 15.59ms, mfu 0.46%
iter 1410: loss 1.9613, time 15.78ms, mfu 0.48%
iter 1415: loss 1.9026, time 15.55ms, mfu 0.49%
iter 1420: loss 1.9714, time 15.64ms, mfu 0.51%
iter 1425: loss 1.9556, time 16.94ms, mfu 0.52%
iter 1430: loss 1.9328, time 15.55ms, mfu 0.53%
iter 1435: loss 1.9019, time 15.25ms, mfu 0.54%
iter 1440: loss 2.0054, time 15.73ms, mfu 0.55%
iter 1445: loss 1.9606, time 15.64ms, mfu 0.56%
iter 1450: loss 1.9253, time 16.70ms, mfu 0.57%
iter 1455: loss 1.9318, time 18.69ms, mfu 0.57%
iter 1460: loss 2.0104, time 15.97ms, mfu 0.57%
iter 1465: loss 1.9282, time 15.17ms, mfu 0.58%
iter 1470: loss 1.8984, time 17.44ms, mfu 0.58%
iter 1475: loss 1.8344, time 17.12ms, mfu 0.58%
iter 1480: loss 1.8791, time 15.82ms, mfu 0.59%
iter 1485: loss 1.9864, time 16.09ms, mfu 0.59%
iter 1490: loss 1.9423, time 15.76ms, mfu 0.60%
iter 1495: loss 1.9139, time 15.96ms, mfu 0.60%
step 1500: train loss 1.8879, val loss 1.9906
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
iter 1500: loss 1.9600, time 439.76ms, mfu 0.54%
iter 1505: loss 1.9840, time 16.65ms, mfu 0.55%
iter 1510: loss 1.9821, time 16.82ms, mfu 0.55%
iter 1515: loss 1.9366, time 17.64ms, mfu 0.56%
iter 1520: loss 1.9697, time 16.70ms, mfu 0.56%
iter 1525: loss 1.9650, time 16.81ms, mfu 0.57%
iter 1530: loss 1.8876, time 16.41ms, mfu 0.57%
iter 1535: loss 1.8282, time 17.14ms, mfu 0.57%
iter 1540: loss 1.9444, time 16.52ms, mfu 0.58%
iter 1545: loss 1.9306, time 16.00ms, mfu 0.58%
iter 1550: loss 1.9218, time 17.25ms, mfu 0.58%
iter 1555: loss 1.9025, time 16.41ms, mfu 0.59%
iter 1560: loss 2.0009, time 25.69ms, mfu 0.57%
iter 1565: loss 1.9683, time 16.81ms, mfu 0.57%
iter 1570: loss 1.9778, time 18.35ms, mfu 0.57%
iter 1575: loss 1.8605, time 15.93ms, mfu 0.57%
iter 1580: loss 1.8825, time 17.39ms, mfu 0.58%
iter 1585: loss 1.9020, time 16.18ms, mfu 0.58%
iter 1590: loss 1.9150, time 15.77ms, mfu 0.59%
iter 1595: loss 1.9337, time 20.63ms, mfu 0.58%
iter 1600: loss 1.9500, time 16.15ms, mfu 0.58%
iter 1605: loss 1.9692, time 16.75ms, mfu 0.58%
iter 1610: loss 1.8950, time 19.89ms, mfu 0.58%
iter 1615: loss 2.0197, time 15.74ms, mfu 0.58%
iter 1620: loss 1.9583, time 20.38ms, mfu 0.57%
iter 1625: loss 1.8939, time 16.15ms, mfu 0.58%
iter 1630: loss 1.8463, time 17.29ms, mfu 0.58%
iter 1635: loss 1.9776, time 19.72ms, mfu 0.57%
iter 1640: loss 2.0236, time 16.10ms, mfu 0.58%
iter 1645: loss 1.8550, time 16.03ms, mfu 0.58%
iter 1650: loss 1.9520, time 15.58ms, mfu 0.59%
iter 1655: loss 1.9157, time 15.71ms, mfu 0.60%
iter 1660: loss 1.8573, time 15.84ms, mfu 0.60%
iter 1665: loss 1.8637, time 21.66ms, mfu 0.59%
iter 1670: loss 2.0618, time 15.99ms, mfu 0.59%
iter 1675: loss 1.9753, time 16.08ms, mfu 0.59%
iter 1680: loss 1.8645, time 16.28ms, mfu 0.60%
iter 1685: loss 1.9482, time 16.37ms, mfu 0.60%
iter 1690: loss 2.0330, time 15.98ms, mfu 0.60%
iter 1695: loss 2.1003, time 16.23ms, mfu 0.60%
iter 1700: loss 1.9244, time 16.00ms, mfu 0.61%
iter 1705: loss 1.9463, time 16.31ms, mfu 0.61%
iter 1710: loss 1.8580, time 16.39ms, mfu 0.61%
iter 1715: loss 1.9743, time 16.17ms, mfu 0.61%
iter 1720: loss 1.9372, time 15.92ms, mfu 0.61%
iter 1725: loss 1.8669, time 16.48ms, mfu 0.61%
iter 1730: loss 1.8574, time 16.60ms, mfu 0.61%
iter 1735: loss 1.9068, time 16.61ms, mfu 0.61%
iter 1740: loss 1.8939, time 15.96ms, mfu 0.61%
iter 1745: loss 2.0279, time 17.07ms, mfu 0.61%
step 1750: train loss 1.8609, val loss 1.9528
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
iter 1750: loss 1.8897, time 441.66ms, mfu 0.55%
iter 1755: loss 1.9492, time 16.39ms, mfu 0.56%
iter 1760: loss 1.8286, time 15.96ms, mfu 0.57%
iter 1765: loss 1.9387, time 16.87ms, mfu 0.57%
iter 1770: loss 1.9181, time 16.65ms, mfu 0.57%
iter 1775: loss 1.8116, time 17.82ms, mfu 0.57%
iter 1780: loss 1.9574, time 15.92ms, mfu 0.58%
iter 1785: loss 1.8570, time 16.40ms, mfu 0.58%
iter 1790: loss 1.8750, time 16.41ms, mfu 0.59%
iter 1795: loss 1.8622, time 15.73ms, mfu 0.59%
iter 1800: loss 1.8456, time 23.90ms, mfu 0.58%
iter 1805: loss 1.9404, time 17.03ms, mfu 0.58%
iter 1810: loss 1.9520, time 16.19ms, mfu 0.58%
iter 1815: loss 1.8274, time 17.15ms, mfu 0.58%
iter 1820: loss 1.9095, time 16.34ms, mfu 0.59%
iter 1825: loss 1.9037, time 16.26ms, mfu 0.59%
iter 1830: loss 1.8952, time 16.00ms, mfu 0.59%
iter 1835: loss 1.8883, time 16.16ms, mfu 0.60%
iter 1840: loss 1.9746, time 16.11ms, mfu 0.60%
iter 1845: loss 1.9215, time 15.75ms, mfu 0.60%
iter 1850: loss 1.8968, time 15.71ms, mfu 0.61%
iter 1855: loss 1.8556, time 16.06ms, mfu 0.61%
iter 1860: loss 1.9812, time 29.59ms, mfu 0.58%
iter 1865: loss 1.9333, time 23.60ms, mfu 0.57%
iter 1870: loss 1.8088, time 20.60ms, mfu 0.56%
iter 1875: loss 1.8241, time 20.47ms, mfu 0.55%
iter 1880: loss 1.9903, time 22.25ms, mfu 0.54%
iter 1885: loss 1.8978, time 21.50ms, mfu 0.54%
iter 1890: loss 1.9896, time 21.14ms, mfu 0.53%
iter 1895: loss 1.9999, time 29.94ms, mfu 0.51%
iter 1900: loss 1.8962, time 19.37ms, mfu 0.51%
iter 1905: loss 1.9907, time 29.27ms, mfu 0.50%
iter 1910: loss 2.0130, time 20.61ms, mfu 0.49%
iter 1915: loss 1.7823, time 20.29ms, mfu 0.50%
iter 1920: loss 1.9228, time 23.37ms, mfu 0.49%
iter 1925: loss 1.8731, time 20.80ms, mfu 0.49%
iter 1930: loss 1.8450, time 20.98ms, mfu 0.49%
iter 1935: loss 1.9062, time 20.75ms, mfu 0.49%
iter 1940: loss 1.9563, time 20.62ms, mfu 0.49%
iter 1945: loss 1.9351, time 32.49ms, mfu 0.47%
iter 1950: loss 1.8658, time 24.24ms, mfu 0.47%
iter 1955: loss 1.8775, time 24.33ms, mfu 0.46%
iter 1960: loss 2.0031, time 27.69ms, mfu 0.45%
iter 1965: loss 1.8800, time 30.35ms, mfu 0.44%
iter 1970: loss 1.8408, time 27.94ms, mfu 0.43%
iter 1975: loss 1.8944, time 24.12ms, mfu 0.43%
iter 1980: loss 1.9758, time 29.07ms, mfu 0.42%
iter 1985: loss 1.9348, time 26.73ms, mfu 0.42%
iter 1990: loss 1.9908, time 17.19ms, mfu 0.43%
iter 1995: loss 1.9598, time 16.05ms, mfu 0.45%
step 2000: train loss 1.7951, val loss 1.9352
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1
iter 2000: loss 1.8937, time 430.19ms, mfu 0.41%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.1 at: 
wandb: Find logs at: wandb/run-20251024_005733-ifuarvsk/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 256
Overriding: batch_size = 8
Overriding: max_iters = 2000
Overriding: dropout = 0.2
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
tokens per iteration will be: 1,024
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 4.74M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 4,768,000 parameters
num non-decayed parameter tensors: 13, with 3,328 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: ⣽ setting up run 6hgsrlvx (0.3s)
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_005827-6hgsrlvx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/6hgsrlvx
step 0: train loss 4.2260, val loss 4.2239
iter 0: loss 4.2206, time 904.72ms, mfu -100.00%
iter 5: loss 3.6248, time 16.01ms, mfu 0.63%
iter 10: loss 3.3980, time 18.62ms, mfu 0.62%
iter 15: loss 3.2740, time 18.15ms, mfu 0.62%
iter 20: loss 3.0814, time 16.53ms, mfu 0.62%
iter 25: loss 3.0199, time 19.83ms, mfu 0.60%
iter 30: loss 2.8295, time 15.77ms, mfu 0.61%
iter 35: loss 2.8136, time 18.74ms, mfu 0.60%
iter 40: loss 2.7280, time 16.23ms, mfu 0.60%
iter 45: loss 2.7386, time 15.48ms, mfu 0.61%
iter 50: loss 2.6574, time 19.17ms, mfu 0.60%
iter 55: loss 2.7598, time 27.04ms, mfu 0.58%
iter 60: loss 2.6444, time 21.15ms, mfu 0.57%
iter 65: loss 2.6710, time 23.84ms, mfu 0.55%
iter 70: loss 2.6557, time 21.02ms, mfu 0.55%
iter 75: loss 2.6423, time 23.01ms, mfu 0.54%
iter 80: loss 2.6194, time 22.14ms, mfu 0.53%
iter 85: loss 2.6271, time 22.10ms, mfu 0.52%
iter 90: loss 2.6853, time 24.09ms, mfu 0.51%
iter 95: loss 2.6171, time 19.59ms, mfu 0.51%
iter 100: loss 2.6225, time 21.18ms, mfu 0.51%
iter 105: loss 2.5646, time 21.60ms, mfu 0.50%
iter 110: loss 2.6051, time 20.44ms, mfu 0.50%
iter 115: loss 2.5892, time 30.01ms, mfu 0.49%
iter 120: loss 2.5140, time 23.37ms, mfu 0.48%
iter 125: loss 2.6108, time 24.51ms, mfu 0.47%
iter 130: loss 2.6069, time 19.71ms, mfu 0.48%
iter 135: loss 2.5838, time 21.93ms, mfu 0.48%
iter 140: loss 2.5775, time 22.64ms, mfu 0.47%
iter 145: loss 2.5184, time 23.33ms, mfu 0.47%
iter 150: loss 2.5365, time 21.48ms, mfu 0.47%
iter 155: loss 2.5612, time 27.01ms, mfu 0.46%
iter 160: loss 2.5057, time 28.10ms, mfu 0.45%
iter 165: loss 2.5480, time 33.09ms, mfu 0.44%
iter 170: loss 2.5239, time 24.80ms, mfu 0.43%
iter 175: loss 2.5223, time 40.54ms, mfu 0.41%
iter 180: loss 2.5381, time 17.20ms, mfu 0.43%
iter 185: loss 2.4912, time 16.18ms, mfu 0.45%
iter 190: loss 2.5049, time 16.62ms, mfu 0.47%
iter 195: loss 2.4993, time 16.12ms, mfu 0.48%
iter 200: loss 2.4878, time 15.75ms, mfu 0.50%
iter 205: loss 2.5322, time 16.43ms, mfu 0.51%
iter 210: loss 2.5194, time 15.96ms, mfu 0.52%
iter 215: loss 2.5430, time 15.89ms, mfu 0.53%
iter 220: loss 2.5338, time 16.24ms, mfu 0.54%
iter 225: loss 2.4204, time 15.88ms, mfu 0.55%
iter 230: loss 2.5074, time 17.51ms, mfu 0.55%
iter 235: loss 2.5023, time 15.87ms, mfu 0.56%
iter 240: loss 2.4472, time 15.45ms, mfu 0.57%
iter 245: loss 2.6178, time 16.35ms, mfu 0.58%
step 250: train loss 2.4884, val loss 2.4699
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
iter 250: loss 2.5091, time 403.84ms, mfu 0.52%
iter 255: loss 2.4733, time 16.77ms, mfu 0.53%
iter 260: loss 2.5337, time 15.64ms, mfu 0.54%
iter 265: loss 2.5094, time 16.20ms, mfu 0.55%
iter 270: loss 2.4713, time 15.87ms, mfu 0.56%
iter 275: loss 2.4967, time 16.67ms, mfu 0.56%
iter 280: loss 2.4915, time 17.38ms, mfu 0.56%
iter 285: loss 2.5052, time 16.22ms, mfu 0.57%
iter 290: loss 2.5242, time 15.87ms, mfu 0.58%
iter 295: loss 2.5110, time 16.27ms, mfu 0.58%
iter 300: loss 2.4357, time 15.96ms, mfu 0.59%
iter 305: loss 2.4457, time 17.68ms, mfu 0.59%
iter 310: loss 2.5802, time 16.97ms, mfu 0.59%
iter 315: loss 2.4267, time 16.73ms, mfu 0.59%
iter 320: loss 2.4735, time 18.43ms, mfu 0.58%
iter 325: loss 2.4501, time 16.58ms, mfu 0.59%
iter 330: loss 2.4131, time 16.13ms, mfu 0.59%
iter 335: loss 2.4550, time 19.34ms, mfu 0.58%
iter 340: loss 2.5034, time 16.56ms, mfu 0.59%
iter 345: loss 2.3367, time 24.79ms, mfu 0.57%
iter 350: loss 2.4348, time 16.51ms, mfu 0.57%
iter 355: loss 2.4164, time 16.52ms, mfu 0.58%
iter 360: loss 2.4414, time 16.65ms, mfu 0.58%
iter 365: loss 2.4602, time 16.20ms, mfu 0.58%
iter 370: loss 2.4854, time 19.72ms, mfu 0.58%
iter 375: loss 2.4619, time 16.54ms, mfu 0.58%
iter 380: loss 2.4754, time 16.08ms, mfu 0.59%
iter 385: loss 2.4456, time 16.12ms, mfu 0.59%
iter 390: loss 2.4539, time 18.39ms, mfu 0.59%
iter 395: loss 2.4847, time 15.97ms, mfu 0.59%
iter 400: loss 2.4794, time 16.74ms, mfu 0.59%
iter 405: loss 2.4546, time 16.34ms, mfu 0.59%
iter 410: loss 2.4253, time 16.21ms, mfu 0.60%
iter 415: loss 2.3866, time 17.44ms, mfu 0.60%
iter 420: loss 2.3742, time 19.67ms, mfu 0.59%
iter 425: loss 2.4988, time 15.50ms, mfu 0.59%
iter 430: loss 2.4935, time 16.23ms, mfu 0.60%
iter 435: loss 2.4116, time 15.95ms, mfu 0.60%
iter 440: loss 2.4130, time 18.10ms, mfu 0.60%
iter 445: loss 2.4622, time 16.37ms, mfu 0.60%
iter 450: loss 2.4464, time 22.37ms, mfu 0.58%
iter 455: loss 2.4234, time 16.95ms, mfu 0.58%
iter 460: loss 2.4316, time 16.97ms, mfu 0.59%
iter 465: loss 2.4844, time 16.63ms, mfu 0.59%
iter 470: loss 2.4187, time 19.21ms, mfu 0.58%
iter 475: loss 2.4224, time 16.47ms, mfu 0.59%
iter 480: loss 2.4173, time 16.66ms, mfu 0.59%
iter 485: loss 2.4561, time 15.79ms, mfu 0.59%
iter 490: loss 2.4406, time 15.78ms, mfu 0.60%
iter 495: loss 2.4018, time 16.97ms, mfu 0.60%
step 500: train loss 2.3910, val loss 2.3945
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
iter 500: loss 2.4135, time 438.65ms, mfu 0.54%
iter 505: loss 2.4221, time 16.97ms, mfu 0.55%
iter 510: loss 2.3911, time 15.85ms, mfu 0.55%
iter 515: loss 2.4374, time 15.83ms, mfu 0.56%
iter 520: loss 2.4152, time 16.12ms, mfu 0.57%
iter 525: loss 2.4043, time 17.53ms, mfu 0.57%
iter 530: loss 2.3800, time 16.68ms, mfu 0.57%
iter 535: loss 2.3958, time 28.74ms, mfu 0.55%
iter 540: loss 2.4270, time 16.04ms, mfu 0.56%
iter 545: loss 2.3842, time 20.26ms, mfu 0.55%
iter 550: loss 2.3170, time 16.43ms, mfu 0.56%
iter 555: loss 2.3953, time 17.49ms, mfu 0.56%
iter 560: loss 2.3917, time 17.28ms, mfu 0.56%
iter 565: loss 2.3890, time 16.39ms, mfu 0.57%
iter 570: loss 2.3424, time 19.05ms, mfu 0.57%
iter 575: loss 2.4128, time 15.81ms, mfu 0.57%
iter 580: loss 2.3581, time 16.26ms, mfu 0.58%
iter 585: loss 2.3466, time 16.57ms, mfu 0.58%
iter 590: loss 2.3747, time 16.44ms, mfu 0.58%
iter 595: loss 2.4102, time 20.40ms, mfu 0.58%
iter 600: loss 2.3029, time 16.76ms, mfu 0.58%
iter 605: loss 2.3565, time 16.09ms, mfu 0.58%
iter 610: loss 2.3200, time 15.95ms, mfu 0.59%
iter 615: loss 2.2901, time 15.80ms, mfu 0.59%
iter 620: loss 2.3814, time 16.07ms, mfu 0.60%
iter 625: loss 2.2483, time 15.37ms, mfu 0.60%
iter 630: loss 2.3907, time 16.06ms, mfu 0.61%
iter 635: loss 2.3363, time 15.89ms, mfu 0.61%
iter 640: loss 2.2981, time 24.45ms, mfu 0.59%
iter 645: loss 2.2614, time 25.29ms, mfu 0.57%
iter 650: loss 2.2951, time 25.18ms, mfu 0.55%
iter 655: loss 2.2763, time 21.20ms, mfu 0.55%
iter 660: loss 2.3547, time 20.81ms, mfu 0.54%
iter 665: loss 2.2934, time 21.26ms, mfu 0.53%
iter 670: loss 2.3754, time 21.09ms, mfu 0.53%
iter 675: loss 2.4254, time 27.05ms, mfu 0.51%
iter 680: loss 2.3046, time 23.12ms, mfu 0.50%
iter 685: loss 2.3033, time 23.84ms, mfu 0.50%
iter 690: loss 2.2803, time 19.90ms, mfu 0.50%
iter 695: loss 2.3055, time 20.50ms, mfu 0.50%
iter 700: loss 2.2732, time 21.39ms, mfu 0.49%
iter 705: loss 2.3496, time 26.95ms, mfu 0.48%
iter 710: loss 2.2933, time 21.49ms, mfu 0.48%
iter 715: loss 2.3160, time 25.21ms, mfu 0.47%
iter 720: loss 2.2833, time 19.66ms, mfu 0.48%
iter 725: loss 2.2586, time 23.84ms, mfu 0.47%
iter 730: loss 2.2721, time 21.13ms, mfu 0.47%
iter 735: loss 2.3083, time 24.64ms, mfu 0.47%
iter 740: loss 2.3031, time 23.69ms, mfu 0.46%
iter 745: loss 2.2577, time 28.93ms, mfu 0.45%
step 750: train loss 2.1857, val loss 2.2258
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
iter 750: loss 2.2794, time 563.63ms, mfu 0.41%
iter 755: loss 2.3004, time 19.92ms, mfu 0.42%
iter 760: loss 2.3099, time 16.18ms, mfu 0.44%
iter 765: loss 2.2601, time 16.04ms, mfu 0.46%
iter 770: loss 2.3041, time 16.04ms, mfu 0.47%
iter 775: loss 2.2388, time 15.89ms, mfu 0.49%
iter 780: loss 2.2852, time 16.26ms, mfu 0.50%
iter 785: loss 2.2854, time 15.85ms, mfu 0.52%
iter 790: loss 2.2511, time 16.41ms, mfu 0.53%
iter 795: loss 2.2416, time 16.08ms, mfu 0.54%
iter 800: loss 2.2714, time 16.57ms, mfu 0.54%
iter 805: loss 2.2799, time 18.98ms, mfu 0.54%
iter 810: loss 2.2103, time 16.04ms, mfu 0.55%
iter 815: loss 2.2428, time 16.39ms, mfu 0.56%
iter 820: loss 2.2598, time 16.54ms, mfu 0.56%
iter 825: loss 2.2810, time 15.76ms, mfu 0.57%
iter 830: loss 2.2051, time 16.00ms, mfu 0.58%
iter 835: loss 2.2476, time 16.60ms, mfu 0.58%
iter 840: loss 2.2687, time 16.44ms, mfu 0.58%
iter 845: loss 2.2465, time 16.11ms, mfu 0.59%
iter 850: loss 2.1966, time 15.98ms, mfu 0.59%
iter 855: loss 2.2804, time 16.16ms, mfu 0.60%
iter 860: loss 2.3359, time 16.24ms, mfu 0.60%
iter 865: loss 2.2066, time 16.26ms, mfu 0.60%
iter 870: loss 2.2157, time 16.09ms, mfu 0.60%
iter 875: loss 2.2207, time 15.91ms, mfu 0.61%
iter 880: loss 2.3268, time 16.10ms, mfu 0.61%
iter 885: loss 2.3053, time 15.85ms, mfu 0.61%
iter 890: loss 2.2130, time 16.86ms, mfu 0.61%
iter 895: loss 2.3222, time 15.84ms, mfu 0.61%
iter 900: loss 2.2547, time 16.08ms, mfu 0.61%
iter 905: loss 2.2064, time 16.35ms, mfu 0.62%
iter 910: loss 2.1071, time 16.30ms, mfu 0.62%
iter 915: loss 2.2478, time 16.05ms, mfu 0.62%
iter 920: loss 2.2317, time 16.17ms, mfu 0.62%
iter 925: loss 2.2431, time 16.26ms, mfu 0.62%
iter 930: loss 2.1480, time 16.27ms, mfu 0.62%
iter 935: loss 2.1539, time 23.62ms, mfu 0.60%
iter 940: loss 2.1474, time 15.88ms, mfu 0.60%
iter 945: loss 2.1593, time 16.66ms, mfu 0.60%
iter 950: loss 2.1754, time 16.82ms, mfu 0.60%
iter 955: loss 2.2360, time 18.18ms, mfu 0.60%
iter 960: loss 2.2858, time 16.41ms, mfu 0.60%
iter 965: loss 2.2056, time 16.10ms, mfu 0.60%
iter 970: loss 2.1392, time 16.35ms, mfu 0.60%
iter 975: loss 2.0624, time 17.70ms, mfu 0.60%
iter 980: loss 2.2080, time 15.90ms, mfu 0.60%
iter 985: loss 2.1457, time 16.48ms, mfu 0.61%
iter 990: loss 2.3189, time 18.08ms, mfu 0.60%
iter 995: loss 2.2440, time 16.38ms, mfu 0.60%
step 1000: train loss 2.1441, val loss 2.1557
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
iter 1000: loss 2.2231, time 456.70ms, mfu 0.54%
iter 1005: loss 2.1579, time 15.91ms, mfu 0.55%
iter 1010: loss 2.1573, time 16.19ms, mfu 0.56%
iter 1015: loss 2.1391, time 16.30ms, mfu 0.57%
iter 1020: loss 2.2492, time 16.07ms, mfu 0.57%
iter 1025: loss 2.1580, time 15.77ms, mfu 0.58%
iter 1030: loss 2.2420, time 25.85ms, mfu 0.56%
iter 1035: loss 2.1382, time 22.61ms, mfu 0.55%
iter 1040: loss 2.2258, time 26.01ms, mfu 0.53%
iter 1045: loss 2.1868, time 16.13ms, mfu 0.54%
iter 1050: loss 2.2098, time 16.26ms, mfu 0.55%
iter 1055: loss 2.2109, time 15.96ms, mfu 0.56%
iter 1060: loss 2.2056, time 16.35ms, mfu 0.56%
iter 1065: loss 2.2770, time 15.92ms, mfu 0.57%
iter 1070: loss 2.2013, time 16.57ms, mfu 0.58%
iter 1075: loss 2.2167, time 16.32ms, mfu 0.58%
iter 1080: loss 2.2873, time 18.30ms, mfu 0.58%
iter 1085: loss 2.1796, time 15.84ms, mfu 0.58%
iter 1090: loss 2.1929, time 27.18ms, mfu 0.56%
iter 1095: loss 2.2169, time 15.72ms, mfu 0.57%
iter 1100: loss 2.2349, time 15.72ms, mfu 0.58%
iter 1105: loss 2.1704, time 15.57ms, mfu 0.58%
iter 1110: loss 2.2927, time 16.41ms, mfu 0.59%
iter 1115: loss 2.1563, time 15.91ms, mfu 0.59%
iter 1120: loss 2.1616, time 16.19ms, mfu 0.60%
iter 1125: loss 2.3133, time 16.05ms, mfu 0.60%
iter 1130: loss 2.2031, time 19.03ms, mfu 0.59%
iter 1135: loss 2.1204, time 15.71ms, mfu 0.60%
iter 1140: loss 2.1605, time 16.06ms, mfu 0.60%
iter 1145: loss 2.1614, time 15.39ms, mfu 0.61%
iter 1150: loss 2.1896, time 18.47ms, mfu 0.60%
iter 1155: loss 2.1703, time 15.95ms, mfu 0.60%
iter 1160: loss 2.1442, time 15.60ms, mfu 0.61%
iter 1165: loss 2.1703, time 15.26ms, mfu 0.61%
iter 1170: loss 2.1577, time 15.70ms, mfu 0.62%
iter 1175: loss 2.1200, time 15.91ms, mfu 0.62%
iter 1180: loss 2.0961, time 15.67ms, mfu 0.62%
iter 1185: loss 2.2001, time 15.84ms, mfu 0.62%
iter 1190: loss 2.1416, time 16.21ms, mfu 0.62%
iter 1195: loss 2.1754, time 15.91ms, mfu 0.62%
iter 1200: loss 2.1793, time 15.28ms, mfu 0.63%
iter 1205: loss 2.1606, time 17.80ms, mfu 0.62%
iter 1210: loss 2.1556, time 16.48ms, mfu 0.62%
iter 1215: loss 2.1591, time 15.93ms, mfu 0.62%
iter 1220: loss 2.1311, time 16.96ms, mfu 0.62%
iter 1225: loss 2.1753, time 28.32ms, mfu 0.59%
iter 1230: loss 2.1456, time 25.41ms, mfu 0.57%
iter 1235: loss 2.1986, time 19.65ms, mfu 0.57%
iter 1240: loss 2.0668, time 29.93ms, mfu 0.54%
iter 1245: loss 2.1921, time 22.07ms, mfu 0.54%
step 1250: train loss 2.1186, val loss 2.1397
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
iter 1250: loss 2.1578, time 498.91ms, mfu 0.48%
iter 1255: loss 2.2514, time 20.11ms, mfu 0.49%
iter 1260: loss 2.0883, time 21.26ms, mfu 0.49%
iter 1265: loss 2.2817, time 25.72ms, mfu 0.48%
iter 1270: loss 2.2813, time 21.17ms, mfu 0.48%
iter 1275: loss 2.0787, time 20.92ms, mfu 0.48%
iter 1280: loss 2.1391, time 24.83ms, mfu 0.47%
iter 1285: loss 2.1932, time 23.34ms, mfu 0.47%
iter 1290: loss 2.1635, time 25.62ms, mfu 0.46%
iter 1295: loss 2.1625, time 23.34ms, mfu 0.46%
iter 1300: loss 2.1471, time 23.66ms, mfu 0.45%
iter 1305: loss 2.2026, time 26.11ms, mfu 0.45%
iter 1310: loss 2.1206, time 24.98ms, mfu 0.44%
iter 1315: loss 2.1842, time 16.20ms, mfu 0.46%
iter 1320: loss 2.0815, time 15.51ms, mfu 0.48%
iter 1325: loss 2.1270, time 16.02ms, mfu 0.49%
iter 1330: loss 2.1575, time 17.02ms, mfu 0.50%
iter 1335: loss 2.2253, time 17.41ms, mfu 0.51%
iter 1340: loss 2.2269, time 16.72ms, mfu 0.52%
iter 1345: loss 2.2015, time 16.27ms, mfu 0.53%
iter 1350: loss 2.1875, time 16.26ms, mfu 0.54%
iter 1355: loss 2.2651, time 15.74ms, mfu 0.55%
iter 1360: loss 2.1787, time 16.45ms, mfu 0.56%
iter 1365: loss 2.1285, time 16.39ms, mfu 0.56%
iter 1370: loss 2.0836, time 16.06ms, mfu 0.57%
iter 1375: loss 2.2008, time 16.22ms, mfu 0.57%
iter 1380: loss 2.2714, time 16.76ms, mfu 0.58%
iter 1385: loss 2.1373, time 17.31ms, mfu 0.58%
iter 1390: loss 2.1138, time 17.44ms, mfu 0.58%
iter 1395: loss 2.1840, time 16.15ms, mfu 0.58%
iter 1400: loss 2.2215, time 15.94ms, mfu 0.59%
iter 1405: loss 2.1405, time 15.91ms, mfu 0.59%
iter 1410: loss 2.0994, time 26.01ms, mfu 0.57%
iter 1415: loss 2.1155, time 16.98ms, mfu 0.57%
iter 1420: loss 2.1447, time 16.42ms, mfu 0.58%
iter 1425: loss 2.0941, time 15.96ms, mfu 0.58%
iter 1430: loss 2.1234, time 16.91ms, mfu 0.59%
iter 1435: loss 2.1013, time 17.17ms, mfu 0.59%
iter 1440: loss 2.1793, time 16.11ms, mfu 0.59%
iter 1445: loss 2.1534, time 18.21ms, mfu 0.59%
iter 1450: loss 2.1258, time 16.43ms, mfu 0.59%
iter 1455: loss 2.1237, time 15.74ms, mfu 0.59%
iter 1460: loss 2.1797, time 15.67ms, mfu 0.60%
iter 1465: loss 2.1415, time 15.91ms, mfu 0.60%
iter 1470: loss 2.0452, time 16.12ms, mfu 0.61%
iter 1475: loss 2.0489, time 16.24ms, mfu 0.61%
iter 1480: loss 2.0740, time 15.96ms, mfu 0.61%
iter 1485: loss 2.1782, time 16.49ms, mfu 0.61%
iter 1490: loss 2.1502, time 15.83ms, mfu 0.61%
iter 1495: loss 2.1185, time 16.19ms, mfu 0.61%
step 1500: train loss 2.0585, val loss 2.1245
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
iter 1500: loss 2.0984, time 439.66ms, mfu 0.55%
iter 1505: loss 2.1522, time 16.28ms, mfu 0.56%
iter 1510: loss 2.1787, time 16.12ms, mfu 0.57%
iter 1515: loss 2.1154, time 16.34ms, mfu 0.57%
iter 1520: loss 2.1783, time 15.85ms, mfu 0.58%
iter 1525: loss 2.1557, time 15.72ms, mfu 0.59%
iter 1530: loss 2.0763, time 15.90ms, mfu 0.59%
iter 1535: loss 2.0299, time 16.09ms, mfu 0.59%
iter 1540: loss 2.1214, time 15.84ms, mfu 0.60%
iter 1545: loss 2.1132, time 24.10ms, mfu 0.58%
iter 1550: loss 2.1494, time 16.32ms, mfu 0.58%
iter 1555: loss 2.0986, time 16.54ms, mfu 0.59%
iter 1560: loss 2.1843, time 15.99ms, mfu 0.59%
iter 1565: loss 2.1438, time 17.20ms, mfu 0.59%
iter 1570: loss 2.1625, time 16.01ms, mfu 0.60%
iter 1575: loss 2.0400, time 17.61ms, mfu 0.59%
iter 1580: loss 2.1029, time 16.18ms, mfu 0.60%
iter 1585: loss 2.0851, time 16.48ms, mfu 0.60%
iter 1590: loss 2.0833, time 16.35ms, mfu 0.60%
iter 1595: loss 2.1331, time 16.12ms, mfu 0.60%
iter 1600: loss 2.1274, time 16.18ms, mfu 0.60%
iter 1605: loss 2.1532, time 15.53ms, mfu 0.61%
iter 1610: loss 2.1158, time 16.40ms, mfu 0.61%
iter 1615: loss 2.2067, time 17.35ms, mfu 0.61%
iter 1620: loss 2.1434, time 17.79ms, mfu 0.60%
iter 1625: loss 2.0930, time 16.21ms, mfu 0.61%
iter 1630: loss 2.0637, time 16.28ms, mfu 0.61%
iter 1635: loss 2.1572, time 15.72ms, mfu 0.61%
iter 1640: loss 2.1669, time 16.41ms, mfu 0.61%
iter 1645: loss 2.0493, time 15.62ms, mfu 0.61%
iter 1650: loss 2.1521, time 16.35ms, mfu 0.62%
iter 1655: loss 2.0925, time 15.93ms, mfu 0.62%
iter 1660: loss 2.0866, time 16.92ms, mfu 0.61%
iter 1665: loss 2.0708, time 16.27ms, mfu 0.62%
iter 1670: loss 2.2353, time 15.55ms, mfu 0.62%
iter 1675: loss 2.1823, time 16.07ms, mfu 0.62%
iter 1680: loss 2.0615, time 16.45ms, mfu 0.62%
iter 1685: loss 2.1157, time 16.01ms, mfu 0.62%
iter 1690: loss 2.1932, time 16.10ms, mfu 0.62%
iter 1695: loss 2.2694, time 15.70ms, mfu 0.62%
iter 1700: loss 2.1006, time 22.80ms, mfu 0.61%
iter 1705: loss 2.1142, time 16.08ms, mfu 0.61%
iter 1710: loss 2.0671, time 16.14ms, mfu 0.61%
iter 1715: loss 2.1637, time 16.48ms, mfu 0.61%
iter 1720: loss 2.1431, time 15.90ms, mfu 0.61%
iter 1725: loss 2.0752, time 17.41ms, mfu 0.61%
iter 1730: loss 2.0446, time 16.33ms, mfu 0.61%
iter 1735: loss 2.1239, time 16.09ms, mfu 0.61%
iter 1740: loss 2.0797, time 16.47ms, mfu 0.61%
iter 1745: loss 2.1764, time 17.14ms, mfu 0.61%
step 1750: train loss 2.0320, val loss 2.0891
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
iter 1750: loss 2.0742, time 441.55ms, mfu 0.55%
iter 1755: loss 2.1362, time 16.09ms, mfu 0.56%
iter 1760: loss 1.9973, time 16.58ms, mfu 0.56%
iter 1765: loss 2.1232, time 16.52ms, mfu 0.57%
iter 1770: loss 2.0942, time 19.66ms, mfu 0.56%
iter 1775: loss 2.0454, time 30.17ms, mfu 0.54%
iter 1780: loss 2.1842, time 21.97ms, mfu 0.53%
iter 1785: loss 2.0795, time 23.85ms, mfu 0.52%
iter 1790: loss 2.0560, time 21.16ms, mfu 0.52%
iter 1795: loss 2.0460, time 28.71ms, mfu 0.50%
iter 1800: loss 2.0683, time 21.02ms, mfu 0.50%
iter 1805: loss 2.1392, time 21.20ms, mfu 0.50%
iter 1810: loss 2.1440, time 21.80ms, mfu 0.49%
iter 1815: loss 2.0294, time 20.93ms, mfu 0.49%
iter 1820: loss 2.0837, time 23.34ms, mfu 0.49%
iter 1825: loss 2.1489, time 21.20ms, mfu 0.49%
iter 1830: loss 2.0708, time 22.97ms, mfu 0.48%
iter 1835: loss 2.1144, time 29.83ms, mfu 0.47%
iter 1840: loss 2.1378, time 22.52ms, mfu 0.46%
iter 1845: loss 2.1251, time 30.04ms, mfu 0.45%
iter 1850: loss 2.0846, time 21.29ms, mfu 0.45%
iter 1855: loss 2.0921, time 21.34ms, mfu 0.46%
iter 1860: loss 2.1696, time 28.73ms, mfu 0.45%
iter 1865: loss 2.1380, time 25.33ms, mfu 0.44%
iter 1870: loss 2.0200, time 27.64ms, mfu 0.43%
iter 1875: loss 2.0288, time 30.52ms, mfu 0.42%
iter 1880: loss 2.1803, time 28.89ms, mfu 0.42%
iter 1885: loss 2.1287, time 24.84ms, mfu 0.42%
iter 1890: loss 2.1448, time 16.39ms, mfu 0.44%
iter 1895: loss 2.2054, time 15.72ms, mfu 0.46%
iter 1900: loss 2.1040, time 17.14ms, mfu 0.47%
iter 1905: loss 2.1762, time 16.44ms, mfu 0.48%
iter 1910: loss 2.1853, time 18.16ms, mfu 0.49%
iter 1915: loss 1.9946, time 17.16ms, mfu 0.50%
iter 1920: loss 2.0935, time 16.45ms, mfu 0.51%
iter 1925: loss 2.0696, time 16.68ms, mfu 0.52%
iter 1930: loss 2.0078, time 16.19ms, mfu 0.53%
iter 1935: loss 2.1184, time 18.85ms, mfu 0.53%
iter 1940: loss 2.1229, time 16.67ms, mfu 0.54%
iter 1945: loss 2.1225, time 17.00ms, mfu 0.55%
iter 1950: loss 2.0355, time 22.40ms, mfu 0.54%
iter 1955: loss 2.0389, time 16.01ms, mfu 0.55%
iter 1960: loss 2.1954, time 16.26ms, mfu 0.55%
iter 1965: loss 2.0921, time 21.69ms, mfu 0.54%
iter 1970: loss 2.0572, time 17.06ms, mfu 0.55%
iter 1975: loss 2.1053, time 21.33ms, mfu 0.54%
iter 1980: loss 2.1595, time 16.29ms, mfu 0.55%
iter 1985: loss 2.1136, time 16.21ms, mfu 0.56%
iter 1990: loss 2.1664, time 15.80ms, mfu 0.57%
iter 1995: loss 2.1562, time 15.76ms, mfu 0.57%
step 2000: train loss 1.9710, val loss 2.0652
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2
iter 2000: loss 2.0614, time 429.76ms, mfu 0.52%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd256_batch_size8_max_iters2000_dropout0.2 at: 
wandb: Find logs at: wandb/run-20251024_005827-6hgsrlvx/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 256
Overriding: batch_size = 16
Overriding: max_iters = 1000
Overriding: dropout = 0.1
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.1
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.1
tokens per iteration will be: 2,048
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 4.74M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 4,768,000 parameters
num non-decayed parameter tensors: 13, with 3,328 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_005921-dw4o2w74
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.1
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/dw4o2w74
step 0: train loss 4.2246, val loss 4.2254
iter 0: loss 4.2315, time 1358.86ms, mfu -100.00%
iter 5: loss 3.5973, time 22.91ms, mfu 0.88%
iter 10: loss 3.3283, time 23.34ms, mfu 0.88%
iter 15: loss 3.1139, time 21.83ms, mfu 0.89%
iter 20: loss 3.0041, time 23.94ms, mfu 0.88%
iter 25: loss 2.8830, time 27.13ms, mfu 0.87%
iter 30: loss 2.8201, time 20.73ms, mfu 0.88%
iter 35: loss 2.7500, time 22.91ms, mfu 0.88%
iter 40: loss 2.6598, time 25.40ms, mfu 0.87%
iter 45: loss 2.6422, time 23.24ms, mfu 0.87%
iter 50: loss 2.5944, time 36.89ms, mfu 0.84%
iter 55: loss 2.5907, time 36.31ms, mfu 0.81%
iter 60: loss 2.5587, time 34.52ms, mfu 0.79%
iter 65: loss 2.5654, time 36.63ms, mfu 0.76%
iter 70: loss 2.6072, time 36.11ms, mfu 0.74%
iter 75: loss 2.5114, time 34.55ms, mfu 0.73%
iter 80: loss 2.5261, time 35.76ms, mfu 0.71%
iter 85: loss 2.5458, time 36.72ms, mfu 0.70%
iter 90: loss 2.5526, time 35.89ms, mfu 0.68%
iter 95: loss 2.5121, time 37.29ms, mfu 0.67%
iter 100: loss 2.4782, time 35.57ms, mfu 0.66%
iter 105: loss 2.5381, time 37.32ms, mfu 0.65%
iter 110: loss 2.5334, time 36.06ms, mfu 0.64%
iter 115: loss 2.5684, time 36.15ms, mfu 0.63%
iter 120: loss 2.5496, time 35.46ms, mfu 0.62%
iter 125: loss 2.4874, time 27.84ms, mfu 0.63%
iter 130: loss 2.5131, time 36.41ms, mfu 0.63%
iter 135: loss 2.5026, time 36.07ms, mfu 0.62%
iter 140: loss 2.4527, time 36.33ms, mfu 0.61%
iter 145: loss 2.5124, time 35.62ms, mfu 0.61%
iter 150: loss 2.4608, time 36.37ms, mfu 0.60%
iter 155: loss 2.5471, time 36.53ms, mfu 0.60%
iter 160: loss 2.4973, time 36.00ms, mfu 0.59%
iter 165: loss 2.4475, time 36.33ms, mfu 0.59%
iter 170: loss 2.4880, time 33.77ms, mfu 0.59%
iter 175: loss 2.4228, time 36.41ms, mfu 0.59%
iter 180: loss 2.4474, time 38.21ms, mfu 0.58%
iter 185: loss 2.4517, time 35.68ms, mfu 0.58%
iter 190: loss 2.4329, time 29.81ms, mfu 0.59%
iter 195: loss 2.4431, time 36.23ms, mfu 0.59%
iter 200: loss 2.4456, time 36.50ms, mfu 0.58%
iter 205: loss 2.4291, time 36.32ms, mfu 0.58%
iter 210: loss 2.4224, time 22.36ms, mfu 0.61%
iter 215: loss 2.4850, time 35.54ms, mfu 0.61%
iter 220: loss 2.4123, time 36.35ms, mfu 0.60%
iter 225: loss 2.4339, time 36.55ms, mfu 0.60%
iter 230: loss 2.3672, time 36.44ms, mfu 0.59%
iter 235: loss 2.4091, time 36.16ms, mfu 0.59%
iter 240: loss 2.4056, time 32.43ms, mfu 0.59%
iter 245: loss 2.4094, time 36.35ms, mfu 0.59%
step 250: train loss 2.3928, val loss 2.4023
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.1
iter 250: loss 2.4251, time 702.88ms, mfu 0.53%
iter 255: loss 2.3962, time 35.88ms, mfu 0.54%
iter 260: loss 2.4158, time 35.57ms, mfu 0.54%
iter 265: loss 2.3885, time 36.91ms, mfu 0.54%
iter 270: loss 2.4002, time 36.34ms, mfu 0.54%
iter 275: loss 2.3789, time 37.89ms, mfu 0.54%
iter 280: loss 2.3727, time 30.43ms, mfu 0.55%
iter 285: loss 2.3273, time 36.27ms, mfu 0.55%
iter 290: loss 2.3836, time 36.34ms, mfu 0.55%
iter 295: loss 2.3405, time 38.10ms, mfu 0.55%
iter 300: loss 2.2992, time 36.69ms, mfu 0.55%
iter 305: loss 2.3431, time 33.01ms, mfu 0.56%
iter 310: loss 2.3534, time 24.36ms, mfu 0.59%
iter 315: loss 2.3749, time 22.24ms, mfu 0.62%
iter 320: loss 2.3170, time 25.46ms, mfu 0.64%
iter 325: loss 2.3086, time 31.64ms, mfu 0.64%
iter 330: loss 2.2919, time 29.17ms, mfu 0.64%
iter 335: loss 2.3028, time 25.06ms, mfu 0.66%
iter 340: loss 2.2677, time 26.34ms, mfu 0.67%
iter 345: loss 2.2532, time 25.82ms, mfu 0.68%
iter 350: loss 2.3383, time 26.84ms, mfu 0.69%
iter 355: loss 2.2541, time 29.66ms, mfu 0.69%
iter 360: loss 2.2464, time 21.71ms, mfu 0.71%
iter 365: loss 2.2853, time 28.10ms, mfu 0.71%
iter 370: loss 2.2780, time 27.31ms, mfu 0.71%
iter 375: loss 2.1516, time 23.68ms, mfu 0.73%
iter 380: loss 2.2687, time 36.90ms, mfu 0.71%
iter 385: loss 2.2870, time 36.93ms, mfu 0.69%
iter 390: loss 2.2405, time 37.20ms, mfu 0.68%
iter 395: loss 2.2159, time 32.47ms, mfu 0.67%
iter 400: loss 2.2457, time 35.86ms, mfu 0.66%
iter 405: loss 2.1183, time 36.52ms, mfu 0.65%
iter 410: loss 2.1705, time 36.34ms, mfu 0.64%
iter 415: loss 2.1756, time 36.70ms, mfu 0.63%
iter 420: loss 2.1507, time 35.02ms, mfu 0.63%
iter 425: loss 2.1443, time 33.60ms, mfu 0.62%
iter 430: loss 2.1726, time 35.89ms, mfu 0.62%
iter 435: loss 2.1476, time 36.92ms, mfu 0.61%
iter 440: loss 2.2326, time 36.67ms, mfu 0.61%
iter 445: loss 2.1855, time 37.74ms, mfu 0.60%
iter 450: loss 2.1571, time 31.05ms, mfu 0.60%
iter 455: loss 2.0960, time 36.97ms, mfu 0.60%
iter 460: loss 2.1161, time 36.95ms, mfu 0.59%
iter 465: loss 2.0835, time 33.86ms, mfu 0.59%
iter 470: loss 2.0717, time 35.98ms, mfu 0.59%
iter 475: loss 2.0524, time 38.18ms, mfu 0.58%
iter 480: loss 2.1750, time 37.49ms, mfu 0.58%
iter 485: loss 2.0890, time 37.01ms, mfu 0.58%
iter 490: loss 2.0751, time 37.34ms, mfu 0.57%
iter 495: loss 2.1398, time 35.38ms, mfu 0.57%
step 500: train loss 2.0508, val loss 2.1188
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.1
iter 500: loss 2.1123, time 735.96ms, mfu 0.52%
iter 505: loss 2.0638, time 36.83ms, mfu 0.52%
iter 510: loss 2.1201, time 36.43ms, mfu 0.52%
iter 515: loss 2.0824, time 39.39ms, mfu 0.52%
iter 520: loss 2.1267, time 36.24ms, mfu 0.53%
iter 525: loss 2.0835, time 40.17ms, mfu 0.52%
iter 530: loss 2.0614, time 36.81ms, mfu 0.53%
iter 535: loss 2.1088, time 36.85ms, mfu 0.53%
iter 540: loss 2.0486, time 36.58ms, mfu 0.53%
iter 545: loss 2.0488, time 29.54ms, mfu 0.55%
iter 550: loss 2.0151, time 37.03ms, mfu 0.55%
iter 555: loss 2.0063, time 37.53ms, mfu 0.55%
iter 560: loss 1.9869, time 36.21ms, mfu 0.55%
iter 565: loss 2.0293, time 37.62ms, mfu 0.55%
iter 570: loss 1.9910, time 37.06ms, mfu 0.55%
iter 575: loss 1.9797, time 33.43ms, mfu 0.55%
iter 580: loss 2.0337, time 37.71ms, mfu 0.55%
iter 585: loss 2.0203, time 36.89ms, mfu 0.55%
iter 590: loss 1.9982, time 36.33ms, mfu 0.55%
iter 595: loss 2.0072, time 36.80ms, mfu 0.55%
iter 600: loss 1.9666, time 28.36ms, mfu 0.57%
iter 605: loss 1.9507, time 36.33ms, mfu 0.57%
iter 610: loss 1.9861, time 35.76ms, mfu 0.57%
iter 615: loss 1.9882, time 37.11ms, mfu 0.56%
iter 620: loss 1.9306, time 37.28ms, mfu 0.56%
iter 625: loss 1.9976, time 37.05ms, mfu 0.56%
iter 630: loss 1.8946, time 27.65ms, mfu 0.58%
iter 635: loss 2.0086, time 21.85ms, mfu 0.61%
iter 640: loss 1.9583, time 22.19ms, mfu 0.64%
iter 645: loss 1.9550, time 23.90ms, mfu 0.66%
iter 650: loss 1.9099, time 26.17ms, mfu 0.67%
iter 655: loss 1.9372, time 27.01ms, mfu 0.68%
iter 660: loss 1.8896, time 30.27ms, mfu 0.68%
iter 665: loss 1.9508, time 23.84ms, mfu 0.70%
iter 670: loss 1.9753, time 22.44ms, mfu 0.72%
iter 675: loss 1.9348, time 22.12ms, mfu 0.74%
iter 680: loss 1.9489, time 29.33ms, mfu 0.73%
iter 685: loss 1.8052, time 24.54ms, mfu 0.74%
iter 690: loss 2.0198, time 22.74ms, mfu 0.76%
iter 695: loss 1.8732, time 24.65ms, mfu 0.76%
iter 700: loss 1.9225, time 26.13ms, mfu 0.76%
iter 705: loss 1.9276, time 29.82ms, mfu 0.75%
iter 710: loss 1.9177, time 36.19ms, mfu 0.73%
iter 715: loss 1.9408, time 36.49ms, mfu 0.72%
iter 720: loss 1.9506, time 37.86ms, mfu 0.70%
iter 725: loss 1.9007, time 37.99ms, mfu 0.68%
iter 730: loss 1.8568, time 36.96ms, mfu 0.67%
iter 735: loss 1.8460, time 36.51ms, mfu 0.66%
iter 740: loss 1.8498, time 19.60ms, mfu 0.69%
iter 745: loss 1.9474, time 38.10ms, mfu 0.68%
step 750: train loss 1.8270, val loss 1.9616
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.1
iter 750: loss 1.9521, time 771.95ms, mfu 0.61%
iter 755: loss 1.8296, time 36.97ms, mfu 0.61%
iter 760: loss 1.8788, time 36.35ms, mfu 0.60%
iter 765: loss 1.8993, time 37.49ms, mfu 0.59%
iter 770: loss 1.8640, time 36.78ms, mfu 0.59%
iter 775: loss 1.8591, time 19.61ms, mfu 0.63%
iter 780: loss 1.8208, time 36.80ms, mfu 0.63%
iter 785: loss 1.8380, time 37.11ms, mfu 0.62%
iter 790: loss 1.8637, time 36.64ms, mfu 0.61%
iter 795: loss 1.7567, time 36.36ms, mfu 0.61%
iter 800: loss 1.8660, time 37.76ms, mfu 0.60%
iter 805: loss 1.8714, time 36.58ms, mfu 0.59%
iter 810: loss 1.8301, time 36.95ms, mfu 0.59%
iter 815: loss 1.9577, time 38.61ms, mfu 0.58%
iter 820: loss 1.8407, time 36.91ms, mfu 0.58%
iter 825: loss 1.9246, time 36.18ms, mfu 0.58%
iter 830: loss 1.8884, time 36.35ms, mfu 0.58%
iter 835: loss 1.8595, time 36.39ms, mfu 0.57%
iter 840: loss 1.8780, time 37.39ms, mfu 0.57%
iter 845: loss 1.8365, time 38.89ms, mfu 0.56%
iter 850: loss 1.8252, time 37.08ms, mfu 0.56%
iter 855: loss 1.8624, time 36.50ms, mfu 0.56%
iter 860: loss 1.7914, time 37.98ms, mfu 0.56%
iter 865: loss 1.8335, time 35.09ms, mfu 0.56%
iter 870: loss 1.8010, time 37.21ms, mfu 0.56%
iter 875: loss 1.8053, time 36.76ms, mfu 0.56%
iter 880: loss 1.7939, time 36.59ms, mfu 0.56%
iter 885: loss 1.8522, time 36.57ms, mfu 0.56%
iter 890: loss 1.8654, time 34.23ms, mfu 0.56%
iter 895: loss 1.8215, time 36.29ms, mfu 0.56%
iter 900: loss 1.8841, time 36.96ms, mfu 0.56%
iter 905: loss 1.8246, time 36.64ms, mfu 0.56%
iter 910: loss 1.7906, time 36.38ms, mfu 0.56%
iter 915: loss 1.8322, time 36.27ms, mfu 0.56%
iter 920: loss 1.8811, time 36.64ms, mfu 0.56%
iter 925: loss 1.8635, time 36.08ms, mfu 0.56%
iter 930: loss 1.9225, time 36.75ms, mfu 0.56%
iter 935: loss 1.7304, time 37.16ms, mfu 0.56%
iter 940: loss 1.8576, time 36.64ms, mfu 0.55%
iter 945: loss 1.8361, time 25.28ms, mfu 0.58%
iter 950: loss 1.8251, time 36.49ms, mfu 0.58%
iter 955: loss 1.8141, time 37.64ms, mfu 0.57%
iter 960: loss 1.7642, time 21.74ms, mfu 0.61%
iter 965: loss 1.7491, time 24.26ms, mfu 0.63%
iter 970: loss 1.7796, time 22.55ms, mfu 0.66%
iter 975: loss 1.8129, time 24.49ms, mfu 0.67%
iter 980: loss 1.8923, time 22.65ms, mfu 0.70%
iter 985: loss 1.7618, time 23.00ms, mfu 0.71%
iter 990: loss 1.8095, time 25.09ms, mfu 0.72%
iter 995: loss 1.8361, time 21.06ms, mfu 0.75%
step 1000: train loss 1.7309, val loss 1.8750
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.1
iter 1000: loss 1.8320, time 842.26ms, mfu 0.67%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.1 at: 
wandb: Find logs at: wandb/run-20251024_005921-dw4o2w74/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 256
Overriding: batch_size = 16
Overriding: max_iters = 1000
Overriding: dropout = 0.2
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.2
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.2
tokens per iteration will be: 2,048
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 4.74M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 4,768,000 parameters
num non-decayed parameter tensors: 13, with 3,328 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: ⣽ setting up run lt6q79pd (0.3s)
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_010010-lt6q79pd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.2
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/lt6q79pd
step 0: train loss 4.2246, val loss 4.2254
iter 0: loss 4.2213, time 1149.08ms, mfu -100.00%
iter 5: loss 3.6369, time 37.38ms, mfu 0.54%
iter 10: loss 3.3667, time 35.76ms, mfu 0.54%
iter 15: loss 3.1739, time 36.72ms, mfu 0.54%
iter 20: loss 3.0454, time 35.42ms, mfu 0.55%
iter 25: loss 2.9313, time 36.74ms, mfu 0.55%
iter 30: loss 2.8640, time 35.99ms, mfu 0.55%
iter 35: loss 2.7836, time 36.42ms, mfu 0.55%
iter 40: loss 2.6883, time 24.70ms, mfu 0.58%
iter 45: loss 2.6653, time 22.07ms, mfu 0.61%
iter 50: loss 2.6165, time 31.94ms, mfu 0.61%
iter 55: loss 2.6107, time 21.76ms, mfu 0.64%
iter 60: loss 2.5703, time 21.95ms, mfu 0.67%
iter 65: loss 2.5852, time 27.94ms, mfu 0.68%
iter 70: loss 2.6227, time 28.11ms, mfu 0.68%
iter 75: loss 2.5290, time 24.94ms, mfu 0.69%
iter 80: loss 2.5459, time 25.15ms, mfu 0.70%
iter 85: loss 2.5556, time 23.37ms, mfu 0.72%
iter 90: loss 2.5788, time 26.39ms, mfu 0.73%
iter 95: loss 2.5461, time 30.72ms, mfu 0.72%
iter 100: loss 2.5068, time 26.78ms, mfu 0.72%
iter 105: loss 2.5687, time 25.84ms, mfu 0.73%
iter 110: loss 2.5476, time 28.24ms, mfu 0.73%
iter 115: loss 2.5533, time 22.18ms, mfu 0.75%
iter 120: loss 2.5594, time 36.19ms, mfu 0.73%
iter 125: loss 2.4870, time 36.62ms, mfu 0.71%
iter 130: loss 2.5380, time 36.94ms, mfu 0.69%
iter 135: loss 2.5194, time 36.58ms, mfu 0.68%
iter 140: loss 2.4844, time 34.72ms, mfu 0.67%
iter 145: loss 2.5404, time 36.97ms, mfu 0.66%
iter 150: loss 2.4813, time 36.26ms, mfu 0.65%
iter 155: loss 2.5529, time 37.07ms, mfu 0.64%
iter 160: loss 2.5272, time 36.08ms, mfu 0.63%
iter 165: loss 2.4664, time 39.35ms, mfu 0.62%
iter 170: loss 2.4937, time 36.05ms, mfu 0.61%
iter 175: loss 2.4449, time 36.73ms, mfu 0.61%
iter 180: loss 2.4611, time 36.21ms, mfu 0.60%
iter 185: loss 2.4651, time 33.83ms, mfu 0.60%
iter 190: loss 2.4597, time 36.60ms, mfu 0.60%
iter 195: loss 2.4598, time 22.53ms, mfu 0.63%
iter 200: loss 2.4799, time 37.29ms, mfu 0.62%
iter 205: loss 2.4588, time 36.31ms, mfu 0.61%
iter 210: loss 2.4431, time 36.63ms, mfu 0.61%
iter 215: loss 2.5094, time 36.85ms, mfu 0.60%
iter 220: loss 2.4435, time 36.15ms, mfu 0.60%
iter 225: loss 2.4600, time 35.85ms, mfu 0.59%
iter 230: loss 2.4103, time 36.37ms, mfu 0.59%
iter 235: loss 2.4403, time 36.64ms, mfu 0.59%
iter 240: loss 2.4494, time 36.75ms, mfu 0.58%
iter 245: loss 2.4527, time 36.56ms, mfu 0.58%
step 250: train loss 2.4175, val loss 2.4293
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.2
iter 250: loss 2.4674, time 693.60ms, mfu 0.52%
iter 255: loss 2.4324, time 36.98ms, mfu 0.53%
iter 260: loss 2.4347, time 37.28ms, mfu 0.53%
iter 265: loss 2.4144, time 36.34ms, mfu 0.53%
iter 270: loss 2.4567, time 36.48ms, mfu 0.53%
iter 275: loss 2.4328, time 37.61ms, mfu 0.53%
iter 280: loss 2.4232, time 36.30ms, mfu 0.54%
iter 285: loss 2.3818, time 36.02ms, mfu 0.54%
iter 290: loss 2.4413, time 42.33ms, mfu 0.53%
iter 295: loss 2.4121, time 36.19ms, mfu 0.53%
iter 300: loss 2.3746, time 36.35ms, mfu 0.54%
iter 305: loss 2.4048, time 37.49ms, mfu 0.54%
iter 310: loss 2.4121, time 35.30ms, mfu 0.54%
iter 315: loss 2.4417, time 38.25ms, mfu 0.54%
iter 320: loss 2.3940, time 23.01ms, mfu 0.57%
iter 325: loss 2.3704, time 25.23ms, mfu 0.60%
iter 330: loss 2.3493, time 36.49ms, mfu 0.59%
iter 335: loss 2.3734, time 36.39ms, mfu 0.59%
iter 340: loss 2.3410, time 37.15ms, mfu 0.58%
iter 345: loss 2.3437, time 36.09ms, mfu 0.58%
iter 350: loss 2.4014, time 33.22ms, mfu 0.58%
iter 355: loss 2.3599, time 33.53ms, mfu 0.59%
iter 360: loss 2.3157, time 36.71ms, mfu 0.58%
iter 365: loss 2.3700, time 36.18ms, mfu 0.58%
iter 370: loss 2.3749, time 37.21ms, mfu 0.58%
iter 375: loss 2.2634, time 33.56ms, mfu 0.58%
iter 380: loss 2.3777, time 22.92ms, mfu 0.61%
iter 385: loss 2.3596, time 23.24ms, mfu 0.64%
iter 390: loss 2.3219, time 21.16ms, mfu 0.67%
iter 395: loss 2.3144, time 23.13ms, mfu 0.69%
iter 400: loss 2.3518, time 25.60ms, mfu 0.70%
iter 405: loss 2.2564, time 28.34ms, mfu 0.70%
iter 410: loss 2.2784, time 29.21ms, mfu 0.70%
iter 415: loss 2.2695, time 21.13ms, mfu 0.72%
iter 420: loss 2.2814, time 32.23ms, mfu 0.71%
iter 425: loss 2.2517, time 26.58ms, mfu 0.72%
iter 430: loss 2.3157, time 20.86ms, mfu 0.74%
iter 435: loss 2.2712, time 30.48ms, mfu 0.74%
iter 440: loss 2.3553, time 25.88ms, mfu 0.74%
iter 445: loss 2.3046, time 16.10ms, mfu 0.79%
iter 450: loss 2.2885, time 36.12ms, mfu 0.77%
iter 455: loss 2.2715, time 36.67ms, mfu 0.75%
iter 460: loss 2.2416, time 35.61ms, mfu 0.73%
iter 465: loss 2.2268, time 36.98ms, mfu 0.71%
iter 470: loss 2.2336, time 37.52ms, mfu 0.69%
iter 475: loss 2.1926, time 36.27ms, mfu 0.68%
iter 480: loss 2.2983, time 36.91ms, mfu 0.67%
iter 485: loss 2.2107, time 36.20ms, mfu 0.66%
iter 490: loss 2.2214, time 37.45ms, mfu 0.64%
iter 495: loss 2.3129, time 36.98ms, mfu 0.63%
step 500: train loss 2.1762, val loss 2.2142
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.2
iter 500: loss 2.2532, time 773.61ms, mfu 0.57%
iter 505: loss 2.1822, time 38.81ms, mfu 0.57%
iter 510: loss 2.2796, time 36.75ms, mfu 0.57%
iter 515: loss 2.2209, time 36.13ms, mfu 0.57%
iter 520: loss 2.2433, time 36.85ms, mfu 0.56%
iter 525: loss 2.2207, time 39.08ms, mfu 0.56%
iter 530: loss 2.2297, time 37.40ms, mfu 0.56%
iter 535: loss 2.2353, time 36.29ms, mfu 0.56%
iter 540: loss 2.2056, time 36.22ms, mfu 0.56%
iter 545: loss 2.1898, time 36.47ms, mfu 0.56%
iter 550: loss 2.1659, time 36.63ms, mfu 0.56%
iter 555: loss 2.1565, time 37.54ms, mfu 0.55%
iter 560: loss 2.1519, time 37.12ms, mfu 0.55%
iter 565: loss 2.2056, time 36.99ms, mfu 0.55%
iter 570: loss 2.1462, time 36.89ms, mfu 0.55%
iter 575: loss 2.1440, time 36.35ms, mfu 0.55%
iter 580: loss 2.1823, time 36.78ms, mfu 0.55%
iter 585: loss 2.1645, time 29.73ms, mfu 0.57%
iter 590: loss 2.1625, time 37.07ms, mfu 0.56%
iter 595: loss 2.1746, time 36.69ms, mfu 0.56%
iter 600: loss 2.1051, time 36.95ms, mfu 0.56%
iter 605: loss 2.1143, time 37.13ms, mfu 0.56%
iter 610: loss 2.1557, time 36.30ms, mfu 0.56%
iter 615: loss 2.1563, time 36.13ms, mfu 0.56%
iter 620: loss 2.1005, time 37.09ms, mfu 0.56%
iter 625: loss 2.1556, time 36.98ms, mfu 0.56%
iter 630: loss 2.0754, time 38.32ms, mfu 0.55%
iter 635: loss 2.1809, time 37.50ms, mfu 0.55%
iter 640: loss 2.1084, time 18.99ms, mfu 0.60%
iter 645: loss 2.0922, time 36.62ms, mfu 0.60%
iter 650: loss 2.0939, time 37.62ms, mfu 0.59%
iter 655: loss 2.1016, time 36.92ms, mfu 0.59%
iter 660: loss 2.0657, time 37.42ms, mfu 0.58%
iter 665: loss 2.1150, time 36.50ms, mfu 0.58%
iter 670: loss 2.1369, time 37.48ms, mfu 0.58%
iter 675: loss 2.1234, time 36.85ms, mfu 0.57%
iter 680: loss 2.1306, time 36.24ms, mfu 0.57%
iter 685: loss 2.0008, time 36.30ms, mfu 0.57%
iter 690: loss 2.1640, time 37.02ms, mfu 0.57%
iter 695: loss 2.0641, time 37.63ms, mfu 0.56%
iter 700: loss 2.0855, time 21.73ms, mfu 0.60%
iter 705: loss 2.0854, time 22.97ms, mfu 0.63%
iter 710: loss 2.0803, time 21.31ms, mfu 0.66%
iter 715: loss 2.1132, time 25.24ms, mfu 0.67%
iter 720: loss 2.1184, time 25.97ms, mfu 0.69%
iter 725: loss 2.0546, time 22.57ms, mfu 0.71%
iter 730: loss 2.0487, time 31.42ms, mfu 0.70%
iter 735: loss 2.0450, time 21.25ms, mfu 0.73%
iter 740: loss 2.0344, time 21.77ms, mfu 0.75%
iter 745: loss 2.1162, time 27.21ms, mfu 0.75%
step 750: train loss 1.9837, val loss 2.0772
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.2
iter 750: loss 2.1107, time 798.29ms, mfu 0.67%
iter 755: loss 2.0495, time 35.36ms, mfu 0.66%
iter 760: loss 2.0742, time 38.12ms, mfu 0.65%
iter 765: loss 2.0739, time 37.10ms, mfu 0.64%
iter 770: loss 2.0450, time 36.43ms, mfu 0.63%
iter 775: loss 2.0117, time 39.25ms, mfu 0.62%
iter 780: loss 1.9973, time 36.88ms, mfu 0.61%
iter 785: loss 2.0357, time 36.23ms, mfu 0.61%
iter 790: loss 2.0329, time 33.92ms, mfu 0.61%
iter 795: loss 1.9505, time 36.55ms, mfu 0.60%
iter 800: loss 2.0484, time 36.68ms, mfu 0.60%
iter 805: loss 2.0405, time 36.20ms, mfu 0.59%
iter 810: loss 2.0098, time 36.45ms, mfu 0.59%
iter 815: loss 2.1500, time 36.78ms, mfu 0.58%
iter 820: loss 2.0177, time 36.21ms, mfu 0.58%
iter 825: loss 2.0822, time 37.05ms, mfu 0.58%
iter 830: loss 2.0660, time 36.68ms, mfu 0.58%
iter 835: loss 2.0361, time 36.51ms, mfu 0.57%
iter 840: loss 2.0457, time 36.40ms, mfu 0.57%
iter 845: loss 2.0056, time 29.56ms, mfu 0.58%
iter 850: loss 2.0095, time 35.56ms, mfu 0.58%
iter 855: loss 2.0330, time 36.95ms, mfu 0.58%
iter 860: loss 1.9817, time 37.71ms, mfu 0.57%
iter 865: loss 2.0548, time 36.76ms, mfu 0.57%
iter 870: loss 1.9887, time 36.76ms, mfu 0.57%
iter 875: loss 2.0026, time 37.54ms, mfu 0.57%
iter 880: loss 1.9664, time 36.72ms, mfu 0.56%
iter 885: loss 2.0395, time 36.08ms, mfu 0.56%
iter 890: loss 2.0360, time 36.44ms, mfu 0.56%
iter 895: loss 1.9974, time 36.76ms, mfu 0.56%
iter 900: loss 2.0647, time 38.03ms, mfu 0.56%
iter 905: loss 2.0072, time 36.31ms, mfu 0.56%
iter 910: loss 1.9676, time 36.14ms, mfu 0.56%
iter 915: loss 1.9958, time 36.68ms, mfu 0.56%
iter 920: loss 2.0568, time 36.36ms, mfu 0.56%
iter 925: loss 2.0322, time 36.97ms, mfu 0.56%
iter 930: loss 2.1012, time 26.25ms, mfu 0.58%
iter 935: loss 1.9248, time 36.12ms, mfu 0.58%
iter 940: loss 2.0617, time 36.19ms, mfu 0.57%
iter 945: loss 1.9843, time 36.16ms, mfu 0.57%
iter 950: loss 2.0127, time 36.40ms, mfu 0.57%
iter 955: loss 1.9974, time 36.55ms, mfu 0.57%
iter 960: loss 1.9752, time 36.85ms, mfu 0.57%
iter 965: loss 1.9568, time 36.06ms, mfu 0.57%
iter 970: loss 1.9855, time 37.30ms, mfu 0.56%
iter 975: loss 1.9683, time 36.11ms, mfu 0.56%
iter 980: loss 2.0724, time 37.01ms, mfu 0.56%
iter 985: loss 1.9356, time 31.00ms, mfu 0.57%
iter 990: loss 1.9991, time 35.99ms, mfu 0.57%
iter 995: loss 2.0469, time 36.44ms, mfu 0.57%
step 1000: train loss 1.8910, val loss 1.9985
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.2
iter 1000: loss 1.9959, time 745.33ms, mfu 0.51%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters1000_dropout0.2 at: 
wandb: Find logs at: wandb/run-20251024_010010-lt6q79pd/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 256
Overriding: batch_size = 16
Overriding: max_iters = 2000
Overriding: dropout = 0.1
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
tokens per iteration will be: 2,048
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 4.74M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 4,768,000 parameters
num non-decayed parameter tensors: 13, with 3,328 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_010058-a1siohfu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/a1siohfu
step 0: train loss 4.2246, val loss 4.2254
iter 0: loss 4.2315, time 1226.85ms, mfu -100.00%
iter 5: loss 3.5973, time 33.59ms, mfu 0.60%
iter 10: loss 3.3283, time 34.43ms, mfu 0.60%
iter 15: loss 3.1139, time 36.09ms, mfu 0.60%
iter 20: loss 3.0041, time 37.24ms, mfu 0.59%
iter 25: loss 2.8830, time 36.93ms, mfu 0.59%
iter 30: loss 2.8201, time 34.88ms, mfu 0.59%
iter 35: loss 2.7500, time 34.80ms, mfu 0.59%
iter 40: loss 2.6598, time 35.66ms, mfu 0.58%
iter 45: loss 2.6422, time 35.74ms, mfu 0.58%
iter 50: loss 2.5944, time 31.93ms, mfu 0.59%
iter 55: loss 2.5907, time 36.08ms, mfu 0.58%
iter 60: loss 2.5587, time 36.23ms, mfu 0.58%
iter 65: loss 2.5654, time 31.22ms, mfu 0.59%
iter 70: loss 2.6072, time 36.02ms, mfu 0.59%
iter 75: loss 2.5114, time 35.32ms, mfu 0.58%
iter 80: loss 2.5261, time 37.30ms, mfu 0.58%
iter 85: loss 2.5458, time 22.28ms, mfu 0.61%
iter 90: loss 2.5526, time 20.41ms, mfu 0.65%
iter 95: loss 2.5121, time 24.40ms, mfu 0.67%
iter 100: loss 2.4782, time 27.58ms, mfu 0.67%
iter 105: loss 2.5381, time 20.59ms, mfu 0.71%
iter 110: loss 2.5334, time 21.02ms, mfu 0.73%
iter 115: loss 2.5684, time 25.58ms, mfu 0.74%
iter 120: loss 2.5496, time 22.79ms, mfu 0.75%
iter 125: loss 2.4874, time 22.42ms, mfu 0.77%
iter 130: loss 2.5131, time 28.47ms, mfu 0.76%
iter 135: loss 2.5026, time 22.60ms, mfu 0.77%
iter 140: loss 2.4527, time 22.68ms, mfu 0.79%
iter 145: loss 2.5124, time 23.88ms, mfu 0.79%
iter 150: loss 2.4608, time 29.39ms, mfu 0.78%
iter 155: loss 2.5471, time 17.02ms, mfu 0.82%
iter 160: loss 2.4973, time 37.02ms, mfu 0.79%
iter 165: loss 2.4475, time 36.31ms, mfu 0.77%
iter 170: loss 2.4880, time 36.37ms, mfu 0.75%
iter 175: loss 2.4228, time 36.53ms, mfu 0.73%
iter 180: loss 2.4474, time 30.77ms, mfu 0.72%
iter 185: loss 2.4517, time 36.44ms, mfu 0.71%
iter 190: loss 2.4329, time 33.57ms, mfu 0.70%
iter 195: loss 2.4431, time 36.18ms, mfu 0.68%
iter 200: loss 2.4456, time 34.51ms, mfu 0.67%
iter 205: loss 2.4291, time 36.51ms, mfu 0.66%
iter 210: loss 2.4224, time 36.66ms, mfu 0.65%
iter 215: loss 2.4850, time 35.69ms, mfu 0.64%
iter 220: loss 2.4123, time 36.54ms, mfu 0.63%
iter 225: loss 2.4339, time 35.34ms, mfu 0.63%
iter 230: loss 2.3672, time 34.48ms, mfu 0.62%
iter 235: loss 2.4091, time 37.56ms, mfu 0.61%
iter 240: loss 2.4056, time 36.56ms, mfu 0.61%
iter 245: loss 2.4094, time 36.33ms, mfu 0.60%
step 250: train loss 2.3928, val loss 2.4023
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
iter 250: loss 2.4251, time 709.58ms, mfu 0.55%
iter 255: loss 2.3962, time 36.40ms, mfu 0.55%
iter 260: loss 2.4158, time 36.13ms, mfu 0.55%
iter 265: loss 2.3885, time 36.51ms, mfu 0.55%
iter 270: loss 2.4002, time 36.65ms, mfu 0.55%
iter 275: loss 2.3789, time 38.58ms, mfu 0.55%
iter 280: loss 2.3727, time 35.65ms, mfu 0.55%
iter 285: loss 2.3273, time 32.69ms, mfu 0.56%
iter 290: loss 2.3836, time 31.75ms, mfu 0.56%
iter 295: loss 2.3405, time 36.41ms, mfu 0.56%
iter 300: loss 2.2992, time 37.57ms, mfu 0.56%
iter 305: loss 2.3431, time 31.39ms, mfu 0.57%
iter 310: loss 2.3534, time 36.36ms, mfu 0.57%
iter 315: loss 2.3749, time 36.42ms, mfu 0.57%
iter 320: loss 2.3170, time 35.71ms, mfu 0.57%
iter 325: loss 2.3086, time 36.84ms, mfu 0.56%
iter 330: loss 2.2919, time 36.45ms, mfu 0.56%
iter 335: loss 2.3028, time 37.22ms, mfu 0.56%
iter 340: loss 2.2677, time 33.16ms, mfu 0.57%
iter 345: loss 2.2532, time 36.89ms, mfu 0.56%
iter 350: loss 2.3383, time 36.04ms, mfu 0.56%
iter 355: loss 2.2541, time 35.63ms, mfu 0.56%
iter 360: loss 2.2464, time 35.33ms, mfu 0.57%
iter 365: loss 2.2853, time 36.45ms, mfu 0.56%
iter 370: loss 2.2780, time 37.84ms, mfu 0.56%
iter 375: loss 2.1516, time 36.68ms, mfu 0.56%
iter 380: loss 2.2687, time 36.44ms, mfu 0.56%
iter 385: loss 2.2870, time 37.15ms, mfu 0.56%
iter 390: loss 2.2405, time 36.86ms, mfu 0.56%
iter 395: loss 2.2159, time 37.51ms, mfu 0.56%
iter 400: loss 2.2457, time 36.35ms, mfu 0.56%
iter 405: loss 2.1183, time 36.32ms, mfu 0.56%
iter 410: loss 2.1705, time 36.48ms, mfu 0.56%
iter 415: loss 2.1756, time 28.71ms, mfu 0.57%
iter 420: loss 2.1507, time 28.79ms, mfu 0.58%
iter 425: loss 2.1443, time 21.74ms, mfu 0.62%
iter 430: loss 2.1726, time 24.28ms, mfu 0.64%
iter 435: loss 2.1476, time 24.20ms, mfu 0.66%
iter 440: loss 2.2326, time 21.27ms, mfu 0.69%
iter 445: loss 2.1855, time 21.38ms, mfu 0.71%
iter 450: loss 2.1571, time 21.72ms, mfu 0.74%
iter 455: loss 2.0960, time 26.23ms, mfu 0.74%
iter 460: loss 2.1161, time 32.15ms, mfu 0.73%
iter 465: loss 2.0835, time 25.44ms, mfu 0.73%
iter 470: loss 2.0717, time 24.69ms, mfu 0.74%
iter 475: loss 2.0524, time 24.41ms, mfu 0.75%
iter 480: loss 2.1750, time 25.93ms, mfu 0.75%
iter 485: loss 2.0890, time 22.59ms, mfu 0.77%
iter 490: loss 2.0751, time 37.82ms, mfu 0.74%
iter 495: loss 2.1398, time 36.27ms, mfu 0.73%
step 500: train loss 2.0508, val loss 2.1188
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
iter 500: loss 2.1123, time 724.35ms, mfu 0.66%
iter 505: loss 2.0638, time 38.17ms, mfu 0.64%
iter 510: loss 2.1201, time 36.88ms, mfu 0.63%
iter 515: loss 2.0824, time 35.15ms, mfu 0.63%
iter 520: loss 2.1267, time 36.17ms, mfu 0.62%
iter 525: loss 2.0835, time 38.43ms, mfu 0.61%
iter 530: loss 2.0614, time 36.50ms, mfu 0.61%
iter 535: loss 2.1088, time 37.04ms, mfu 0.60%
iter 540: loss 2.0486, time 21.15ms, mfu 0.64%
iter 545: loss 2.0488, time 36.63ms, mfu 0.63%
iter 550: loss 2.0151, time 36.95ms, mfu 0.62%
iter 555: loss 2.0063, time 37.31ms, mfu 0.61%
iter 560: loss 1.9869, time 37.06ms, mfu 0.60%
iter 565: loss 2.0293, time 36.28ms, mfu 0.60%
iter 570: loss 1.9910, time 38.97ms, mfu 0.59%
iter 575: loss 1.9797, time 36.26ms, mfu 0.59%
iter 580: loss 2.0337, time 37.01ms, mfu 0.58%
iter 585: loss 2.0203, time 38.08ms, mfu 0.58%
iter 590: loss 1.9982, time 35.89ms, mfu 0.58%
iter 595: loss 2.0072, time 34.07ms, mfu 0.58%
iter 600: loss 1.9666, time 36.84ms, mfu 0.58%
iter 605: loss 1.9507, time 37.59ms, mfu 0.57%
iter 610: loss 1.9861, time 36.37ms, mfu 0.57%
iter 615: loss 1.9882, time 37.27ms, mfu 0.57%
iter 620: loss 1.9306, time 36.77ms, mfu 0.57%
iter 625: loss 1.9976, time 36.14ms, mfu 0.57%
iter 630: loss 1.8946, time 37.75ms, mfu 0.56%
iter 635: loss 2.0086, time 23.76ms, mfu 0.59%
iter 640: loss 1.9583, time 18.40ms, mfu 0.64%
iter 645: loss 1.9550, time 37.20ms, mfu 0.63%
iter 650: loss 1.9099, time 38.00ms, mfu 0.62%
iter 655: loss 1.9372, time 36.82ms, mfu 0.61%
iter 660: loss 1.8896, time 34.55ms, mfu 0.61%
iter 665: loss 1.9508, time 36.32ms, mfu 0.61%
iter 670: loss 1.9753, time 37.67ms, mfu 0.60%
iter 675: loss 1.9348, time 37.51ms, mfu 0.59%
iter 680: loss 1.9489, time 25.70ms, mfu 0.61%
iter 685: loss 1.8052, time 36.77ms, mfu 0.61%
iter 690: loss 2.0198, time 36.48ms, mfu 0.60%
iter 695: loss 1.8732, time 36.28ms, mfu 0.60%
iter 700: loss 1.9225, time 36.69ms, mfu 0.59%
iter 705: loss 1.9276, time 37.70ms, mfu 0.59%
iter 710: loss 1.9177, time 37.75ms, mfu 0.58%
iter 715: loss 1.9408, time 37.59ms, mfu 0.58%
iter 720: loss 1.9506, time 36.51ms, mfu 0.57%
iter 725: loss 1.9007, time 36.26ms, mfu 0.57%
iter 730: loss 1.8568, time 36.61ms, mfu 0.57%
iter 735: loss 1.8460, time 33.28ms, mfu 0.57%
iter 740: loss 1.8498, time 23.89ms, mfu 0.60%
iter 745: loss 1.9474, time 22.90ms, mfu 0.63%
step 750: train loss 1.8270, val loss 1.9616
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
iter 750: loss 1.9521, time 813.00ms, mfu 0.57%
iter 755: loss 1.8296, time 22.38ms, mfu 0.60%
iter 760: loss 1.8788, time 26.22ms, mfu 0.62%
iter 765: loss 1.8993, time 26.10ms, mfu 0.63%
iter 770: loss 1.8640, time 23.06ms, mfu 0.66%
iter 775: loss 1.8591, time 26.69ms, mfu 0.67%
iter 780: loss 1.8208, time 26.95ms, mfu 0.68%
iter 785: loss 1.8380, time 27.92ms, mfu 0.68%
iter 790: loss 1.8637, time 33.99ms, mfu 0.67%
iter 795: loss 1.7567, time 36.79ms, mfu 0.66%
iter 800: loss 1.8660, time 29.66ms, mfu 0.66%
iter 805: loss 1.8714, time 36.32ms, mfu 0.65%
iter 810: loss 1.8301, time 36.51ms, mfu 0.64%
iter 815: loss 1.9577, time 36.57ms, mfu 0.63%
iter 820: loss 1.8407, time 38.18ms, mfu 0.62%
iter 825: loss 1.9246, time 39.21ms, mfu 0.61%
iter 830: loss 1.8884, time 36.72ms, mfu 0.61%
iter 835: loss 1.8595, time 36.37ms, mfu 0.60%
iter 840: loss 1.8780, time 36.04ms, mfu 0.60%
iter 845: loss 1.8365, time 36.22ms, mfu 0.59%
iter 850: loss 1.8252, time 37.61ms, mfu 0.59%
iter 855: loss 1.8624, time 27.37ms, mfu 0.60%
iter 860: loss 1.7914, time 36.31ms, mfu 0.60%
iter 865: loss 1.8335, time 36.41ms, mfu 0.59%
iter 870: loss 1.8010, time 36.34ms, mfu 0.59%
iter 875: loss 1.8053, time 35.76ms, mfu 0.59%
iter 880: loss 1.7939, time 36.10ms, mfu 0.58%
iter 885: loss 1.8522, time 37.19ms, mfu 0.58%
iter 890: loss 1.8654, time 37.03ms, mfu 0.58%
iter 895: loss 1.8215, time 35.63ms, mfu 0.58%
iter 900: loss 1.8841, time 36.25ms, mfu 0.57%
iter 905: loss 1.8246, time 36.73ms, mfu 0.57%
iter 910: loss 1.7906, time 36.23ms, mfu 0.57%
iter 915: loss 1.8322, time 37.85ms, mfu 0.57%
iter 920: loss 1.8811, time 36.88ms, mfu 0.56%
iter 925: loss 1.8635, time 37.68ms, mfu 0.56%
iter 930: loss 1.9225, time 36.54ms, mfu 0.56%
iter 935: loss 1.7304, time 36.87ms, mfu 0.56%
iter 940: loss 1.8576, time 36.53ms, mfu 0.56%
iter 945: loss 1.8361, time 37.77ms, mfu 0.56%
iter 950: loss 1.8251, time 36.83ms, mfu 0.56%
iter 955: loss 1.8141, time 36.61ms, mfu 0.56%
iter 960: loss 1.7642, time 36.79ms, mfu 0.55%
iter 965: loss 1.7491, time 36.60ms, mfu 0.55%
iter 970: loss 1.7796, time 36.24ms, mfu 0.56%
iter 975: loss 1.8129, time 37.67ms, mfu 0.55%
iter 980: loss 1.8923, time 36.42ms, mfu 0.55%
iter 985: loss 1.7618, time 36.76ms, mfu 0.55%
iter 990: loss 1.8095, time 37.17ms, mfu 0.55%
iter 995: loss 1.8361, time 29.66ms, mfu 0.57%
step 1000: train loss 1.7309, val loss 1.8750
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
iter 1000: loss 1.8320, time 775.67ms, mfu 0.51%
iter 1005: loss 1.8774, time 22.48ms, mfu 0.55%
iter 1010: loss 1.8168, time 37.19ms, mfu 0.55%
iter 1015: loss 1.8109, time 35.81ms, mfu 0.55%
iter 1020: loss 1.7578, time 35.41ms, mfu 0.55%
iter 1025: loss 1.8866, time 38.33ms, mfu 0.55%
iter 1030: loss 1.8863, time 38.89ms, mfu 0.55%
iter 1035: loss 1.8380, time 36.31ms, mfu 0.55%
iter 1040: loss 1.7326, time 36.32ms, mfu 0.55%
iter 1045: loss 1.7637, time 26.41ms, mfu 0.57%
iter 1050: loss 1.7844, time 25.81ms, mfu 0.59%
iter 1055: loss 1.8110, time 27.85ms, mfu 0.61%
iter 1060: loss 1.8610, time 22.10ms, mfu 0.64%
iter 1065: loss 1.8153, time 22.77ms, mfu 0.66%
iter 1070: loss 1.7640, time 23.22ms, mfu 0.68%
iter 1075: loss 1.7356, time 23.06ms, mfu 0.70%
iter 1080: loss 1.7870, time 25.04ms, mfu 0.71%
iter 1085: loss 1.8462, time 25.91ms, mfu 0.72%
iter 1090: loss 1.8521, time 29.92ms, mfu 0.71%
iter 1095: loss 1.7762, time 23.74ms, mfu 0.73%
iter 1100: loss 1.7889, time 26.46ms, mfu 0.73%
iter 1105: loss 1.8401, time 24.54ms, mfu 0.74%
iter 1110: loss 1.7618, time 28.35ms, mfu 0.74%
iter 1115: loss 1.7149, time 33.97ms, mfu 0.72%
iter 1120: loss 1.7553, time 36.25ms, mfu 0.71%
iter 1125: loss 1.8126, time 37.01ms, mfu 0.69%
iter 1130: loss 1.7793, time 36.11ms, mfu 0.68%
iter 1135: loss 1.7384, time 36.95ms, mfu 0.66%
iter 1140: loss 1.7662, time 36.45ms, mfu 0.65%
iter 1145: loss 1.7683, time 36.03ms, mfu 0.64%
iter 1150: loss 1.7620, time 36.27ms, mfu 0.64%
iter 1155: loss 1.7967, time 37.30ms, mfu 0.63%
iter 1160: loss 1.7479, time 36.93ms, mfu 0.62%
iter 1165: loss 1.7279, time 36.85ms, mfu 0.61%
iter 1170: loss 1.7780, time 36.28ms, mfu 0.61%
iter 1175: loss 1.8233, time 31.78ms, mfu 0.61%
iter 1180: loss 1.8406, time 36.44ms, mfu 0.60%
iter 1185: loss 1.7844, time 38.16ms, mfu 0.60%
iter 1190: loss 1.7105, time 36.88ms, mfu 0.59%
iter 1195: loss 1.7924, time 34.80ms, mfu 0.59%
iter 1200: loss 1.8230, time 36.88ms, mfu 0.59%
iter 1205: loss 1.7784, time 36.65ms, mfu 0.58%
iter 1210: loss 1.6936, time 35.70ms, mfu 0.58%
iter 1215: loss 1.7888, time 38.55ms, mfu 0.58%
iter 1220: loss 1.7467, time 36.85ms, mfu 0.57%
iter 1225: loss 1.7028, time 35.70ms, mfu 0.57%
iter 1230: loss 1.7860, time 34.55ms, mfu 0.57%
iter 1235: loss 1.7478, time 36.28ms, mfu 0.57%
iter 1240: loss 1.7849, time 36.48ms, mfu 0.57%
iter 1245: loss 1.7807, time 37.92ms, mfu 0.57%
step 1250: train loss 1.7046, val loss 1.8639
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
iter 1250: loss 1.7611, time 741.27ms, mfu 0.51%
iter 1255: loss 1.7949, time 37.71ms, mfu 0.51%
iter 1260: loss 1.7626, time 36.65ms, mfu 0.52%
iter 1265: loss 1.7769, time 36.42ms, mfu 0.52%
iter 1270: loss 1.8114, time 36.52ms, mfu 0.53%
iter 1275: loss 1.7828, time 37.59ms, mfu 0.53%
iter 1280: loss 1.7824, time 37.20ms, mfu 0.53%
iter 1285: loss 1.7454, time 36.50ms, mfu 0.53%
iter 1290: loss 1.7324, time 36.37ms, mfu 0.53%
iter 1295: loss 1.7875, time 38.10ms, mfu 0.53%
iter 1300: loss 1.8425, time 36.96ms, mfu 0.53%
iter 1305: loss 1.7402, time 36.63ms, mfu 0.54%
iter 1310: loss 1.7783, time 36.67ms, mfu 0.54%
iter 1315: loss 1.6460, time 36.35ms, mfu 0.54%
iter 1320: loss 1.7295, time 35.80ms, mfu 0.54%
iter 1325: loss 1.7443, time 28.12ms, mfu 0.56%
iter 1330: loss 1.8023, time 36.38ms, mfu 0.56%
iter 1335: loss 1.7861, time 34.95ms, mfu 0.56%
iter 1340: loss 1.7158, time 36.71ms, mfu 0.56%
iter 1345: loss 1.8253, time 36.39ms, mfu 0.56%
iter 1350: loss 1.7853, time 36.39ms, mfu 0.56%
iter 1355: loss 1.7672, time 36.24ms, mfu 0.56%
iter 1360: loss 1.7541, time 36.93ms, mfu 0.56%
iter 1365: loss 1.8210, time 36.26ms, mfu 0.56%
iter 1370: loss 1.7942, time 36.30ms, mfu 0.56%
iter 1375: loss 1.6832, time 22.09ms, mfu 0.59%
iter 1380: loss 1.7679, time 25.87ms, mfu 0.61%
iter 1385: loss 1.7613, time 22.34ms, mfu 0.64%
iter 1390: loss 1.6830, time 24.94ms, mfu 0.66%
iter 1395: loss 1.7856, time 23.40ms, mfu 0.68%
iter 1400: loss 1.7328, time 28.57ms, mfu 0.68%
iter 1405: loss 1.7417, time 22.60ms, mfu 0.70%
iter 1410: loss 1.8320, time 24.87ms, mfu 0.71%
iter 1415: loss 1.7200, time 21.41ms, mfu 0.74%
iter 1420: loss 1.8275, time 20.14ms, mfu 0.76%
iter 1425: loss 1.7371, time 27.44ms, mfu 0.76%
iter 1430: loss 1.7881, time 32.04ms, mfu 0.75%
iter 1435: loss 1.7201, time 20.53ms, mfu 0.77%
iter 1440: loss 1.8057, time 23.12ms, mfu 0.78%
iter 1445: loss 1.8537, time 25.50ms, mfu 0.78%
iter 1450: loss 1.7887, time 25.02ms, mfu 0.79%
iter 1455: loss 1.7805, time 36.85ms, mfu 0.76%
iter 1460: loss 1.7336, time 35.52ms, mfu 0.74%
iter 1465: loss 1.8300, time 37.39ms, mfu 0.72%
iter 1470: loss 1.7022, time 36.72ms, mfu 0.71%
iter 1475: loss 1.7869, time 35.86ms, mfu 0.69%
iter 1480: loss 1.7434, time 36.38ms, mfu 0.68%
iter 1485: loss 1.7571, time 36.29ms, mfu 0.67%
iter 1490: loss 1.7167, time 36.20ms, mfu 0.65%
iter 1495: loss 1.7808, time 35.61ms, mfu 0.65%
step 1500: train loss 1.6752, val loss 1.8266
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
iter 1500: loss 1.8281, time 723.90ms, mfu 0.58%
iter 1505: loss 1.7651, time 36.60ms, mfu 0.58%
iter 1510: loss 1.7249, time 35.75ms, mfu 0.58%
iter 1515: loss 1.7938, time 31.32ms, mfu 0.59%
iter 1520: loss 1.6976, time 36.31ms, mfu 0.58%
iter 1525: loss 1.7162, time 38.70ms, mfu 0.58%
iter 1530: loss 1.7102, time 36.85ms, mfu 0.57%
iter 1535: loss 1.7392, time 22.71ms, mfu 0.61%
iter 1540: loss 1.8100, time 36.77ms, mfu 0.60%
iter 1545: loss 1.7547, time 35.67ms, mfu 0.60%
iter 1550: loss 1.7963, time 36.40ms, mfu 0.59%
iter 1555: loss 1.7439, time 36.43ms, mfu 0.59%
iter 1560: loss 1.6893, time 36.28ms, mfu 0.59%
iter 1565: loss 1.7190, time 24.98ms, mfu 0.61%
iter 1570: loss 1.6950, time 36.12ms, mfu 0.60%
iter 1575: loss 1.7512, time 36.10ms, mfu 0.60%
iter 1580: loss 1.7684, time 36.28ms, mfu 0.59%
iter 1585: loss 1.8077, time 36.18ms, mfu 0.59%
iter 1590: loss 1.7444, time 36.39ms, mfu 0.59%
iter 1595: loss 1.7623, time 38.03ms, mfu 0.58%
iter 1600: loss 1.6859, time 36.42ms, mfu 0.58%
iter 1605: loss 1.7475, time 36.40ms, mfu 0.58%
iter 1610: loss 1.6849, time 36.43ms, mfu 0.57%
iter 1615: loss 1.6602, time 26.93ms, mfu 0.59%
iter 1620: loss 1.6510, time 19.10ms, mfu 0.64%
iter 1625: loss 1.7264, time 36.98ms, mfu 0.63%
iter 1630: loss 1.7466, time 34.33ms, mfu 0.63%
iter 1635: loss 1.6973, time 38.02ms, mfu 0.62%
iter 1640: loss 1.6782, time 32.81ms, mfu 0.62%
iter 1645: loss 1.7760, time 27.51ms, mfu 0.63%
iter 1650: loss 1.6740, time 36.98ms, mfu 0.62%
iter 1655: loss 1.7081, time 36.15ms, mfu 0.61%
iter 1660: loss 1.6674, time 36.54ms, mfu 0.61%
iter 1665: loss 1.7002, time 36.42ms, mfu 0.60%
iter 1670: loss 1.8665, time 35.72ms, mfu 0.60%
iter 1675: loss 1.7093, time 35.61ms, mfu 0.60%
iter 1680: loss 1.6919, time 36.08ms, mfu 0.59%
iter 1685: loss 1.7132, time 37.26ms, mfu 0.59%
iter 1690: loss 1.6931, time 36.57ms, mfu 0.58%
iter 1695: loss 1.7126, time 34.30ms, mfu 0.58%
iter 1700: loss 1.7281, time 36.69ms, mfu 0.58%
iter 1705: loss 1.7218, time 27.31ms, mfu 0.60%
iter 1710: loss 1.8446, time 21.38ms, mfu 0.63%
iter 1715: loss 1.6727, time 21.41ms, mfu 0.66%
iter 1720: loss 1.7372, time 21.62ms, mfu 0.69%
iter 1725: loss 1.6814, time 21.46ms, mfu 0.72%
iter 1730: loss 1.6911, time 30.73ms, mfu 0.71%
iter 1735: loss 1.7014, time 27.28ms, mfu 0.71%
iter 1740: loss 1.7131, time 23.59ms, mfu 0.73%
iter 1745: loss 1.6827, time 21.50ms, mfu 0.75%
step 1750: train loss 1.6382, val loss 1.7963
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
iter 1750: loss 1.7387, time 846.10ms, mfu 0.68%
iter 1755: loss 1.6325, time 26.28ms, mfu 0.69%
iter 1760: loss 1.7911, time 29.04ms, mfu 0.69%
iter 1765: loss 1.7060, time 26.23ms, mfu 0.69%
iter 1770: loss 1.7182, time 30.24ms, mfu 0.69%
iter 1775: loss 1.7509, time 37.84ms, mfu 0.68%
iter 1780: loss 1.6937, time 36.98ms, mfu 0.66%
iter 1785: loss 1.7766, time 33.93ms, mfu 0.66%
iter 1790: loss 1.6276, time 35.74ms, mfu 0.65%
iter 1795: loss 1.7763, time 36.81ms, mfu 0.64%
iter 1800: loss 1.7409, time 36.54ms, mfu 0.63%
iter 1805: loss 1.7454, time 36.33ms, mfu 0.62%
iter 1810: loss 1.7087, time 35.57ms, mfu 0.62%
iter 1815: loss 1.6874, time 35.72ms, mfu 0.61%
iter 1820: loss 1.7404, time 36.29ms, mfu 0.61%
iter 1825: loss 1.6674, time 28.94ms, mfu 0.62%
iter 1830: loss 1.7573, time 33.14ms, mfu 0.61%
iter 1835: loss 1.6792, time 36.41ms, mfu 0.61%
iter 1840: loss 1.6114, time 36.10ms, mfu 0.60%
iter 1845: loss 1.7240, time 35.31ms, mfu 0.60%
iter 1850: loss 1.7090, time 36.97ms, mfu 0.60%
iter 1855: loss 1.6725, time 19.86ms, mfu 0.64%
iter 1860: loss 1.7568, time 36.39ms, mfu 0.63%
iter 1865: loss 1.6428, time 32.57ms, mfu 0.63%
iter 1870: loss 1.6586, time 35.60ms, mfu 0.62%
iter 1875: loss 1.6452, time 21.97ms, mfu 0.65%
iter 1880: loss 1.6871, time 38.91ms, mfu 0.64%
iter 1885: loss 1.5826, time 32.32ms, mfu 0.64%
iter 1890: loss 1.6745, time 32.98ms, mfu 0.64%
iter 1895: loss 1.6743, time 29.06ms, mfu 0.64%
iter 1900: loss 1.7128, time 28.28ms, mfu 0.65%
iter 1905: loss 1.7152, time 38.51ms, mfu 0.64%
iter 1910: loss 1.6975, time 29.76ms, mfu 0.64%
iter 1915: loss 1.6864, time 36.87ms, mfu 0.63%
iter 1920: loss 1.7296, time 35.46ms, mfu 0.63%
iter 1925: loss 1.6505, time 36.22ms, mfu 0.62%
iter 1930: loss 1.6535, time 36.82ms, mfu 0.61%
iter 1935: loss 1.6599, time 36.25ms, mfu 0.61%
iter 1940: loss 1.7374, time 36.15ms, mfu 0.60%
iter 1945: loss 1.6384, time 37.26ms, mfu 0.60%
iter 1950: loss 1.7992, time 36.72ms, mfu 0.59%
iter 1955: loss 1.6513, time 36.21ms, mfu 0.59%
iter 1960: loss 1.6974, time 36.78ms, mfu 0.58%
iter 1965: loss 1.7005, time 34.69ms, mfu 0.58%
iter 1970: loss 1.6502, time 37.05ms, mfu 0.58%
iter 1975: loss 1.7054, time 36.46ms, mfu 0.58%
iter 1980: loss 1.6980, time 36.46ms, mfu 0.58%
iter 1985: loss 1.6864, time 36.53ms, mfu 0.57%
iter 1990: loss 1.6676, time 36.10ms, mfu 0.57%
iter 1995: loss 1.5918, time 29.70ms, mfu 0.58%
step 2000: train loss 1.6088, val loss 1.7718
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1
iter 2000: loss 1.7747, time 744.77ms, mfu 0.53%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.1 at: 
wandb: Find logs at: wandb/run-20251024_010058-a1siohfu/logs
Overriding config with config/train_shakespeare_char.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-char'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare-char'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare_char'
gradient_accumulation_steps = 1
batch_size = 64
block_size = 256 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

learning_rate = 1e-3 # with baby networks can afford to go a bit higher
max_iters = 5000
lr_decay_iters = 5000 # make equal to max_iters usually
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model

Overriding: compile = False
Overriding: eval_iters = 20
Overriding: log_interval = 5
Overriding: device = cuda
Overriding: lr_decay_iters = 1000
Overriding: block_size = 128
Overriding: n_layer = 6
Overriding: n_head = 8
Overriding: n_embd = 256
Overriding: batch_size = 16
Overriding: max_iters = 2000
Overriding: dropout = 0.2
Overriding: out_dir = out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
Overriding: wandb_log = True
Overriding: wandb_project = owt
Overriding: wandb_run_name = run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
tokens per iteration will be: 2,048
found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)
Initializing a new model from scratch
number of parameters: 4.74M
/content/nanoGPT/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 26, with 4,768,000 parameters
num non-decayed parameter tensors: 13, with 3,328 parameters
using fused AdamW: True
wandb: Currently logged in as: cjackson692 (cjackson692-university-of-north-texas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: ⢿ Waiting for wandb.init()...
wandb: ⣻ Waiting for wandb.init()...
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /content/nanoGPT/nanoGPT/wandb/run-20251024_010227-5ey95jud
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
wandb: ⭐️ View project at https://wandb.ai/cjackson692-university-of-north-texas/owt
wandb: 🚀 View run at https://wandb.ai/cjackson692-university-of-north-texas/owt/runs/5ey95jud
step 0: train loss 4.2246, val loss 4.2254
iter 0: loss 4.2213, time 1121.48ms, mfu -100.00%
iter 5: loss 3.6369, time 35.95ms, mfu 0.56%
iter 10: loss 3.3667, time 34.92ms, mfu 0.56%
iter 15: loss 3.1739, time 35.53ms, mfu 0.56%
iter 20: loss 3.0454, time 35.14ms, mfu 0.57%
iter 25: loss 2.9313, time 35.81ms, mfu 0.57%
iter 30: loss 2.8640, time 36.38ms, mfu 0.56%
iter 35: loss 2.7836, time 35.74ms, mfu 0.56%
iter 40: loss 2.6883, time 19.14ms, mfu 0.61%
iter 45: loss 2.6653, time 35.35ms, mfu 0.61%
iter 50: loss 2.6165, time 35.46ms, mfu 0.61%
iter 55: loss 2.6107, time 36.77ms, mfu 0.60%
iter 60: loss 2.5703, time 36.61ms, mfu 0.60%
iter 65: loss 2.5852, time 35.84ms, mfu 0.59%
iter 70: loss 2.6227, time 35.92ms, mfu 0.59%
iter 75: loss 2.5290, time 35.95ms, mfu 0.59%
iter 80: loss 2.5459, time 36.88ms, mfu 0.58%
iter 85: loss 2.5556, time 36.16ms, mfu 0.58%
iter 90: loss 2.5788, time 35.63ms, mfu 0.58%
iter 95: loss 2.5461, time 35.28ms, mfu 0.58%
iter 100: loss 2.5068, time 36.23ms, mfu 0.58%
iter 105: loss 2.5687, time 36.59ms, mfu 0.57%
iter 110: loss 2.5476, time 22.59ms, mfu 0.61%
iter 115: loss 2.5533, time 21.73ms, mfu 0.64%
iter 120: loss 2.5594, time 21.39ms, mfu 0.67%
iter 125: loss 2.4870, time 21.72ms, mfu 0.70%
iter 130: loss 2.5380, time 24.71ms, mfu 0.71%
iter 135: loss 2.5194, time 24.55ms, mfu 0.72%
iter 140: loss 2.4844, time 27.98ms, mfu 0.72%
iter 145: loss 2.5404, time 27.20ms, mfu 0.72%
iter 150: loss 2.4813, time 22.36ms, mfu 0.74%
iter 155: loss 2.5529, time 20.61ms, mfu 0.76%
iter 160: loss 2.5272, time 24.82ms, mfu 0.77%
iter 165: loss 2.4664, time 23.87ms, mfu 0.78%
iter 170: loss 2.4937, time 33.47ms, mfu 0.76%
iter 175: loss 2.4449, time 25.45ms, mfu 0.76%
iter 180: loss 2.4611, time 25.98ms, mfu 0.76%
iter 185: loss 2.4651, time 26.32ms, mfu 0.76%
iter 190: loss 2.4597, time 27.74ms, mfu 0.76%
iter 195: loss 2.4598, time 28.61ms, mfu 0.76%
iter 200: loss 2.4799, time 18.91ms, mfu 0.79%
iter 205: loss 2.4588, time 32.49ms, mfu 0.77%
iter 210: loss 2.4431, time 32.43ms, mfu 0.76%
iter 215: loss 2.5094, time 36.06ms, mfu 0.74%
iter 220: loss 2.4435, time 37.17ms, mfu 0.72%
iter 225: loss 2.4600, time 37.64ms, mfu 0.70%
iter 230: loss 2.4103, time 38.49ms, mfu 0.68%
iter 235: loss 2.4403, time 36.35ms, mfu 0.67%
iter 240: loss 2.4494, time 36.31ms, mfu 0.66%
iter 245: loss 2.4527, time 35.91ms, mfu 0.65%
step 250: train loss 2.4175, val loss 2.4293
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
iter 250: loss 2.4674, time 708.56ms, mfu 0.59%
iter 255: loss 2.4324, time 36.30ms, mfu 0.58%
iter 260: loss 2.4347, time 36.78ms, mfu 0.58%
iter 265: loss 2.4144, time 36.35ms, mfu 0.58%
iter 270: loss 2.4567, time 36.27ms, mfu 0.58%
iter 275: loss 2.4328, time 37.89ms, mfu 0.57%
iter 280: loss 2.4232, time 36.06ms, mfu 0.57%
iter 285: loss 2.3818, time 36.42ms, mfu 0.57%
iter 290: loss 2.4413, time 36.00ms, mfu 0.57%
iter 295: loss 2.4121, time 37.63ms, mfu 0.56%
iter 300: loss 2.3746, time 36.14ms, mfu 0.56%
iter 305: loss 2.4048, time 36.90ms, mfu 0.56%
iter 310: loss 2.4121, time 35.58ms, mfu 0.56%
iter 315: loss 2.4417, time 36.29ms, mfu 0.56%
iter 320: loss 2.3940, time 36.72ms, mfu 0.56%
iter 325: loss 2.3704, time 37.24ms, mfu 0.56%
iter 330: loss 2.3493, time 36.42ms, mfu 0.56%
iter 335: loss 2.3734, time 36.07ms, mfu 0.56%
iter 340: loss 2.3410, time 36.66ms, mfu 0.56%
iter 345: loss 2.3437, time 36.38ms, mfu 0.56%
iter 350: loss 2.4014, time 35.93ms, mfu 0.56%
iter 355: loss 2.3599, time 37.31ms, mfu 0.56%
iter 360: loss 2.3157, time 36.30ms, mfu 0.56%
iter 365: loss 2.3700, time 25.64ms, mfu 0.58%
iter 370: loss 2.3749, time 36.16ms, mfu 0.58%
iter 375: loss 2.2634, time 35.89ms, mfu 0.58%
iter 380: loss 2.3777, time 36.49ms, mfu 0.57%
iter 385: loss 2.3596, time 36.83ms, mfu 0.57%
iter 390: loss 2.3219, time 36.82ms, mfu 0.57%
iter 395: loss 2.3144, time 36.67ms, mfu 0.57%
iter 400: loss 2.3518, time 36.17ms, mfu 0.57%
iter 405: loss 2.2564, time 35.99ms, mfu 0.57%
iter 410: loss 2.2784, time 36.19ms, mfu 0.57%
iter 415: loss 2.2695, time 37.18ms, mfu 0.56%
iter 420: loss 2.2814, time 37.01ms, mfu 0.56%
iter 425: loss 2.2517, time 36.77ms, mfu 0.56%
iter 430: loss 2.3157, time 35.53ms, mfu 0.56%
iter 435: loss 2.2712, time 36.68ms, mfu 0.56%
iter 440: loss 2.3553, time 36.33ms, mfu 0.56%
iter 445: loss 2.3046, time 36.81ms, mfu 0.56%
iter 450: loss 2.2885, time 34.45ms, mfu 0.56%
iter 455: loss 2.2715, time 38.44ms, mfu 0.56%
iter 460: loss 2.2416, time 21.04ms, mfu 0.60%
iter 465: loss 2.2268, time 21.19ms, mfu 0.63%
iter 470: loss 2.2336, time 22.16ms, mfu 0.66%
iter 475: loss 2.1926, time 31.86ms, mfu 0.66%
iter 480: loss 2.2983, time 20.79ms, mfu 0.69%
iter 485: loss 2.2107, time 21.03ms, mfu 0.72%
iter 490: loss 2.2214, time 24.22ms, mfu 0.73%
iter 495: loss 2.3129, time 21.57ms, mfu 0.75%
step 500: train loss 2.1762, val loss 2.2142
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
iter 500: loss 2.2532, time 829.48ms, mfu 0.68%
iter 505: loss 2.1822, time 26.45ms, mfu 0.69%
iter 510: loss 2.2796, time 25.81ms, mfu 0.70%
iter 515: loss 2.2209, time 27.17ms, mfu 0.70%
iter 520: loss 2.2433, time 26.56ms, mfu 0.71%
iter 525: loss 2.2207, time 35.56ms, mfu 0.69%
iter 530: loss 2.2297, time 26.79ms, mfu 0.70%
iter 535: loss 2.2353, time 36.99ms, mfu 0.68%
iter 540: loss 2.2056, time 37.03ms, mfu 0.67%
iter 545: loss 2.1898, time 36.73ms, mfu 0.66%
iter 550: loss 2.1659, time 36.70ms, mfu 0.65%
iter 555: loss 2.1565, time 37.57ms, mfu 0.64%
iter 560: loss 2.1519, time 38.02ms, mfu 0.63%
iter 565: loss 2.2056, time 37.09ms, mfu 0.62%
iter 570: loss 2.1462, time 34.71ms, mfu 0.61%
iter 575: loss 2.1440, time 36.57ms, mfu 0.61%
iter 580: loss 2.1823, time 36.72ms, mfu 0.60%
iter 585: loss 2.1645, time 37.68ms, mfu 0.60%
iter 590: loss 2.1625, time 36.13ms, mfu 0.59%
iter 595: loss 2.1746, time 37.07ms, mfu 0.59%
iter 600: loss 2.1051, time 30.62ms, mfu 0.59%
iter 605: loss 2.1143, time 37.65ms, mfu 0.59%
iter 610: loss 2.1557, time 37.58ms, mfu 0.58%
iter 615: loss 2.1563, time 36.65ms, mfu 0.58%
iter 620: loss 2.1005, time 35.87ms, mfu 0.58%
iter 625: loss 2.1556, time 38.14ms, mfu 0.57%
iter 630: loss 2.0754, time 36.41ms, mfu 0.57%
iter 635: loss 2.1809, time 36.45ms, mfu 0.57%
iter 640: loss 2.1084, time 37.57ms, mfu 0.57%
iter 645: loss 2.0922, time 37.70ms, mfu 0.56%
iter 650: loss 2.0939, time 36.58ms, mfu 0.56%
iter 655: loss 2.1016, time 34.53ms, mfu 0.57%
iter 660: loss 2.0657, time 37.05ms, mfu 0.56%
iter 665: loss 2.1150, time 37.44ms, mfu 0.56%
iter 670: loss 2.1369, time 37.79ms, mfu 0.56%
iter 675: loss 2.1234, time 37.46ms, mfu 0.56%
iter 680: loss 2.1306, time 37.81ms, mfu 0.55%
iter 685: loss 2.0008, time 37.23ms, mfu 0.55%
iter 690: loss 2.1640, time 36.14ms, mfu 0.55%
iter 695: loss 2.0641, time 36.86ms, mfu 0.55%
iter 700: loss 2.0855, time 30.54ms, mfu 0.56%
iter 705: loss 2.0854, time 23.89ms, mfu 0.59%
iter 710: loss 2.0803, time 17.19ms, mfu 0.65%
iter 715: loss 2.1132, time 37.18ms, mfu 0.64%
iter 720: loss 2.1184, time 37.47ms, mfu 0.63%
iter 725: loss 2.0546, time 37.03ms, mfu 0.62%
iter 730: loss 2.0487, time 36.46ms, mfu 0.61%
iter 735: loss 2.0450, time 37.63ms, mfu 0.61%
iter 740: loss 2.0344, time 37.12ms, mfu 0.60%
iter 745: loss 2.1162, time 38.77ms, mfu 0.59%
step 750: train loss 1.9837, val loss 2.0772
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
iter 750: loss 2.1107, time 751.35ms, mfu 0.54%
iter 755: loss 2.0495, time 38.03ms, mfu 0.54%
iter 760: loss 2.0742, time 37.32ms, mfu 0.54%
iter 765: loss 2.0739, time 39.57ms, mfu 0.53%
iter 770: loss 2.0450, time 36.90ms, mfu 0.54%
iter 775: loss 2.0117, time 28.47ms, mfu 0.55%
iter 780: loss 1.9973, time 36.52ms, mfu 0.55%
iter 785: loss 2.0357, time 27.32ms, mfu 0.57%
iter 790: loss 2.0329, time 22.06ms, mfu 0.61%
iter 795: loss 1.9505, time 37.26ms, mfu 0.60%
iter 800: loss 2.0484, time 32.05ms, mfu 0.60%
iter 805: loss 2.0405, time 25.66ms, mfu 0.62%
iter 810: loss 2.0098, time 21.57ms, mfu 0.65%
iter 815: loss 2.1500, time 23.90ms, mfu 0.67%
iter 820: loss 2.0177, time 24.34ms, mfu 0.69%
iter 825: loss 2.0822, time 23.33ms, mfu 0.71%
iter 830: loss 2.0660, time 33.24ms, mfu 0.70%
iter 835: loss 2.0361, time 25.36ms, mfu 0.71%
iter 840: loss 2.0457, time 41.45ms, mfu 0.68%
iter 845: loss 2.0056, time 29.87ms, mfu 0.68%
iter 850: loss 2.0095, time 38.16ms, mfu 0.67%
iter 855: loss 2.0330, time 20.43ms, mfu 0.70%
iter 860: loss 1.9817, time 23.32ms, mfu 0.72%
iter 865: loss 2.0548, time 25.53ms, mfu 0.72%
iter 870: loss 1.9887, time 25.25ms, mfu 0.73%
iter 875: loss 2.0026, time 29.52ms, mfu 0.73%
iter 880: loss 1.9664, time 25.86ms, mfu 0.73%
iter 885: loss 2.0395, time 26.92ms, mfu 0.73%
iter 890: loss 2.0360, time 25.67ms, mfu 0.74%
iter 895: loss 1.9974, time 26.93ms, mfu 0.74%
iter 900: loss 2.0647, time 36.96ms, mfu 0.72%
iter 905: loss 2.0072, time 36.89ms, mfu 0.70%
iter 910: loss 1.9676, time 37.53ms, mfu 0.69%
iter 915: loss 1.9958, time 37.94ms, mfu 0.67%
iter 920: loss 2.0568, time 36.86ms, mfu 0.66%
iter 925: loss 2.0322, time 37.64ms, mfu 0.65%
iter 930: loss 2.1012, time 36.10ms, mfu 0.64%
iter 935: loss 1.9248, time 37.13ms, mfu 0.63%
iter 940: loss 2.0617, time 34.34ms, mfu 0.63%
iter 945: loss 1.9843, time 35.29ms, mfu 0.62%
iter 950: loss 2.0127, time 36.49ms, mfu 0.61%
iter 955: loss 1.9974, time 36.56ms, mfu 0.61%
iter 960: loss 1.9752, time 27.75ms, mfu 0.62%
iter 965: loss 1.9568, time 36.71ms, mfu 0.61%
iter 970: loss 1.9855, time 38.92ms, mfu 0.60%
iter 975: loss 1.9683, time 37.09ms, mfu 0.60%
iter 980: loss 2.0724, time 37.63ms, mfu 0.59%
iter 985: loss 1.9356, time 37.21ms, mfu 0.59%
iter 990: loss 1.9991, time 36.77ms, mfu 0.58%
iter 995: loss 2.0469, time 37.08ms, mfu 0.58%
step 1000: train loss 1.8910, val loss 1.9985
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
iter 1000: loss 1.9959, time 780.01ms, mfu 0.52%
iter 1005: loss 2.0434, time 38.94ms, mfu 0.52%
iter 1010: loss 2.0197, time 28.08ms, mfu 0.54%
iter 1015: loss 1.9899, time 37.04ms, mfu 0.54%
iter 1020: loss 1.9447, time 36.39ms, mfu 0.54%
iter 1025: loss 2.0375, time 40.88ms, mfu 0.54%
iter 1030: loss 2.0656, time 35.71ms, mfu 0.54%
iter 1035: loss 2.0172, time 36.63ms, mfu 0.54%
iter 1040: loss 1.9208, time 36.95ms, mfu 0.54%
iter 1045: loss 1.9605, time 31.22ms, mfu 0.55%
iter 1050: loss 1.9827, time 36.50ms, mfu 0.55%
iter 1055: loss 1.9899, time 37.63ms, mfu 0.55%
iter 1060: loss 2.0379, time 39.92ms, mfu 0.55%
iter 1065: loss 1.9969, time 38.08ms, mfu 0.55%
iter 1070: loss 1.9695, time 36.25ms, mfu 0.55%
iter 1075: loss 1.9378, time 36.72ms, mfu 0.55%
iter 1080: loss 1.9804, time 36.35ms, mfu 0.55%
iter 1085: loss 2.0144, time 37.64ms, mfu 0.55%
iter 1090: loss 2.0331, time 36.67ms, mfu 0.55%
iter 1095: loss 1.9836, time 33.35ms, mfu 0.55%
iter 1100: loss 2.0054, time 37.26ms, mfu 0.55%
iter 1105: loss 2.0094, time 36.86ms, mfu 0.55%
iter 1110: loss 1.9681, time 37.54ms, mfu 0.55%
iter 1115: loss 1.9062, time 36.27ms, mfu 0.55%
iter 1120: loss 1.9406, time 38.13ms, mfu 0.55%
iter 1125: loss 1.9894, time 37.97ms, mfu 0.55%
iter 1130: loss 1.9504, time 37.24ms, mfu 0.55%
iter 1135: loss 1.9117, time 36.83ms, mfu 0.55%
iter 1140: loss 1.9655, time 37.25ms, mfu 0.55%
iter 1145: loss 1.9583, time 34.25ms, mfu 0.55%
iter 1150: loss 1.9477, time 22.49ms, mfu 0.59%
iter 1155: loss 1.9628, time 26.88ms, mfu 0.60%
iter 1160: loss 1.9364, time 26.05ms, mfu 0.62%
iter 1165: loss 1.9271, time 25.71ms, mfu 0.64%
iter 1170: loss 1.9736, time 24.28ms, mfu 0.66%
iter 1175: loss 1.9968, time 31.18ms, mfu 0.66%
iter 1180: loss 2.0307, time 21.10ms, mfu 0.69%
iter 1185: loss 1.9548, time 36.37ms, mfu 0.67%
iter 1190: loss 1.8833, time 27.14ms, mfu 0.68%
iter 1195: loss 1.9698, time 31.25ms, mfu 0.68%
iter 1200: loss 1.9993, time 23.42ms, mfu 0.70%
iter 1205: loss 1.9596, time 21.33ms, mfu 0.72%
iter 1210: loss 1.9157, time 28.48ms, mfu 0.72%
iter 1215: loss 1.9598, time 26.87ms, mfu 0.72%
iter 1220: loss 1.9325, time 33.41ms, mfu 0.71%
iter 1225: loss 1.9046, time 25.26ms, mfu 0.72%
iter 1230: loss 1.9604, time 26.75ms, mfu 0.72%
iter 1235: loss 1.9480, time 31.09ms, mfu 0.72%
iter 1240: loss 1.9473, time 25.52ms, mfu 0.72%
iter 1245: loss 1.9598, time 20.19ms, mfu 0.75%
step 1250: train loss 1.8732, val loss 1.9962
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
iter 1250: loss 1.9377, time 758.19ms, mfu 0.68%
iter 1255: loss 1.9699, time 36.98ms, mfu 0.67%
iter 1260: loss 1.9214, time 34.51ms, mfu 0.66%
iter 1265: loss 1.9467, time 36.85ms, mfu 0.65%
iter 1270: loss 1.9877, time 36.86ms, mfu 0.64%
iter 1275: loss 1.9581, time 29.82ms, mfu 0.64%
iter 1280: loss 1.9653, time 27.87ms, mfu 0.65%
iter 1285: loss 1.9336, time 36.72ms, mfu 0.64%
iter 1290: loss 1.9056, time 36.41ms, mfu 0.63%
iter 1295: loss 1.9786, time 36.83ms, mfu 0.62%
iter 1300: loss 2.0259, time 35.33ms, mfu 0.62%
iter 1305: loss 1.9407, time 32.74ms, mfu 0.62%
iter 1310: loss 1.9634, time 37.21ms, mfu 0.61%
iter 1315: loss 1.8679, time 36.02ms, mfu 0.61%
iter 1320: loss 1.9333, time 36.64ms, mfu 0.60%
iter 1325: loss 1.9392, time 37.34ms, mfu 0.59%
iter 1330: loss 1.9921, time 36.53ms, mfu 0.59%
iter 1335: loss 1.9757, time 36.41ms, mfu 0.59%
iter 1340: loss 1.9431, time 38.50ms, mfu 0.58%
iter 1345: loss 2.0210, time 36.71ms, mfu 0.58%
iter 1350: loss 1.9764, time 36.38ms, mfu 0.58%
iter 1355: loss 1.9579, time 36.23ms, mfu 0.57%
iter 1360: loss 1.9400, time 33.31ms, mfu 0.58%
iter 1365: loss 2.0231, time 36.62ms, mfu 0.57%
iter 1370: loss 1.9738, time 37.20ms, mfu 0.57%
iter 1375: loss 1.8699, time 37.01ms, mfu 0.57%
iter 1380: loss 2.0043, time 37.48ms, mfu 0.57%
iter 1385: loss 1.9597, time 36.11ms, mfu 0.57%
iter 1390: loss 1.8829, time 30.58ms, mfu 0.57%
iter 1395: loss 1.9870, time 36.44ms, mfu 0.57%
iter 1400: loss 1.8992, time 36.83ms, mfu 0.57%
iter 1405: loss 1.9373, time 36.38ms, mfu 0.57%
iter 1410: loss 1.9985, time 36.09ms, mfu 0.57%
iter 1415: loss 1.9069, time 37.46ms, mfu 0.57%
iter 1420: loss 1.9996, time 36.35ms, mfu 0.56%
iter 1425: loss 1.9236, time 36.90ms, mfu 0.56%
iter 1430: loss 1.9495, time 37.10ms, mfu 0.56%
iter 1435: loss 1.9221, time 36.00ms, mfu 0.56%
iter 1440: loss 1.9917, time 36.45ms, mfu 0.56%
iter 1445: loss 2.0138, time 31.32ms, mfu 0.57%
iter 1450: loss 1.9668, time 35.06ms, mfu 0.57%
iter 1455: loss 1.9566, time 37.01ms, mfu 0.57%
iter 1460: loss 1.9297, time 36.50ms, mfu 0.57%
iter 1465: loss 2.0071, time 36.30ms, mfu 0.56%
iter 1470: loss 1.8807, time 30.97ms, mfu 0.57%
iter 1475: loss 1.9584, time 36.51ms, mfu 0.57%
iter 1480: loss 1.9458, time 35.70ms, mfu 0.57%
iter 1485: loss 1.9136, time 36.38ms, mfu 0.57%
iter 1490: loss 1.9118, time 38.07ms, mfu 0.57%
iter 1495: loss 1.9522, time 37.77ms, mfu 0.56%
step 1500: train loss 1.8398, val loss 1.9656
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
iter 1500: loss 1.9927, time 776.71ms, mfu 0.51%
iter 1505: loss 1.9524, time 23.23ms, mfu 0.55%
iter 1510: loss 1.9202, time 24.36ms, mfu 0.57%
iter 1515: loss 1.9812, time 21.57ms, mfu 0.61%
iter 1520: loss 1.8963, time 22.32ms, mfu 0.64%
iter 1525: loss 1.9295, time 22.05ms, mfu 0.67%
iter 1530: loss 1.9223, time 26.17ms, mfu 0.68%
iter 1535: loss 1.9199, time 26.90ms, mfu 0.69%
iter 1540: loss 1.9965, time 58.55ms, mfu 0.65%
iter 1545: loss 1.9535, time 37.86ms, mfu 0.64%
iter 1550: loss 1.9820, time 27.26ms, mfu 0.65%
iter 1555: loss 1.9298, time 26.95ms, mfu 0.66%
iter 1560: loss 1.8864, time 27.58ms, mfu 0.67%
iter 1565: loss 1.8833, time 39.28ms, mfu 0.65%
iter 1570: loss 1.8975, time 27.82ms, mfu 0.66%
iter 1575: loss 1.9073, time 27.08ms, mfu 0.67%
iter 1580: loss 1.9486, time 25.66ms, mfu 0.68%
iter 1585: loss 1.9855, time 24.84ms, mfu 0.69%
iter 1590: loss 1.9184, time 17.53ms, mfu 0.74%
iter 1595: loss 1.9219, time 37.19ms, mfu 0.72%
iter 1600: loss 1.8818, time 36.90ms, mfu 0.70%
iter 1605: loss 1.9263, time 36.45ms, mfu 0.69%
iter 1610: loss 1.8669, time 36.42ms, mfu 0.67%
iter 1615: loss 1.8558, time 37.19ms, mfu 0.66%
iter 1620: loss 1.8430, time 36.54ms, mfu 0.65%
iter 1625: loss 1.8927, time 26.86ms, mfu 0.66%
iter 1630: loss 1.9081, time 36.35ms, mfu 0.65%
iter 1635: loss 1.8843, time 36.84ms, mfu 0.64%
iter 1640: loss 1.8691, time 36.75ms, mfu 0.63%
iter 1645: loss 1.9536, time 36.24ms, mfu 0.62%
iter 1650: loss 1.8473, time 40.24ms, mfu 0.61%
iter 1655: loss 1.8857, time 36.25ms, mfu 0.61%
iter 1660: loss 1.8733, time 37.20ms, mfu 0.60%
iter 1665: loss 1.8706, time 36.28ms, mfu 0.60%
iter 1670: loss 2.0515, time 36.37ms, mfu 0.59%
iter 1675: loss 1.9081, time 36.88ms, mfu 0.59%
iter 1680: loss 1.8957, time 28.41ms, mfu 0.60%
iter 1685: loss 1.9083, time 33.89ms, mfu 0.60%
iter 1690: loss 1.8859, time 36.27ms, mfu 0.60%
iter 1695: loss 1.9179, time 35.34ms, mfu 0.59%
iter 1700: loss 1.9134, time 36.72ms, mfu 0.59%
iter 1705: loss 1.8979, time 36.43ms, mfu 0.59%
iter 1710: loss 2.0154, time 36.93ms, mfu 0.58%
iter 1715: loss 1.8718, time 36.47ms, mfu 0.58%
iter 1720: loss 1.9159, time 36.23ms, mfu 0.58%
iter 1725: loss 1.8838, time 36.65ms, mfu 0.57%
iter 1730: loss 1.8628, time 37.08ms, mfu 0.57%
iter 1735: loss 1.9045, time 37.55ms, mfu 0.57%
iter 1740: loss 1.8847, time 36.53ms, mfu 0.57%
iter 1745: loss 1.8689, time 36.58ms, mfu 0.57%
step 1750: train loss 1.8021, val loss 1.9280
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
iter 1750: loss 1.8946, time 743.12ms, mfu 0.51%
iter 1755: loss 1.8187, time 37.29ms, mfu 0.51%
iter 1760: loss 1.9692, time 36.30ms, mfu 0.52%
iter 1765: loss 1.8850, time 36.19ms, mfu 0.52%
iter 1770: loss 1.8938, time 36.04ms, mfu 0.53%
iter 1775: loss 1.9495, time 29.06ms, mfu 0.54%
iter 1780: loss 1.8942, time 35.80ms, mfu 0.55%
iter 1785: loss 1.9783, time 35.65ms, mfu 0.55%
iter 1790: loss 1.8152, time 31.56ms, mfu 0.56%
iter 1795: loss 1.9502, time 30.61ms, mfu 0.57%
iter 1800: loss 1.9092, time 36.76ms, mfu 0.57%
iter 1805: loss 1.9306, time 36.35ms, mfu 0.56%
iter 1810: loss 1.9271, time 36.46ms, mfu 0.56%
iter 1815: loss 1.8684, time 37.62ms, mfu 0.56%
iter 1820: loss 1.9113, time 36.30ms, mfu 0.56%
iter 1825: loss 1.8542, time 35.12ms, mfu 0.56%
iter 1830: loss 1.9284, time 35.44ms, mfu 0.56%
iter 1835: loss 1.8551, time 35.64ms, mfu 0.56%
iter 1840: loss 1.8156, time 36.95ms, mfu 0.56%
iter 1845: loss 1.8993, time 28.64ms, mfu 0.58%
iter 1850: loss 1.8687, time 26.56ms, mfu 0.59%
iter 1855: loss 1.8862, time 22.72ms, mfu 0.62%
iter 1860: loss 1.9452, time 21.81ms, mfu 0.65%
iter 1865: loss 1.7972, time 21.62ms, mfu 0.68%
iter 1870: loss 1.8835, time 28.35ms, mfu 0.69%
iter 1875: loss 1.8582, time 31.27ms, mfu 0.68%
iter 1880: loss 1.8916, time 20.47ms, mfu 0.71%
iter 1885: loss 1.7934, time 21.75ms, mfu 0.73%
iter 1890: loss 1.8487, time 20.69ms, mfu 0.76%
iter 1895: loss 1.8443, time 30.68ms, mfu 0.75%
iter 1900: loss 1.8793, time 23.45ms, mfu 0.76%
iter 1905: loss 1.8972, time 25.74ms, mfu 0.76%
iter 1910: loss 1.8768, time 23.33ms, mfu 0.77%
iter 1915: loss 1.8940, time 31.33ms, mfu 0.76%
iter 1920: loss 1.8909, time 25.47ms, mfu 0.76%
iter 1925: loss 1.8445, time 35.38ms, mfu 0.74%
iter 1930: loss 1.8337, time 33.78ms, mfu 0.73%
iter 1935: loss 1.8353, time 26.24ms, mfu 0.73%
iter 1940: loss 1.9305, time 28.91ms, mfu 0.73%
iter 1945: loss 1.8252, time 29.33ms, mfu 0.73%
iter 1950: loss 1.9552, time 20.59ms, mfu 0.75%
iter 1955: loss 1.8254, time 38.41ms, mfu 0.73%
iter 1960: loss 1.9016, time 36.52ms, mfu 0.71%
iter 1965: loss 1.9062, time 35.81ms, mfu 0.70%
iter 1970: loss 1.8432, time 36.46ms, mfu 0.68%
iter 1975: loss 1.8885, time 29.14ms, mfu 0.68%
iter 1980: loss 1.8699, time 37.20ms, mfu 0.67%
iter 1985: loss 1.8630, time 36.22ms, mfu 0.66%
iter 1990: loss 1.8477, time 36.30ms, mfu 0.65%
iter 1995: loss 1.7538, time 36.43ms, mfu 0.64%
step 2000: train loss 1.7736, val loss 1.9077
saving checkpoint to out/block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2
iter 2000: loss 1.9329, time 733.41ms, mfu 0.58%
wandb: 
wandb: 🚀 View run run_block_size128_n_layer6_n_head8_n_embd256_batch_size16_max_iters2000_dropout0.2 at: 
wandb: Find logs at: wandb/run-20251024_010227-5ey95jud/logs
